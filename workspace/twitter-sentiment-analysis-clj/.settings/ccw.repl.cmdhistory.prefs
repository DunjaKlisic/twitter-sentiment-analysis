cmdhistory=["(prepare-request-with-multi statuses-filter {\:proxy \\"172.16.0.3\:8080\\"})" "(tr/prepare-request-with-multi statuses-filter {\:proxy \\"172.16.0.3\:8080\\"})" "(ns twitter-sentiment-analysis.data-collection\\r\\n  (\:use\\r\\n   [twitter.oauth]\\r\\n   [twitter.callbacks]\\r\\n   [twitter.callbacks.handlers]\\r\\n   [twitter.api.streaming]\\r\\n   [twitter.request \:as tr]\\r\\n   [clojure.java.io \:as io])\\r\\n  (\:require\\r\\n   [clojure.data.json \:as json]\\r\\n   [http.async.client \:as ac])\\r\\n  (\:import\\r\\n   (twitter.callbacks.protocols AsyncStreamingCallback)))" "(tr/prepare-request-with-multi statuses-filter {\:proxy \\"172.16.0.3\:8080\\"})" "(tr/prepare-request-with-multi statuses-filter \\"twitter/dunja.html\\" {\:proxy \\"172.16.0.3\:8080\\"})" "(tr/prepare-request-with-multi statuses-filter \\"twitter/dunja.html\\" \:key {\:proxy \\"172.16.0.3\:8080\\"})" "(ns twitter-sentiment-analysis.data-collection\\r\\n  (\:use\\r\\n   ;;[twitter.oauth]\\r\\n   ;;[twitter.callbacks]\\r\\n   ;;[twitter.callbacks.handlers]\\r\\n   ;;[twitter.api.streaming]\\r\\n   [twitter callbacks oauth  callbacks.handler api  api.streaming utils request]\\r\\n   [clojure.java.io \:as io])\\r\\n  (\:require\\r\\n   [clojure.data.json \:as json]\\r\\n   [http.async.client \:as ac])\\r\\n  (\:import\\r\\n   (twitter.callbacks.protocols AsyncStreamingCallback)))" "(ns twitter-sentiment-analysis.data-collection\\r\\n  (\:use\\r\\n   ;;[twitter.oauth]\\r\\n   ;;[twitter.callbacks]\\r\\n   ;;[twitter.callbacks.handlers]\\r\\n   ;;[twitter.api.streaming]\\r\\n   [twitter callbacks oauth api utils request]\\r\\n   [clojure.java.io \:as io])\\r\\n  (\:require\\r\\n   [clojure.data.json \:as json]\\r\\n   [http.async.client \:as ac])\\r\\n  (\:import\\r\\n   (twitter.callbacks.protocols AsyncStreamingCallback)))" "(def my-creds (make-oauth-creds \\"nemg8aXAA9wtSo8h6ZZbrsYwk\\"\\r\\n                                \\"XmRZzVqlj1i5Ag72RJpd99IQLPoK6dfvbna89SwQ1NHIJVqN1l\\"\\r\\n                                \\"3021555060-YgU9ZHHTKVQGpkbDqNw9By4VNcFz7Ph455VAY0G\\"\\r\\n                                \\"XR9MZ3SE6quDbTfGcPNTiG2QebsdYnepStnZnrrzUeuye\\"))" "(tr/prepare-request-with-multi statuses-filter \\"twitter/dunja.html\\" {\:proxy \\"172.16.0.3\:8080\\"})" "(tr/prepare-request-with-multi statuses-filter \\"twitter/dunja.html\\" \:key {\:proxy \\"172.16.0.3\:8080\\"})" "(tr/prepare-request-with-multi statuses-filter \\"https\://twitter.com/search\\" \:key {\:proxy \\"172.16.0.3\:8080\\"})" "(ns twitter-sentiment-analysis.data-collection\\r\\n  (\:use\\r\\n   ;;[twitter.oauth]\\r\\n   ;;[twitter.callbacks]\\r\\n   ;;[twitter.callbacks.handlers]\\r\\n   ;;[twitter.api.streaming]\\r\\n   [twitter callbacks oauth api utils request]\\r\\n   [clojure.java.io \:as io])\\r\\n  (\:require\\r\\n   [clojure.data.json \:as json]\\r\\n   [http.async.client \:as ac])\\r\\n  (\:import\\r\\n   (twitter.callbacks.protocols AsyncStreamingCallback)))" "(twitter.request/prepare-request-with-multi statuses-filter \\"https\://twitter.com/search\\" \:key {\:proxy \\"172.16.0.3\:8080\\"})" "(ns twitter-sentiment-analysis.data-collection\\r\\n  (\:use\\r\\n   ;;[twitter.oauth]\\r\\n   ;;[twitter.callbacks]\\r\\n   ;;[twitter.callbacks.handlers]\\r\\n   ;;[twitter.api.streaming]\\r\\n   [twitter callbacks oauth api utils request]\\r\\n   [clojure.java.io \:as io])\\r\\n  (\:require\\r\\n   [clojure.data.json \:as json]\\r\\n   [http.async.client \:as ac]\\r\\n   [http.async.client.util \:as requ]\\r\\n   [http.async.client.request \:as req]\\r\\n   [clojure.string \:as string])\\r\\n  (\:import (com.ning.http.client Cookie\\r\\n                                 FluentCaseInsensitiveStringsMap\\r\\n                                 PerRequestConfig\\r\\n                                 Request\\r\\n                                 RequestBuilder)\\r\\n           (com.ning.http.multipart StringPart\\r\\n                                    FilePart)\\r\\n           (java.io File InputStream)\\r\\n  \\r\\n   (twitter.callbacks.protocols AsyncStreamingCallback)))" "(twitter.request/prepare-request-with-multi statuses-filter \\"https\://twitter.com/search\\" \:key {\:proxy \\"172.16.0.3\:8080\\"})" "(ns twitter-sentiment-analysis.data-collection\\r\\n  (\:use\\r\\n   ;;[twitter.oauth]\\r\\n   ;;[twitter.callbacks]\\r\\n   ;;[twitter.callbacks.handlers]\\r\\n   ;;[twitter.api.streaming]\\r\\n   [twitter callbacks oauth api utils request]\\r\\n   [clojure.java.io \:as io])\\r\\n  (\:require\\r\\n   [clojure.data.json \:as json]\\r\\n   [http.async.client \:as ac]\\r\\n   [http.async.client.util \:as requ]\\r\\n   [http.async.client.request \:as req]\\r\\n   [clojure.string \:as string])\\r\\n  (\:import (com.ning.http.client Cookie\\r\\n                                 FluentCaseInsensitiveStringsMap\\r\\n                                 PerRequestConfig\\r\\n                                 Request\\r\\n                                 RequestBuilder\\n                                 RequestBuilderBase)\\r\\n           (com.ning.http.multipart StringPart\\r\\n                                    FilePart)\\r\\n           (java.io File InputStream)\\r\\n  \\r\\n   (twitter.callbacks.protocols AsyncStreamingCallback)))" "(twitter.request/prepare-request-with-multi statuses-filter \\"https\://twitter.com/search\\" \:key {\:proxy \\"172.16.0.3\:8080\\"})" "(twitter.request/prepare-request-with-multi statuses-filter \\"https\://twitter.com/search\\")" "(twitter.request/prepare-request-with-multi statuses-filter \\"https\://twitter.com/search\\" \:key [\:proxy 172.16.0.3])" "(twitter.request/prepare-request-with-multi statuses-filter \\"https\://twitter.com/search\\" \:key [\:proxy \\"172.16.0.3\:8080\\"])" "(ns mynamespace\\r\\n  (\:use\\r\\n   [twitter.oauth]\\r\\n   [twitter.callbacks]\\r\\n   [twitter.callbacks.handlers]\\r\\n   [twitter.api.restful])\\r\\n  (\:import\\r\\n   (twitter.callbacks.protocols SyncSingleCallback)))" "(def my-creds (make-oauth-creds \\"nemg8aXAA9wtSo8h6ZZbrsYwk\\"\\r\\n                                \\"XmRZzVqlj1i5Ag72RJpd99IQLPoK6dfvbna89SwQ1NHIJVqN1l\\"\\r\\n                                \\"3021555060-YgU9ZHHTKVQGpkbDqNw9By4VNcFz7Ph455VAY0G\\"\\r\\n                                \\"XR9MZ3SE6quDbTfGcPNTiG2QebsdYnepStnZnrrzUeuye\\"))" "(users-show \:oauth-creds my-creds \:params {\:screen-name \\"AdamJWynne\\"})" "(users-show \:oauth-creds my-creds \:params {\:screen-name \\"AdamJWynne\\"} \:proxy \\"172.16.0.3\\")" "(users-show \:oauth-creds my-creds \:params {\:screen-name \\"AdamJWynne\\"} \:proxy \\"172.16.0.3\:8080\\")" "(users-show \:oauth-creds my-creds \:params {\:screen-name \\"AdamJWynne\\"} {\:proxy \\"172.16.0.3\:8080\\"})" "(users-show \:oauth-creds my-creds \:params {\:screen-name \\"AdamJWynne\\"} \:key {\:proxy \\"172.16.0.3\:8080\\"})" "(twitter.request/prepare-request-with-multi statuses-filter \\"/1.1/users/show.json?screen_name\=AdamJWynne\\" \:key [\:proxy \\"172.16.0.3\:8080\\"])" "(twitter.request/prepare-request-with-multi users-show \\"/1.1/users/show.json?screen_name\=AdamJWynne\\" \:key [\:proxy \\"172.16.0.3\:8080\\"])" "(use 'clojure.java.io)" "(with-open [rdr (reader \\"/doc/positive.json\\")]\\r\\n  (doseq [line (line-seq rdr)]\\r\\n    (println line)))" "(with-open [rdr (reader \\"/resources/positive.json\\")]\\r\\n  (doseq [line (line-seq rdr)]\\r\\n    (println line)))" "(with-open [rdr (reader \\"resources/positive.json\\")]\\r\\n  (doseq [line (line-seq rdr)]\\r\\n    (println line)))" "(with-open [rdr (reader \\"resources/positive.json\\")]\\r\\n  (doseq [line (line-seq rdr)]\\r\\n    (println \:text line)))" "(with-open [rdr (reader \\"resources/positive.json\\")]\\r\\n  (doseq [line (line-seq rdr)]\\r\\n    (read-str \:text line)))" "(use clojure.data.json)" "(use 'clojure.data.json)" "(with-open [rdr (reader \\"resources/positive.json\\")]\\r\\n  (doseq [line (line-seq rdr)]\\r\\n    (read-str \:text line)))" "(with-open [rdr (reader \\"resources/positive.json\\")]\\r\\n  (doseq [line (line-seq rdr)]\\r\\n    (read-str line)))" "(with-open [rdr (reader \\"resources/positive.json\\")]\\r\\n  (doseq [line (line-seq rdr)]\\r\\n    (\u0458json/read-str line)))" "(with-open [rdr (reader \\"resources/positive.json\\")]\\r\\n  (doseq [line (line-seq rdr)]\\r\\n    (json/read-str line)))" "(with-open [rdr (reader \\"resources/positive.json\\")]\\r\\n  (doseq [line (line-seq rdr)]\\r\\n    (read-str line)))" "(with-open [rdr (reader \\"resources/positive.json\\")]\\r\\n  (doseq [line (line-seq rdr)]\\r\\n    (json/read line)))" "(with-open [rdr (reader \\"resources/positive.json\\")]\\r\\n  (doseq [line (line-seq rdr)]\\r\\n    (read line)))" "(with-open [rdr (reader \\"resources/positive.json\\")]\\r\\n  (doseq [line (line-seq rdr)]\\r\\n    (read rdr)))" "(with-open [rdr (reader \\"resources/positive.json\\")]\\r\\n    (read rdr))" "(def positive (map \:text (json/read-str (slurp \\"resources/positive.json\\"))))" "(def positive (map \:text (read-str (slurp \\"resources/positive.json\\"))))" "positive" "(read-str (slurp \\"resources/positive.json\\"))" "(def positive (map lang (read-str (slurp \\"resources/positive.json\\"))))" "(slurp \\"resources/positive.json\\")" "(def positive (map lang (read-str (slurp \\"resources/positive.json\\") \:key-fn\: keyword)))" "(def positive (map lang (read-str (slurp \\"resources/positive.json\\") \:key-fn keyword)))" "(def positive (map \:lang (read-str (slurp \\"resources/positive.json\\") \:key-fn keyword)))" "positive" "(read-str (slurp \\"resources/positive.json\\") \:key-fn keyword)" "(def positive (map \:text (read-str (slurp \\"resources/positive.json\\") \:key-fn keyword)))" "positive" "(\:text positive)" "((read-str (slurp \\"resources/positive.json\\") \:key-fn keyword) \:text)" "(def positive (map (read-str (slurp \\"resources/positive.json\\") \:key-fn keyword) \:text))" "positive" "(def positive (map \:text (read-str (slurp \\"resources/positive.json\\") \:key-fn keyword)))" "(\:text positive)" "(with-open [rdr (reader \\"resources/positive.json\\")]\\r\\n  (doseq [line (line-seq rdr)]\\r\\n    (read-str line \:key-fn keyword)))" "(with-open [rdr (reader \\"resources/positive.json\\")]\\r\\n  (doseq [line (line-seq rdr)]\\r\\n    (println (read-str line \:key-fn keyword))))" "(with-open [rdr (reader \\"resources/positive.json\\")]\\r\\n  (doseq [line (line-seq rdr)]\\r\\n    (println \:text (read-str line \:key-fn keyword))))" "positive" "(with-open [rdr (reader \\"resources/positive.json\\")]\\r\\n  (doseq [line (line-seq rdr)]\\r\\n    (println (\:text (read-str line \:key-fn keyword)))))" "(with-open [rdr (reader \\"resources/positive.json\\")]\\r\\n  (doseq [line (line-seq rdr)]\\r\\n    (println (replace (\:text (read-str line \:key-fn keyword) \#\\"\:)\\" \\"\\")))))" "(with-open [rdr (reader \\"resources/positive.json\\")]\\r\\n  (doseq [line (line-seq rdr)]\\r\\n    (println (replace (\:text (read-str line \:key-fn keyword) \\"\:)\\" \\"\\")))))" "(with-open [rdr (reader \\"resources/positive.json\\")]\\r\\n  (doseq [line (line-seq rdr)]\\r\\n    (println (replace (\:text (read-str line \:key-fn keyword)) \\"\:)\\" \\"\\"))))" "(with-open [rdr (reader \\"resources/positive.json\\")]\\r\\n  (doseq [line (line-seq rdr)]\\r\\n    (println (replace (\:text (read-str line \:key-fn keyword)) \#\\"\:)\\" \\"\\"))))" "(with-open [rdr (reader \\"resources/positive.json\\")]\\r\\n  (doseq [line (line-seq rdr)]\\r\\n    (println (replace (\:text (read-str line \:key-fn keyword)) \#\\"[\:;\=B](')*(-)*[)(P3DSO*sp]+\\" \\"\\"))))" "(with-open [rdr (reader \\"resources/positive.json\\")]\\r\\n  (doseq [line (line-seq rdr)]\\r\\n    (println (replace (\:text (read-str line \:key-fn keyword)) \#\\"[\:;\=B](')*(-)*[)(P3DSO*sp]+\\" \\" \\"))))" "(with-open [rdr (reader \\"resources/positive.json\\")]\\r\\n  (doseq [line (line-seq rdr)]\\r\\n    (println (clojure.string/replace (\:text (read-str line \:key-fn keyword)) \#\\"[\:;\=B](')*(-)*[)(P3DSO*sp]+\\" \\" \\"))))" "(ns twitter-sentiment-analysis.data-collection\\r\\n  (\:use\\r\\n   ;;[twitter.oauth]\\r\\n   ;;[twitter.callbacks]\\r\\n   ;;[twitter.callbacks.handlers]\\r\\n   ;;[twitter.api.streaming]\\r\\n   [twitter callbacks oauth api utils request]\\r\\n   [clojure.java.io \:as io])\\r\\n  (\:require\\r\\n   [clojure.data.json \:as json]\\r\\n   [http.async.client \:as ac])\\r\\n  (\:import\\r\\n   (twitter.callbacks.protocols AsyncStreamingCallback)))" "(def my-creds (make-oauth-creds \\"nemg8aXAA9wtSo8h6ZZbrsYwk\\"\\r\\n                                \\"XmRZzVqlj1i5Ag72RJpd99IQLPoK6dfvbna89SwQ1NHIJVqN1l\\"\\r\\n                                \\"3021555060-YgU9ZHHTKVQGpkbDqNw9By4VNcFz7Ph455VAY0G\\"\\r\\n                                \\"XR9MZ3SE6quDbTfGcPNTiG2QebsdYnepStnZnrrzUeuye\\"))" "(prepare-request-with-multi statuses-filter {\:proxy \\"172.16.0.3\:8080\\"})" "(let [w (io/writer \\"positive.json\\")\\r\\n      callback (AsyncStreamingCallback.\\r\\n                 (fn [_resp payload]\\r\\n                   (.write w (-> (str payload) payload))\\r\\n                   (.write w \\"\\\\n\\"))\\r\\n                 (fn [_resp]\\r\\n                   (.close w))\\r\\n                 (fn [_resp ex]\\r\\n                   (.close w)\\r\\n                   (.printStackTrace ex)))]\\r\\n  (statuses-filter\\r\\n    \:params {\:lang \\"en\\" \:track \\"\:)\\"}\\r\\n    \:oauth-creds my-creds\\r\\n    \:callbacks callback))" "(ns twitter-sentiment-analysis.data-collection\\r\\n  (\:use\\r\\n   [twitter.oauth]\\r\\n   [twitter.callbacks]\\r\\n   [twitter.callbacks.handlers]\\r\\n   [twitter.api.streaming]\\r\\n   [twitter callbacks oauth api utils request]\\r\\n   [clojure.java.io \:as io])\\r\\n  (\:require\\r\\n   [clojure.data.json \:as json]\\r\\n   [http.async.client \:as ac])\\r\\n  (\:import\\r\\n   (twitter.callbacks.protocols AsyncStreamingCallback)))" "(def my-creds (make-oauth-creds \\"nemg8aXAA9wtSo8h6ZZbrsYwk\\"\\r\\n                                \\"XmRZzVqlj1i5Ag72RJpd99IQLPoK6dfvbna89SwQ1NHIJVqN1l\\"\\r\\n                                \\"3021555060-YgU9ZHHTKVQGpkbDqNw9By4VNcFz7Ph455VAY0G\\"\\r\\n                                \\"XR9MZ3SE6quDbTfGcPNTiG2QebsdYnepStnZnrrzUeuye\\"))" "(let [w (io/writer \\"positive.json\\")\\r\\n      callback (AsyncStreamingCallback.\\r\\n                 (fn [_resp payload]\\r\\n                   (.write w (-> (str payload) payload))\\r\\n                   (.write w \\"\\\\n\\"))\\r\\n                 (fn [_resp]\\r\\n                   (.close w))\\r\\n                 (fn [_resp ex]\\r\\n                   (.close w)\\r\\n                   (.printStackTrace ex)))]\\r\\n  (statuses-filter\\r\\n    \:params {\:lang \\"en\\" \:track \\"\:)\\"}\\r\\n    \:oauth-creds my-creds\\r\\n    \:callbacks callback))" "(def ^\:dynamic \\r\\n     *custom-streaming-callback* \\r\\n     (AsyncStreamingCallback. (comp println \#(\:text %) json/read-json \#(str %2)) \\r\\n                      (comp println response-return-everything)\\r\\n                  exception-print))\\r\\n\\r\\n(statuses-filter \:params {\:track \\"Borat\\"}\\r\\n         \:oauth-creds my-creds\\r\\n         \:callbacks *custom-streaming-callback*)" "(friendships-show \:oauth-creds my-creds \\r\\n                  \:params {\:target-screen-name \\"AdamJWynne\\"})" "(ns mynamespace\\r\\n  (\:use\\r\\n   [twitter.oauth]\\r\\n   [twitter.callbacks]\\r\\n   [twitter.callbacks.handlers]\\r\\n   [twitter.api.restful])\\r\\n  (\:import\\r\\n   (twitter.callbacks.protocols SyncSingleCallback)))" "(friendships-show \:oauth-creds my-creds \\r\\n                  \:params {\:target-screen-name \\"AdamJWynne\\"})" "(def my-creds (make-oauth-creds \\"nemg8aXAA9wtSo8h6ZZbrsYwk\\"\\r\\n                                \\"XmRZzVqlj1i5Ag72RJpd99IQLPoK6dfvbna89SwQ1NHIJVqN1l\\"\\r\\n                                \\"3021555060-YgU9ZHHTKVQGpkbDqNw9By4VNcFz7Ph455VAY0G\\"\\r\\n                                \\"XR9MZ3SE6quDbTfGcPNTiG2QebsdYnepStnZnrrzUeuye\\"))" "(friendships-show \:oauth-creds my-creds \\r\\n                  \:params {\:target-screen-name \\"AdamJWynne\\"})" "(defn strip-emoticons [text]\\r\\n  (clojure.string/replace text \#\\"[\:;\=B](')*(-)*[)(P3DSO*sp]+\\" \\"\\"))\\r\\n\\r\\n(defn strip-usernames [text]\\r\\n  (clojure.string/replace text \#\\"@\\\\\\\\w*\\" \\"USERNAME\\"))\\r\\n\\r\\n(defn strip-urls [text]\\r\\n  (clojure.string/replace text \#\\"http(s)*\://t.co/\\\\\\\\w*\\" \\"URL\\"))\\r\\n\\r\\n(defn strip-repeated-letters [text]\\r\\n  (clojure.string/replace text \#\\"http(s)*\://t.co/\\\\\\\\w*\\" \\"URL\\"))\\r\\n\\r\\n(defn reduce-features\\r\\n  (comp strip-repeated-letters strip-urls strip-usernames strip-emoticons))" "(defn strip-emoticons [text]\\r\\n  (clojure.string/replace text \#\\"[\:;\=B](')*(-)*[)(P3DSO*sp]+\\" \\"\\"))\\r\\n\\r\\n(defn strip-usernames [text]\\r\\n  (clojure.string/replace text \#\\"@\\\\\\\\w*\\" \\"USERNAME\\"))\\r\\n\\r\\n(defn strip-urls [text]\\r\\n  (clojure.string/replace text \#\\"http(s)*\://t.co/\\\\\\\\w*\\" \\"URL\\"))\\r\\n\\r\\n(defn strip-repeated-letters [text]\\r\\n  (clojure.string/replace text \#\\"http(s)*\://t.co/\\\\\\\\w*\\" \\"URL\\"))" "(with-open [rdr (reader \\"resources/positive.json\\")]\\r\\n  (doseq [line (line-seq rdr)]\\r\\n    (println ((comp strip-repeated-letters strip-urls strip-usernames strip-emoticons) (\:text (read-str line \:key-fn keyword))))))" "(ns twitter-sentiment-analysis.data-formatting\\r\\n  (\:use\\r\\n   [twitter.oauth]\\r\\n   [twitter.callbacks]\\r\\n   [twitter.callbacks.handlers]\\r\\n   [twitter.api.streaming]\\r\\n   [twitter callbacks oauth api utils request]\\r\\n   [clojure.java.io \:as io])\\r\\n  (\:require\\r\\n   [clojure.data.json \:as json]\\r\\n   [http.async.client \:as ac])\\r\\n  (\:import\\r\\n   (twitter.callbacks.protocols AsyncStreamingCallback)))" "(defn strip-emoticons [text]\\r\\n  (clojure.string/replace text \#\\"[\:;\=B](')*(-)*[)(P3DSO*sp]+\\" \\"\\"))\\r\\n\\r\\n(defn strip-usernames [text]\\r\\n  (clojure.string/replace text \#\\"@\\\\\\\\w*\\" \\"USERNAME\\"))\\r\\n\\r\\n(defn strip-urls [text]\\r\\n  (clojure.string/replace text \#\\"http(s)*\://t.co/\\\\\\\\w*\\" \\"URL\\"))\\r\\n\\r\\n(defn strip-repeated-letters [text]\\r\\n  (clojure.string/replace text \#\\"http(s)*\://t.co/\\\\\\\\w*\\" \\"URL\\"))" "(with-open [rdr (reader \\"resources/positive.json\\")]\\r\\n  (doseq [line (line-seq rdr)]\\r\\n    (println ((comp strip-repeated-letters strip-urls strip-usernames strip-emoticons) (\:text (read-str line \:key-fn keyword))))))" "(with-open [rdr (reader \\"resources/positive.json\\")]\\r\\n  (doseq [line (line-seq rdr)]\\r\\n    (println ((comp strip-repeated-letters strip-urls strip-usernames strip-emoticons) (\:text (read-string line \:key-fn keyword))))))" "(with-open [rdr (reader \\"resources/positive.json\\")]\\r\\n  (doseq [line (line-seq rdr)]\\r\\n    (println ((comp strip-repeated-letters strip-urls strip-usernames strip-emoticons) (\:text (json/read-str line \:key-fn keyword))))))" "(with-open [rdr (reader \\"resources/positive.json\\")]\\r\\n  (doseq [line (line-seq rdr)]\\r\\n    (println ((comp strip-repeated-letters strip-urls strip-usernames) (\:text (json/read-string line \:key-fn keyword))))))" "(with-open [rdr (reader \\"resources/positive.json\\")]\\r\\n  (doseq [line (line-seq rdr)]\\r\\n    (println ((comp strip-repeated-letters strip-urls strip-usernames) (\:text (json/read-str line \:key-fn keyword))))))" "(strip-repeated-letters \\"aaaaa\\")" "(defn strip-repeated-letters [text]\\r\\n  (clojure.string/replace text \#\\"([a-zA-Z\u0420\u00B0-\u0421\u20AC\u0420\u0452-\u0420\u0401])\\\\\\\\1+\\" \\"$1$1\\"))" "(defn strip-repeated-letters [text]\\r\\n  (clojure.string/replace text \#\\"([a-zA-Z])\\\\\\\\1+\\" \\"$1$1\\"))" "(strip-repeated-letters \\"aaaaa\\")" "(strip-usernames \\"@dunja je car\\")" "(strip-emoticons \\"\:(\\")" "(strip-emoticons \\"\:( aaa\\")" "(defn strip-usernames [text]\\r\\n  (clojure.string/replace text \#\\"@\\\\\\\\w*\\" \\"\\"))" "(strip-usernames \\"@dunja je car\\")" "(re-matches \#\\"@\\\\\\\\w*\\" \\"@dunja je car\\")" "(re-matches \#\\"@\\\\\\\\\\\\\\\\w*\\" \\"@dunja je car\\")" "(re-matches \#\\"@\\\\\\\\\\\\w*\\" \\"@dunja je car\\")" "(re-matches \#\\"@w*\\" \\"@dunja je car\\")" "(re-matches \#\\"\\\\\\\\@\\\\\\\\w*\\" \\"@dunja je car\\")" "(re-matches \#\\"@\\\\w*\\" \\"@dunja je car\\")" "(re-matches \#\\"@\\" \\"@dunja je car\\")" "(re-matches \#\\"w*\\" \\"@dunja je car\\")" "(clojure.string/replace \\"@dunja je car\\" \\"\\\\\\\\@[A-Za-z0-9_]\\" \\" \\")" "(clojure.string/replace \\"@dunja je car\\" \\"[A-Za-z0-9_]\\" \\" \\")" "(clojure.string/replace \\"@dunja je car\\" \#\\"[A-Za-z0-9_]\\" \\" \\")" "(clojure.string/replace \\"@dunja je car\\" \#\\"@[A-Za-z0-9_]\\" \\" \\")" "(clojure.string/replace \\"@dunja je car\\" \#\\"@[A-Za-z0-9_]*\\" \\" \\")" "(clojure.string/replace \\"@dunja je car\\" \#\\"@[A-Za-z0-9_]*\\" \\"USERNAME\\")" "(clojure.string/replace \\"http//.\\" \#\\"http(s)*\://t.co/[A-Za-z0-9_]*\\" \\"URL\\")" "(clojure.string/replace \\"http\://t.co/hzTq0sDXnx\\" \#\\"http(s)*\://t.co/[A-Za-z0-9_]*\\" \\"URL\\")" "(clojure.string/replace \\"aaaa\\" \#\\"([a-zA-Z])\\\\\\\\1+\\" \\"\\\\\\\\$1\\\\\\\\$1\\")" "(clojure.string/replace \\"aaaa\\" \#\\"([a-zA-Z])\\\\\\\\\\\\\\\\1+\\" \\"\\\\\\\\$1\\\\\\\\$1\\")" "(clojure.string/replace \\"aaaa\\" \#\\"([a-zA-Z])\\\\\\\\\\\\1+\\" \\"\\\\\\\\$1\\\\\\\\$1\\")" "(clojure.string/replace \\"aaaa\\" \#\\"([a-zA-Z])\\\\\\\\1+\\" \\"b\\")" "(clojure.string/replace \\"aaaa\\" \#\\"([a-zA-Z])1+\\" \\"b\\")" "(clojure.string/replace \\"aaaa\\" \#\\"([a-zA-Z])\\\\\\\\1+\\" \#\\"b\\")" "(clojure.string/replace \\"aaaa jebem ti\\" \#\\"([a-zA-Z])\\\\\\\\1+\\" \#\\"b\\")" "(clojure.string/replace \\"aaaa jebem ti\\" \#\\"([a-zA-Z])\\\\1+\\" \\"\\\\\\\\$1\\\\\\\\$1\\")" "(clojure.string/replace \\"aaaa jebem ti\\" \#\\"([a-zA-Z])\\\\1+\\" \\"$1$1\\")" "(defn strip-repeated-letters [text]\\r\\n  (clojure.string/replace text \#\\"([a-zA-Z])1+\\" \\"$1$1\\"))" "(strip-repeated-letters \\"aaaabba\\")" "(defn strip-repeated-letters [text]\\r\\n  (clojure.string/replace text \#\\"([a-zA-Z])\\\\1+\\" \\"$1$1\\"))" "(strip-repeated-letters \\"aaaabba\\")" "(ns twitter-sentiment-analysis.data_formatting\\r\\n  (\:use\\r\\n   [twitter.oauth]\\r\\n   [twitter.callbacks]\\r\\n   [twitter.callbacks.handlers]\\r\\n   [twitter.api.streaming]\\r\\n   [twitter callbacks oauth api utils request]\\r\\n   [clojure.java.io \:as io])\\r\\n  (\:require\\r\\n   [clojure.data.json \:as json]\\r\\n   [http.async.client \:as ac])\\r\\n  (\:import\\r\\n   (twitter.callbacks.protocols AsyncStreamingCallback)))" "(defn strip-emoticons [text]\\r\\n  (clojure.string/replace text \#\\"[\:;\=B](')*(-)*[)(P3DSO*sp]+\\" \\"\\"))\\r\\n\\r\\n(defn strip-usernames [text]\\r\\n  (clojure.string/replace text \#\\"@[A-Za-z0-9_]*\\" \\"USERNAME\\"))\\r\\n\\r\\n(defn strip-urls [text]\\r\\n  (clojure.string/replace text \#\\"http(s)*\://t.co/[A-Za-z0-9_]*\\" \\"URL\\"))\\r\\n\\r\\n(defn strip-repeated-letters [text]\\r\\n  (clojure.string/replace text \#\\"([a-zA-Z])\\\\1+\\" \\"$1$1\\"))" "(with-open [rdr (reader \\"resources/positive.json\\")]\\r\\n  (doseq [line (line-seq rdr)]\\r\\n    (println ((comp strip-repeated-letters strip-urls strip-usernames strip-emoticons) (\:text (read-str line \:key-fn keyword))))))" "(with-open [rdr (reader \\"resources/positive.json\\")]\\r\\n  (doseq [line (line-seq rdr)]\\r\\n    (println ((comp strip-repeated-letters strip-urls strip-usernames strip-emoticons) (\:text (json/read-str line \:key-fn keyword))))))" "(defn reduce-features [path]\\r\\n  (with-open [rdr (reader [path])]\\r\\n  (doseq [line (line-seq rdr)]\\r\\n    (println ((comp strip-repeated-letters strip-urls strip-usernames strip-emoticons) (\:text (json/read-str line \:key-fn keyword)))))))" "(reduce-features \\"resources/positive.json\\")" "(defn reduce-features [path]\\r\\n  (with-open [rdr (reader path)]\\r\\n  (doseq [line (line-seq rdr)]\\r\\n    (println ((comp strip-repeated-letters strip-urls strip-usernames strip-emoticons) (\:text (json/read-str line \:key-fn keyword)))))))" "(reduce-features \\"resources/positive.json\\")" "(statuses-filter\\r\\n    \:params {\:lang \\"en\\" \:track \\"\:)\\"}\\r\\n    \:oauth-creds my-creds)" "(ns twitter-sentiment-analysis.data-collection\\r\\n  (\:use\\r\\n   [twitter.oauth]\\r\\n   [twitter.callbacks]\\r\\n   [twitter.callbacks.handlers]\\r\\n   [twitter.api.streaming]\\r\\n   [twitter callbacks oauth api utils request]\\r\\n   [clojure.java.io \:as io])\\r\\n  (\:require\\r\\n   [clojure.data.json \:as json]\\r\\n   [http.async.client \:as ac])\\r\\n  (\:import\\r\\n   (twitter.callbacks.protocols AsyncStreamingCallback)))" "(def my-creds (make-oauth-creds \\"nemg8aXAA9wtSo8h6ZZbrsYwk\\"\\r\\n                                \\"XmRZzVqlj1i5Ag72RJpd99IQLPoK6dfvbna89SwQ1NHIJVqN1l\\"\\r\\n                                \\"3021555060-YgU9ZHHTKVQGpkbDqNw9By4VNcFz7Ph455VAY0G\\"\\r\\n                                \\"XR9MZ3SE6quDbTfGcPNTiG2QebsdYnepStnZnrrzUeuye\\"))" "(statuses-filter\\r\\n    \:params {\:lang \\"en\\" \:track \\"\:)\\"}\\r\\n    \:oauth-creds my-creds)" "(ns twitter-sentiment-analysis.data-collection\\r\\n  (\:use\\r\\n   [twitter.oauth]\\r\\n   [twitter.callbacks]\\r\\n   [twitter.callbacks.handlers]\\r\\n   [twitter.api.streaming]\\r\\n   [twitter callbacks oauth api utils request]\\r\\n   [clojure.java.io \:as io])\\r\\n  (\:require\\r\\n   [clojure.data.json \:as json]\\r\\n   [http.async.client \:as ac])\\r\\n  (\:import\\r\\n   (twitter.callbacks.protocols AsyncStreamingCallback)))" "(def my-creds (make-oauth-creds \\"nemg8aXAA9wtSo8h6ZZbrsYwk\\"\\r\\n                                \\"XmRZzVqlj1i5Ag72RJpd99IQLPoK6dfvbna89SwQ1NHIJVqN1l\\"\\r\\n                                \\"3021555060-YgU9ZHHTKVQGpkbDqNw9By4VNcFz7Ph455VAY0G\\"\\r\\n                                \\"XR9MZ3SE6quDbTfGcPNTiG2QebsdYnepStnZnrrzUeuye\\"))" "  (statuses-filter\\r\\n    \:params {\:lang \\"en\\" \:track \\"\:)\\"}\\r\\n    \:oauth-creds my-creds\\r\\n    \:callbacks response-return-everything)" "(ns twitter-sentiment-analysis.data-collection\\r\\n  (\:use\\r\\n   [twitter.oauth]\\r\\n   [twitter.callbacks]\\r\\n   [twitter.callbacks.handlers]\\r\\n   [twitter.api.streaming]\\r\\n   [twitter callbacks oauth api utils request]\\n   [twitter.callbacks.handlers]\\r\\n   [clojure.java.io \:as io])\\r\\n  (\:require\\r\\n   [clojure.data.json \:as json]\\r\\n   [http.async.client \:as ac])\\r\\n  (\:import\\r\\n   (twitter.callbacks.protocols AsyncStreamingCallback)))" "(def my-creds (make-oauth-creds \\"nemg8aXAA9wtSo8h6ZZbrsYwk\\"\\r\\n                                \\"XmRZzVqlj1i5Ag72RJpd99IQLPoK6dfvbna89SwQ1NHIJVqN1l\\"\\r\\n                                \\"3021555060-YgU9ZHHTKVQGpkbDqNw9By4VNcFz7Ph455VAY0G\\"\\r\\n                                \\"XR9MZ3SE6quDbTfGcPNTiG2QebsdYnepStnZnrrzUeuye\\"))" "  (statuses-filter\\r\\n    \:params {\:lang \\"en\\" \:track \\"\:)\\"}\\r\\n    \:oauth-creds my-creds\\r\\n    \:callbacks response-return-everything)" "  (search-tweets\\r\\n    \:params {\:lang \\"en\\" \:track \\"\:)\\"}\\r\\n    \:oauth-creds my-creds)" "\\r\\n(ns twitter.api.test.search\\r\\n  (\:use\\r\\n   [clojure.test]\\r\\n   [twitter.test-utils.core]\\r\\n   [twitter.test.creds]\\r\\n   [twitter.test.utils]\\r\\n   [twitter.callbacks]\\r\\n   [twitter.api.search]))" "(defn test-search\\r\\n  (is-200 search \:params {\:q \\"sports\\"}))" "(deftest test-search\\r\\n  (is-200 search \:params {\:q \\"sports\\"}))" "(ns twitter-sentiment-analysis.data-collection\\r\\n  (\:use\\r\\n   [twitter.oauth]\\r\\n   [twitter.callbacks]\\r\\n   [twitter.callbacks.handlers]\\r\\n   [twitter.api.streaming]\\r\\n   [twitter callbacks oauth api utils request]\\r\\n   [clojure.java.io \:as io])\\r\\n  (\:require\\r\\n   [clojure.data.json \:as json]\\r\\n   [http.async.client \:as ac])\\r\\n  (\:import\\r\\n   (twitter.callbacks.protocols AsyncStreamingCallback)))" "(let [w (io/writer \\"mary.txt\\")\\r\\n      callback (AsyncStreamingCallback.\\r\\n                 (fn [_resp payload]\\r\\n                   (.write w (-> (str payload) json/read-json \:text))\\r\\n                   (.write w \\"\\\\n\\"))\\r\\n                 (fn [_resp]\\r\\n                   (.close w))\\r\\n                 (fn [_resp ex]\\r\\n                   (.close w)\\r\\n                   (.printStackTrace ex)))]\\r\\n  (statuses-filter\\r\\n    \:params {\:track \\"mary rose\\"}\\r\\n    \:oauth-creds my-creds\\r\\n    \:callbacks callback))" "(let [w (io/writer \\"mary.txt\\")\\r\\n      callback (AsyncStreamingCallback.\\r\\n                 (fn [_resp payload]\\r\\n                   (.write w (-> (str payload) json/read-json \:text))\\r\\n                   (.write w \\"\\\\n\\"))\\r\\n                 (fn [_resp]\\r\\n                   (.close w))\\r\\n                 (fn [_resp ex]\\r\\n                   (.close w)\\r\\n                   (.printStackTrace ex)))]\\r\\n  (statuses-filter\\r\\n    \:params {\:track \\"\:)\\"}\\r\\n    \:oauth-creds my-creds\\r\\n    \:callbacks callback))" "(let [w (io/writer \\"mary.txt\\")\\r\\n      callback (AsyncStreamingCallback.\\r\\n                 (fn [_resp payload]\\r\\n                   (.write w (-> (str payload) (if-not (clojure.string/blank? \:text)json/read-json \:text)))\\r\\n                   (.write w \\"\\\\n\\"))\\r\\n                 (fn [_resp]\\r\\n                   (.close w))\\r\\n                 (fn [_resp ex]\\r\\n                   (.close w)\\r\\n                   (.printStackTrace ex)))]\\r\\n  (statuses-filter\\r\\n    \:params {\:track \\"\:)\\"}\\r\\n    \:oauth-creds my-creds\\r\\n    \:callbacks callback))" "(let [w (io/writer \\"mary.txt\\")\\r\\n      callback (AsyncStreamingCallback.\\r\\n                 (fn [_resp payload]\\r\\n                   (.write w (-> (str payload) (if-not (clojure.string/blank? \:text) (json/read-json \:text) {})))\\r\\n                   (.write w \\"\\\\n\\"))\\r\\n                 (fn [_resp]\\r\\n                   (.close w))\\r\\n                 (fn [_resp ex]\\r\\n                   (.close w)\\r\\n                   (.printStackTrace ex)))]\\r\\n  (statuses-filter\\r\\n    \:params {\:track \\"\:)\\"}\\r\\n    \:oauth-creds my-creds\\r\\n    \:callbacks callback))" "(let [w (io/writer \\"mary.txt\\")\\r\\n      callback (AsyncStreamingCallback.\\r\\n                 (fn [_resp payload]\\r\\n                   (.write w (-> (str payload) (if-not (clojure.string/blank? \:text payload)\\r\\n   (json/read-str \:text payload)\\r\\n   {}))))\\r\\n                   (.write w \\"\\\\n\\"))\\r\\n                 (fn [_resp]\\r\\n                   (.close w))\\r\\n                 (fn [_resp ex]\\r\\n                   (.close w)\\r\\n                   (.printStackTrace ex)))]\\r\\n  (statuses-filter\\r\\n    \:params {\:track \\"\:)\\"}\\r\\n    \:oauth-creds my-creds\\r\\n    \:callbacks callback))" "(let [w (io/writer \\"mary.txt\\")\\r\\n      callback (AsyncStreamingCallback.\\r\\n                 (fn [_resp payload]\\r\\n                   (.write w (-> (str payload) (if-not (clojure.string/blank? \:text payload)\\r\\n   (json/read-str \:text payload)\\r\\n   {}))))\\r\\n                   (.write w \\"\\\\n\\"))\\r\\n                 (fn [_resp]\\r\\n                   (.close w))\\r\\n                 (fn [_resp ex]\\r\\n                   (.close w)\\r\\n                   (.printStackTrace ex))]\\r\\n  (statuses-filter\\r\\n    \:params {\:track \\"\:)\\"}\\r\\n    \:oauth-creds my-creds\\r\\n    \:callbacks callback))" "(let [w (io/writer \\"mary.txt\\")\\r\\n      callback (AsyncStreamingCallback.\\r\\n                 (fn [_resp payload]\\r\\n                   (.write w (-> (str payload) (if-not (clojure.string/blank? \:text payload)\\r\\n   (json/read-str \:text payload)\\r\\n   {}))))\\r\\n                   (.write w \\"\\\\n\\"))\\r\\n                 ]\\r\\n  (statuses-filter\\r\\n    \:params {\:track \\"\:)\\"}\\r\\n    \:oauth-creds my-creds\\r\\n    \:callbacks callback))" "(def ^\:dynamic *custom-streaming-callback* \\r\\n  (AsyncStreamingCallback. (if-not (clojure.string/blank? %)\\r\\n   (json/read-str % \:key-fn keyword)\\r\\n   {}) \\r\\n                           (comp println response-return-everything)\\r\\n                           exception-print))" "(def ^\:dynamic *custom-streaming-callback* \\r\\n  (AsyncStreamingCallback. (comp println \#(\:text %) json/read-json \#(str %2)) \\r\\n                           (comp println response-return-everything)\\r\\n                           exception-print))" "(defn start-filtering []\\r\\n  (statuses-filter \:params {\:q \\"\:)\\"}\\r\\n                   \:oauth-creds *creds*\\r\\n                   \:callbacks *custom-streaming-callback*))" "(defn start-filtering []\\r\\n  (statuses-filter \:params {\:q \\"\:)\\"}\\r\\n                   \:oauth-creds my-creds\\r\\n                   \:callbacks *custom-streaming-callback*))" "(start-filtering)" "(def ^\:dynamic *custom-streaming-callback* \\r\\n  (AsyncStreamingCallback. (comp println \#(\:text %) json/read-str \#(str %2)) \\r\\n                           (comp println response-return-everything)\\r\\n                           exception-print))" "(defn start-filtering []\\r\\n  (statuses-filter \:params {\:q \\"\:)\\"}\\r\\n                   \:oauth-creds my-creds\\r\\n                   \:callbacks *custom-streaming-callback*))" "(start-filtering)" "(def ^\:dynamic *custom-streaming-callback* \\r\\n  (AsyncStreamingCallback. (comp println \#(\:text %) \#(str %2)) \\r\\n                           (comp println response-return-everything)\\r\\n                           exception-print))" "(defn start-filtering []\\r\\n  (statuses-filter \:params {\:q \\"\:)\\"}\\r\\n                   \:oauth-creds my-creds\\r\\n                   \:callbacks *custom-streaming-callback*))" "(start-filtering)" "(def ^\:dynamic *custom-streaming-callback* \\r\\n  (AsyncStreamingCallback. (comp println response-return-everything) \\r\\n                           (comp println response-return-everything)\\r\\n                           exception-print))" "(defn start-filtering []\\r\\n  (statuses-filter \:params {\:q \\"\:)\\"}\\r\\n                   \:oauth-creds my-creds\\r\\n                   \:callbacks *custom-streaming-callback*))" "(start-filtering)" "(defn start-filtering []\\r\\n  (statuses-filter \:params {\:track \\"\:)\\"}\\r\\n                   \:oauth-creds my-creds\\r\\n                   \:callbacks *custom-streaming-callback*))" "(start-filtering)" "(ns mynamespace\\r\\n  (\:use\\r\\n   [twitter.oauth]\\r\\n   [twitter.callbacks]\\r\\n   [twitter.callbacks.handlers]\\r\\n   [twitter.api.streaming])\\r\\n  (\:require\\r\\n   [clojure.data.json \:as json]\\r\\n   [http.async.client \:as ac]\\r\\n   [clojure.java.io \:as io])\\r\\n  (\:import\\r\\n   (twitter.callbacks.protocols AsyncStreamingCallback)))" "(def my-creds (make-oauth-creds \\"nemg8aXAA9wtSo8h6ZZbrsYwk\\"\\r\\n                                \\"XmRZzVqlj1i5Ag72RJpd99IQLPoK6dfvbna89SwQ1NHIJVqN1l\\"\\r\\n                                \\"3021555060-YgU9ZHHTKVQGpkbDqNw9By4VNcFz7Ph455VAY0G\\"\\r\\n                                \\"XR9MZ3SE6quDbTfGcPNTiG2QebsdYnepStnZnrrzUeuye\\"))" "(def ^\:dynamic \\r\\n     *custom-streaming-callback* \\r\\n     (AsyncStreamingCallback. (comp println \#(\:text %) json/read-json \#(str %2)) \\r\\n                     (comp println response-return-everything)\\r\\n              exception-print))" "(statuses-filter \:params {\:track \\"mary rose\\"}\\r\\n     \:oauth-creds my-creds\\r\\n     \:callbacks *custom-streaming-callback*)" "(def mary (statuses-filter \:params {\:track \\"mary rose\\"}\\r\\n     \:oauth-creds my-creds\\r\\n     \:callbacks *custom-streaming-callback*))" "(with-open [r @(\:body (statuses-filter \:params {\:track \\"mary rose\\"}\\r\\n    \:oauth-creds my-creds\\r\\n    \:callbacks *custom-streaming-callback*))\\r\\n            w (io/writer \\"mary.txt\\")]\\r\\n  (dosync (.write w (str r \\"\\\\n\\"))))" "(ns foo\\r\\n  (\:require [twitter-streaming-client.core \:as client]\\r\\n            [twitter.oauth \:as oauth]))" "(ns twitter-sentiment-analysis.data-collection\\r\\n  (\:use\\r\\n   [twitter.oauth]\\r\\n   [twitter.callbacks]\\r\\n   [twitter.callbacks.handlers]\\r\\n   [twitter.api.streaming]\\r\\n   [twitter callbacks oauth api utils request]\\r\\n   [clojure.java.io \:as io])\\r\\n  (\:require\\r\\n   [clojure.data.json \:as json]\\r\\n   [http.async.client \:as ac])\\r\\n  (\:import\\r\\n   (twitter.callbacks.protocols AsyncStreamingCallback)))" "(def my-creds (make-oauth-creds \\"nemg8aXAA9wtSo8h6ZZbrsYwk\\"\\r\\n                                \\"XmRZzVqlj1i5Ag72RJpd99IQLPoK6dfvbna89SwQ1NHIJVqN1l\\"\\r\\n                                \\"3021555060-YgU9ZHHTKVQGpkbDqNw9By4VNcFz7Ph455VAY0G\\"\\r\\n                                \\"XR9MZ3SE6quDbTfGcPNTiG2QebsdYnepStnZnrrzUeuye\\"))" "(def ^\:dynamic *custom-streaming-callback* \\r\\n  (AsyncStreamingCallback. (comp println \#(\:text %) ( if (clojure.string/blank? \#(str %2) {} json/read-json \#(str %2)))) \\r\\n                           (comp println response-return-everything)\\r\\n                           exception-print))" "(def ^\:dynamic *custom-streaming-callback* \\r\\n  (AsyncStreamingCallback. (comp println \#(\:text %) (if (clojure.string/blank? \#(str %2) ({}) (json/read-json \#(str %2))))) \\r\\n                           (comp println response-return-everything)\\r\\n                           exception-print))" "(def ^\:dynamic *custom-streaming-callback* \\r\\n  (AsyncStreamingCallback. (comp println \#(\:text %) (if (clojure.string/blank? \#(str %2)) {} (json/read-json \#(str %2)))) \\r\\n                           (comp println response-return-everything)\\r\\n                           exception-print))" "(def ^\:dynamic *custom-streaming-callback* \\r\\n  (AsyncStreamingCallback. (comp println \#(\:text %) (if (clojure.string/blank? \#(str %2)) {} json/read-json \#(str %2))) \\r\\n                           (comp println response-return-everything)\\r\\n                           exception-print))" "(def ^\:dynamic *custom-streaming-callback* \\r\\n  (AsyncStreamingCallback. (comp println \#(\:text %) (if (clojure.string/blank? (str %2)) {} (json/read-json \#(str %2)))) \\r\\n                           (comp println response-return-everything)\\r\\n                           exception-print))" "(def ^\:dynamic *custom-streaming-callback* \\r\\n  (AsyncStreamingCallback. (comp println \#(\:text %) (if (clojure.string/blank? \#(str %2)) {} (json/read-json \#(str %2)))) \\r\\n                           (comp println response-return-everything)\\r\\n                           exception-print))" "(def ^\:dynamic *custom-streaming-callback* \\r\\n  (AsyncStreamingCallback. (comp println \#(\:text %) (if (clojure.string/blank? \#(str %2)) {} (json/read-str \#(str %2)))) \\r\\n                           (comp println response-return-everything)\\r\\n                           exception-print))" "(def ^\:dynamic *custom-streaming-callback* \\r\\n  (AsyncStreamingCallback. (comp println \#(\:text %) (if (clojure.string/blank? \#(str %2)) {} \#(str %2))) \\r\\n                           (comp println response-return-everything)\\r\\n                           exception-print))" "(def ^\:dynamic *custom-streaming-callback* \\r\\n  (AsyncStreamingCallback. (comp println \#(\:text %) (if (clojure.string/blank? \#(str %2)) {} \\"aaaaa\\")) \\r\\n                           (comp println response-return-everything)\\r\\n                           exception-print))" "(def ^\:dynamic *custom-streaming-callback* \\r\\n  (AsyncStreamingCallback. (comp println \#(\:text %) (if (clojure.string/blank? \#(str %2)) \\"bbbb\\" \\"aaaaa\\")) \\r\\n                           (comp println response-return-everything)\\r\\n                           exception-print))" "(def ^\:dynamic *custom-streaming-callback* \\r\\n  (AsyncStreamingCallback. (comp println \#(\:text %) (if (true) \\"bbbb\\" \\"aaaaa\\")) \\r\\n                           (comp println response-return-everything)\\r\\n                           exception-print))" "(def ^\:dynamic *custom-streaming-callback* \\r\\n  (AsyncStreamingCallback. (comp println \#(\:text %) (if (nil) \\"bbbb\\" \\"aaaaa\\")) \\r\\n                           (comp println response-return-everything)\\r\\n                           exception-print))" "(def ^\:dynamic *custom-streaming-callback* \\r\\n  (AsyncStreamingCallback. (comp println \#(\:text %) (if (nil? {}) \\"bbbb\\" \\"aaaaa\\")) \\r\\n                           (comp println response-return-everything)\\r\\n                           exception-print))" "(defn start-filtering []\\r\\n  (statuses-filter \:params {\:follow 12345}\\r\\n                   \:oauth-creds *creds*\\r\\n                   \:callbacks *custom-streaming-callback*))" "(defn start-filtering []\\r\\n  (statuses-filter \:params {\:follow 12345}\\r\\n                   \:oauth-creds my-creds\\r\\n                   \:callbacks *custom-streaming-callback*))" "(start-filtering)" "(def ^\:dynamic *custom-streaming-callback* \\r\\n  (AsyncStreamingCallback. (comp println \#(\:text %) (if (nil? {}) (str) (str \\"false\\"))) \\r\\n                           (comp println response-return-everything)\\r\\n                           exception-print))" "(defn start-filtering []\\r\\n  (statuses-filter \:params {\:follow 12345}\\r\\n                   \:oauth-creds my-creds\\r\\n                   \:callbacks *custom-streaming-callback*))" "(start-filtering)" "(def ^\:dynamic *custom-streaming-callback* \\r\\n  (AsyncStreamingCallback. (comp println \#(\:text %)) \\r\\n                           (comp println response-return-everything)\\r\\n                           exception-print))" "(defn start-filtering []\\r\\n  (statuses-filter \:params {\:follow 12345}\\r\\n                   \:oauth-creds my-creds\\r\\n                   \:callbacks *custom-streaming-callback*))" "(start-filtering)" "(def ^\:dynamic *custom-streaming-callback* \\r\\n  (AsyncStreamingCallback. (comp println \#(\:text %) (str %2)) \\r\\n                           (comp println response-return-everything)\\r\\n                           exception-print))" "(def ^\:dynamic *custom-streaming-callback* \\r\\n  (AsyncStreamingCallback. (comp println \#(\:text %) \#(str %2)) \\r\\n                           (comp println response-return-everything)\\r\\n                           exception-print))" "(defn start-filtering []\\r\\n  (statuses-filter \:params {\:follow 12345}\\r\\n                   \:oauth-creds my-creds\\r\\n                   \:callbacks *custom-streaming-callback*))" "(start-filtering)" "(def ^\:dynamic *custom-streaming-callback* \\r\\n  (AsyncStreamingCallback. (comp println \#(\:text %) json/read-str \#(str %2)) \\r\\n                           (comp println response-return-everything)\\r\\n                           exception-print))" "(ns twitter-sentiment-analysis.data-collection\\r\\n  (\:use\\r\\n   [twitter.oauth]\\r\\n   [twitter.callbacks]\\r\\n   [twitter.callbacks.handlers]\\r\\n   [twitter.api.streaming]\\r\\n   [twitter callbacks oauth api utils request]\\r\\n   [clojure.java.io \:as io])\\r\\n  (\:require\\r\\n   [clojure.data.json \:as json]\\r\\n   [http.async.client \:as ac])\\r\\n  (\:import\\r\\n   (twitter.callbacks.protocols AsyncStreamingCallback)))" "(def my-creds (make-oauth-creds \\"nemg8aXAA9wtSo8h6ZZbrsYwk\\"\\r\\n                                \\"XmRZzVqlj1i5Ag72RJpd99IQLPoK6dfvbna89SwQ1NHIJVqN1l\\"\\r\\n                                \\"3021555060-YgU9ZHHTKVQGpkbDqNw9By4VNcFz7Ph455VAY0G\\"\\r\\n                                \\"XR9MZ3SE6quDbTfGcPNTiG2QebsdYnepStnZnrrzUeuye\\"))" "(def ^\:dynamic *custom-streaming-callback* \\r\\n  (AsyncStreamingCallback. (comp println \#(\:text %) json/read-str \#(str %2)) \\r\\n                           (comp println response-return-everything)\\r\\n                           exception-print))" "(defn start-filtering []\\r\\n  (statuses-filter \:params {\:follow 12345}\\r\\n                   \:oauth-creds my-creds\\r\\n                   \:callbacks *custom-streaming-callback*))" "(start-filtering)" "(def ^\:dynamic *custom-streaming-callback* \\r\\n  (AsyncStreamingCallback. (comp println \#(str %2)) \\r\\n                           (comp println response-return-everything)\\r\\n                           exception-print))" "(defn start-filtering []\\r\\n  (statuses-filter \:params {\:follow 12345}\\r\\n                   \:oauth-creds my-creds\\r\\n                   \:callbacks *custom-streaming-callback*))" "(start-filtering)" "(let [w (io/writer \\"positive.json\\")\\r\\n      callback (AsyncStreamingCallback.\\r\\n                 (fn [_resp payload]\\r\\n                   (.write w (-> (str payload) payload))\\r\\n                   (.write w \\"\\\\n\\"))\\r\\n                 (fn [_resp]\\r\\n                   (.close w))\\r\\n                 (fn [_resp ex]\\r\\n                   (.close w)\\r\\n                   (.printStackTrace ex)))]\\r\\n  (statuses-filter\\r\\n    \:params {\:lang \\"en\\" \:track \\"\:)\\"}\\r\\n    \:oauth-creds my-creds\\r\\n    \:callbacks callback))" "(ns twitter-sentiment-analysis.data-collection\\r\\n  (\:use\\r\\n   [twitter.oauth]\\r\\n   [twitter.callbacks]\\r\\n   [twitter.callbacks.handlers]\\r\\n   [twitter.api.streaming]\\r\\n   [twitter callbacks oauth api utils request]\\r\\n   [clojure.java.io \:as io])\\r\\n  (\:require\\r\\n   [clojure.data.json \:as json]\\r\\n   [http.async.client \:as ac])\\r\\n  (\:import\\r\\n   (twitter.callbacks.protocols AsyncStreamingCallback)))\\r\\n\\r\\n(def my-creds (make-oauth-creds \\"nemg8aXAA9wtSo8h6ZZbrsYwk\\"\\r\\n                                \\"XmRZzVqlj1i5Ag72RJpd99IQLPoK6dfvbna89SwQ1NHIJVqN1l\\"\\r\\n                                \\"3021555060-YgU9ZHHTKVQGpkbDqNw9By4VNcFz7Ph455VAY0G\\"\\r\\n                                \\"XR9MZ3SE6quDbTfGcPNTiG2QebsdYnepStnZnrrzUeuye\\"))" "(let [w (io/writer \\"positive.json\\")\\r\\n      callback (AsyncStreamingCallback.\\r\\n                 (fn [_resp payload]\\r\\n                   (.write w (-> (str payload) payload))\\r\\n                   (.write w \\"\\\\n\\"))\\r\\n                 (fn [_resp]\\r\\n                   (.close w))\\r\\n                 (fn [_resp ex]\\r\\n                   (.close w)\\r\\n                   (.printStackTrace ex)))]\\r\\n  (statuses-filter\\r\\n    \:params {\:lang \\"en\\" \:track \\"\:)\\"}\\r\\n    \:oauth-creds my-creds\\r\\n    \:callbacks callback))" "(let [w (io/writer \\"positive.json\\")\\r\\n      callback (AsyncStreamingCallback.\\r\\n                 (fn [_resp payload]\\r\\n                   (.write w (-> (str payload) json/read \:text))\\r\\n                   (.write w \\"\\\\n\\"))\\r\\n                 (fn [_resp]\\r\\n                   (.close w))\\r\\n                 (fn [_resp ex]\\r\\n                   (.close w)\\r\\n                   (.printStackTrace ex)))]\\r\\n  (statuses-filter\\r\\n    \:params {\:lang \\"en\\" \:track \\"\:)\\"}\\r\\n    \:oauth-creds my-creds\\r\\n    \:callbacks callback))" "(let [w (io/writer \\"positive.json\\")\\r\\n      callback (AsyncStreamingCallback.\\r\\n                 (fn [_resp payload]\\r\\n                   (.write w (-> (str payload) json/read-str \:text))\\r\\n                   (.write w \\"\\\\n\\"))\\r\\n                 (fn [_resp]\\r\\n                   (.close w))\\r\\n                 (fn [_resp ex]\\r\\n                   (.close w)\\r\\n                   (.printStackTrace ex)))]\\r\\n  (statuses-filter\\r\\n    \:params {\:lang \\"en\\" \:track \\"\:)\\"}\\r\\n    \:oauth-creds my-creds\\r\\n    \:callbacks callback))" "(let [w (io/writer \\"positive.json\\")\\r\\n      callback (AsyncStreamingCallback.\\r\\n                 (fn [_resp payload]\\n                   (if (\:text payload)\\r\\n                   (.write w (-> (str payload) json/read-str \:text))\\r\\n                   (.write w \\"\\\\n\\")))\\r\\n                 (fn [_resp]\\r\\n                   (.close w))\\r\\n                 (fn [_resp ex]\\r\\n                   (.close w)\\r\\n                   (.printStackTrace ex)))]\\r\\n  (statuses-filter\\r\\n    \:params {\:lang \\"en\\" \:track \\"\:)\\"}\\r\\n    \:oauth-creds my-creds\\r\\n    \:callbacks callback))" "(let [w (io/writer \\"positive.json\\")\\r\\n      callback (AsyncStreamingCallback.\\r\\n                 (fn [_resp payload]\\n                   (if (payload)\\r\\n                   (.write w (-> (str payload) json/read-str payload))\\r\\n                   (.write w \\"\\\\n\\")))\\r\\n                 (fn [_resp]\\r\\n                   (.close w))\\r\\n                 (fn [_resp ex]\\r\\n                   (.close w)\\r\\n                   (.printStackTrace ex)))]\\r\\n  (statuses-filter\\r\\n    \:params {\:lang \\"en\\" \:track \\"\:)\\"}\\r\\n    \:oauth-creds my-creds\\r\\n    \:callbacks callback))" "(statuses-filter\\r\\n    \:params {\:lang \\"en\\" \:track \\"\:)\\"}\\r\\n    \:oauth-creds my-creds\\r\\n    \:callbacks callback)" "(statuses-filter\\r\\n    \:params {\:lang \\"en\\" \:track \\"\:)\\"}\\r\\n    \:oauth-creds my-creds)" "(ns twitter-sentiment-analysis.data-collection\\r\\n  (\:use\\r\\n   [twitter.oauth]\\r\\n   [twitter.callbacks]\\r\\n   [twitter.callbacks.handlers]\\r\\n   [twitter.api.streaming]\\r\\n   [twitter callbacks oauth api utils request]\\r\\n   [clojure.java.io \:as io])\\r\\n  (\:require\\r\\n   [clojure.data.json \:as json]\\r\\n   [http.async.client \:as ac])\\r\\n  (\:import\\r\\n   (twitter.callbacks.protocols AsyncStreamingCallback)))\\r\\n\\r\\n(def my-creds (make-oauth-creds \\"nemg8aXAA9wtSo8h6ZZbrsYwk\\"\\r\\n                                \\"XmRZzVqlj1i5Ag72RJpd99IQLPoK6dfvbna89SwQ1NHIJVqN1l\\"\\r\\n                                \\"3021555060-YgU9ZHHTKVQGpkbDqNw9By4VNcFz7Ph455VAY0G\\"\\r\\n                                \\"XR9MZ3SE6quDbTfGcPNTiG2QebsdYnepStnZnrrzUeuye\\"))" "(let [w (io/writer \\"positive.json\\")\\r\\n      callback (AsyncStreamingCallback.\\r\\n                 (fn [_resp payload]\\r\\n                   (.write w  payload)\\r\\n                   (.write w \\"\\\\n\\"))\\r\\n                 (fn [_resp]\\r\\n                   (.close w))\\r\\n                 (fn [_resp ex]\\r\\n                   (.close w)\\r\\n                   (.printStackTrace ex)))]\\r\\n  (statuses-filter\\r\\n    \:params {\:lang \\"en\\" \:track \\"\:)\\"}\\r\\n    \:oauth-creds my-creds\\r\\n    \:callbacks callback))" "(let [w (io/writer \\"positive.json\\")\\r\\n      callback (AsyncStreamingCallback.\\r\\n                 (fn [_resp payload]\\r\\n                   (println  payload)\\r\\n                   (.write w \\"\\\\n\\"))\\r\\n                 (fn [_resp]\\r\\n                   (.close w))\\r\\n                 (fn [_resp ex]\\r\\n                   (.close w)\\r\\n                   (.printStackTrace ex)))]\\r\\n  (statuses-filter\\r\\n    \:params {\:lang \\"en\\" \:track \\"\:)\\"}\\r\\n    \:oauth-creds my-creds\\r\\n    \:callbacks callback))" "(ns twitter-sentiment-analysis.data-collection\\r\\n  (\:use\\r\\n   [twitter.oauth]\\r\\n   [twitter.callbacks]\\r\\n   [twitter.callbacks.handlers]\\r\\n   [twitter.api.streaming]\\r\\n   [twitter callbacks oauth api utils request]\\r\\n   [clojure.java.io \:as io])\\r\\n  (\:require\\r\\n   [clojure.data.json \:as json]\\r\\n   [http.async.client \:as ac])\\r\\n  (\:import\\r\\n   (twitter.callbacks.protocols AsyncStreamingCallback)))\\r\\n\\r\\n(def my-creds (make-oauth-creds \\"nemg8aXAA9wtSo8h6ZZbrsYwk\\"\\r\\n                                \\"XmRZzVqlj1i5Ag72RJpd99IQLPoK6dfvbna89SwQ1NHIJVqN1l\\"\\r\\n                                \\"3021555060-YgU9ZHHTKVQGpkbDqNw9By4VNcFz7Ph455VAY0G\\"\\r\\n                                \\"XR9MZ3SE6quDbTfGcPNTiG2QebsdYnepStnZnrrzUeuye\\"))" "(let [w (io/writer \\"positive.json\\")\\r\\n      callback (AsyncStreamingCallback.\\r\\n                 (fn [_resp payload]\\r\\n                   (println  (str payload))\\r\\n                   (.write w \\"\\\\n\\"))\\r\\n                 (fn [_resp]\\r\\n                   (.close w))\\r\\n                 (fn [_resp ex]\\r\\n                   (.close w)\\r\\n                   (.printStackTrace ex)))]\\r\\n  (statuses-filter\\r\\n    \:params {\:lang \\"en\\" \:track \\"\:)\\"}\\r\\n    \:oauth-creds my-creds\\r\\n    \:callbacks callback))" "(ns twitter-sentiment-analysis.data-collection\\r\\n  (\:use\\r\\n   [twitter.oauth]\\r\\n   [twitter.callbacks]\\r\\n   [twitter.callbacks.handlers]\\r\\n   [twitter.api.streaming]\\r\\n   [twitter callbacks oauth api utils request]\\r\\n   [clojure.java.io \:as io])\\r\\n  (\:require\\r\\n   [clojure.data.json \:as json]\\r\\n   [http.async.client \:as ac])\\r\\n  (\:import\\r\\n   (twitter.callbacks.protocols AsyncStreamingCallback)))\\r\\n\\r\\n(def my-creds (make-oauth-creds \\"nemg8aXAA9wtSo8h6ZZbrsYwk\\"\\r\\n                                \\"XmRZzVqlj1i5Ag72RJpd99IQLPoK6dfvbna89SwQ1NHIJVqN1l\\"\\r\\n                                \\"3021555060-YgU9ZHHTKVQGpkbDqNw9By4VNcFz7Ph455VAY0G\\"\\r\\n                                \\"XR9MZ3SE6quDbTfGcPNTiG2QebsdYnepStnZnrrzUeuye\\"))" "(let [w (io/writer \\"positive1.json\\")\\r\\n      callback (AsyncStreamingCallback.\\r\\n                 (fn [_resp payload]\\r\\n                   (.write w (str payload))\\r\\n                   (.write w \\"\\\\n\\"))\\r\\n                 (fn [_resp]\\r\\n                   (.close w))\\r\\n                 (fn [_resp ex]\\r\\n                   (.close w)\\r\\n                   (.printStackTrace ex)))]\\r\\n  (statuses-filter\\r\\n    \:params {\:lang \\"en\\" \:track \\"\:)\\"}\\r\\n    \:oauth-creds my-creds\\r\\n    \:callbacks callback))" "(ns twitter-sentiment-analysis.data-collection\\r\\n  (\:use\\r\\n   [twitter.oauth]\\r\\n   [twitter.callbacks]\\r\\n   [twitter.callbacks.handlers]\\r\\n   [twitter.api.streaming]\\r\\n   [twitter callbacks oauth api utils request]\\r\\n   [clojure.java.io \:as io])\\r\\n  (\:require\\r\\n   [clojure.data.json \:as json]\\r\\n   [http.async.client \:as ac])\\r\\n  (\:import\\r\\n   (twitter.callbacks.protocols AsyncStreamingCallback)))\\r\\n\\r\\n(def my-creds (make-oauth-creds \\"nemg8aXAA9wtSo8h6ZZbrsYwk\\"\\r\\n                                \\"XmRZzVqlj1i5Ag72RJpd99IQLPoK6dfvbna89SwQ1NHIJVqN1l\\"\\r\\n                                \\"3021555060-YgU9ZHHTKVQGpkbDqNw9By4VNcFz7Ph455VAY0G\\"\\r\\n                                \\"XR9MZ3SE6quDbTfGcPNTiG2QebsdYnepStnZnrrzUeuye\\"))" "(def no-of-tweets)" "(def no-of-tweets 1)" "(no-of-tweets)" "no-of-tweets" "(let [w (io/writer \\"positive1.json\\")\\r\\n      callback (AsyncStreamingCallback.\\r\\n                 (fn [_resp payload]\\n                   (if (no-of-tweets < 1000)\\r\\n                   (do\\r\\n                      (.write w (str payload))\\r\\n                      (inc no-of-tweets))\\r\\n                   (.close w)))\\r\\n                 (fn [_resp]\\r\\n                   (.close w))\\r\\n                 (fn [_resp ex]\\r\\n                   (.close w)\\r\\n                   (.printStackTrace ex)))]\\r\\n  (statuses-filter\\r\\n    \:params {\:lang \\"en\\" \:track \\"\:)\\"}\\r\\n    \:oauth-creds my-creds\\r\\n    \:callbacks callback))" "(let [w (io/writer \\"positive1.json\\")\\r\\n      callback (AsyncStreamingCallback.\\r\\n                 (fn [_resp payload]\\n                   (if (no-of-tweets < 1000)\\r\\n                   ((do\\r\\n                      (.write w (str payload))\\r\\n                      (inc no-of-tweets)))\\r\\n                   (.close w)))\\r\\n                 (fn [_resp]\\r\\n                   (.close w))\\r\\n                 (fn [_resp ex]\\r\\n                   (.close w)\\r\\n                   (.printStackTrace ex)))]\\r\\n  (statuses-filter\\r\\n    \:params {\:lang \\"en\\" \:track \\"\:)\\"}\\r\\n    \:oauth-creds my-creds\\r\\n    \:callbacks callback))" "(if (no-of-tweets < 1000)\\r\\n                   ((do\\r\\n                      (println \\"moze\\")\\r\\n                      (inc no-of-tweets)))\\r\\n                   (.close w))" "(if (no-of-tweets < 1000)\\r\\n                   ((do\\r\\n                      (println \\"moze\\")\\r\\n                      (inc no-of-tweets)))\\r\\n                   (println \\"zatvara\\"))" "(if (no-of-tweets < 1000)\\r\\n                   (do\\r\\n                      (println \\"moze\\")\\r\\n                      (inc no-of-tweets))\\r\\n                   (println \\"zatvara\\"))" "(if (no-of-tweets < 1000)\\r\\n                   (inc no-of-tweets))\\r\\n                   (println \\"zatvara\\")" "(if (< no-of-tweets 1000)\\r\\n                   (inc no-of-tweets))\\r\\n                   (println \\"zatvara\\")" "(let [w (io/writer \\"positive1.json\\")\\r\\n      callback (AsyncStreamingCallback.\\r\\n                 (fn [_resp payload]\\n                   (if (< no-of-tweets 1000)\\r\\n                   ((do\\r\\n                      (.write w (str payload))\\r\\n                      (inc no-of-tweets)))\\r\\n                   (.close w)))\\r\\n                 (fn [_resp]\\r\\n                   (.close w))\\r\\n                 (fn [_resp ex]\\r\\n                   (.close w)\\r\\n                   (.printStackTrace ex)))]\\r\\n  (statuses-filter\\r\\n    \:params {\:lang \\"en\\" \:track \\"\:)\\"}\\r\\n    \:oauth-creds my-creds\\r\\n    \:callbacks callback))" "(let [w (io/writer \\"positive1.json\\")\\r\\n      callback (AsyncStreamingCallback.\\r\\n                 (fn [_resp payload]\\n                   (if (< no-of-tweets 1000)\\r\\n                   (do\\r\\n                      (.write w (str payload))\\r\\n                      (inc no-of-tweets))\\r\\n                   (.close w)))\\r\\n                 (fn [_resp]\\r\\n                   (.close w))\\r\\n                 (fn [_resp ex]\\r\\n                   (.close w)\\r\\n                   (.printStackTrace ex)))]\\r\\n  (statuses-filter\\r\\n    \:params {\:lang \\"en\\" \:track \\"\:)\\"}\\r\\n    \:oauth-creds my-creds\\r\\n    \:callbacks callback))" "(let [w (io/writer \\"negative.json\\")\\r\\n      callback (AsyncStreamingCallback.\\r\\n                 (fn [_resp payload]\\n                   (if (< no-of-tweets 1000)\\r\\n                   (do\\r\\n                      (.write w (str payload))\\r\\n                      (inc no-of-tweets))\\r\\n                   (.close w)))\\r\\n                 (fn [_resp]\\r\\n                   (.close w))\\r\\n                 (fn [_resp ex]\\r\\n                   (.close w)\\r\\n                   (.printStackTrace ex)))]\\r\\n  (statuses-filter\\r\\n    \:params {\:lang \\"en\\" \:track \\"\:(\\"}\\r\\n    \:oauth-creds my-creds\\r\\n    \:callbacks callback))" "(ns twitter-sentiment-analysis.data-collection\\r\\n  (\:use\\r\\n   [twitter.oauth]\\r\\n   [twitter.callbacks]\\r\\n   [twitter.callbacks.handlers]\\r\\n   [twitter.api.streaming]\\r\\n   [twitter callbacks oauth api utils request]\\r\\n   [clojure.java.io \:as io])\\r\\n  (\:require\\r\\n   [clojure.data.json \:as json]\\r\\n   [http.async.client \:as ac])\\r\\n  (\:import\\r\\n   (twitter.callbacks.protocols AsyncStreamingCallback)))" "(def my-creds (make-oauth-creds \\"nemg8aXAA9wtSo8h6ZZbrsYwk\\"\\r\\n                                \\"XmRZzVqlj1i5Ag72RJpd99IQLPoK6dfvbna89SwQ1NHIJVqN1l\\"\\r\\n                                \\"3021555060-YgU9ZHHTKVQGpkbDqNw9By4VNcFz7Ph455VAY0G\\"\\r\\n                                \\"XR9MZ3SE6quDbTfGcPNTiG2QebsdYnepStnZnrrzUeuye\\"))" "(let [w (io/writer \\"positive.json\\")\\r\\n      callback (AsyncStreamingCallback.\\r\\n                 (fn [_resp payload]\\n                   (if (< no-of-tweets 1000)\\r\\n                   (do\\r\\n                      (.write w (str payload))\\r\\n                      (inc no-of-tweets))\\r\\n                   (.close w)))\\r\\n                 (fn [_resp]\\r\\n                   (.close w))\\r\\n                 (fn [_resp ex]\\r\\n                   (.close w)\\r\\n                   (.printStackTrace ex)))]\\r\\n  (statuses-filter\\r\\n    \:params {\:lang \\"en\\" \:track \\"\:(\\"}\\r\\n    \:oauth-creds my-creds\\r\\n    \:callbacks callback))" "(def no-of-tweets 1)" "(let [w (io/writer \\"positive.json\\")\\r\\n      callback (AsyncStreamingCallback.\\r\\n                 (fn [_resp payload]\\n                   (if (< no-of-tweets 1000)\\r\\n                   (do\\r\\n                      (.write w (str payload))\\r\\n                      (inc no-of-tweets))\\r\\n                   (.close w)))\\r\\n                 (fn [_resp]\\r\\n                   (.close w))\\r\\n                 (fn [_resp ex]\\r\\n                   (.close w)\\r\\n                   (.printStackTrace ex)))]\\r\\n  (statuses-filter\\r\\n    \:params {\:lang \\"en\\" \:track \\"\:(\\"}\\r\\n    \:oauth-creds my-creds\\r\\n    \:callbacks callback))" "(use 'clj-ml.io)" "(def ds (make-dataset \\"trainigData\\" [\:text {\:sentiment_class [\:positive \:negative]}] [ [\\"bla \:)\\" \:positive] [\\"blaa \:(\\" \:negative] ]))" "(def ds (clj-ml.io/make-dataset \\"trainigData\\" [\:text {\:sentiment_class [\:positive \:negative]}] [ [\\"bla \:)\\" \:positive] [\\"blaa \:(\\" \:negative] ]))" "(ns twitter-sentiment-analysis.data-processing\\r\\n  (\:use\\r\\n   [clj-ml.io \:as \\"weka\\"]))" "(ns twitter-sentiment-analysis.data-processing\\r\\n  (\:use\\r\\n   [clj-ml.io \:as weka]))" "(def ds (weka/make-dataset \\"trainigData\\" [\:text {\:sentiment_class [\:positive \:negative]}] [ [\\"bla \:)\\" \:positive] [\\"blaa \:(\\" \:negative] ]))" "(def ds (weka.make-dataset \\"trainigData\\" [\:text {\:sentiment_class [\:positive \:negative]}] [ [\\"bla \:)\\" \:positive] [\\"blaa \:(\\" \:negative] ]))" "(ns twitter-sentiment-analysis.data-processing\\r\\n  (\:use\\r\\n   [clj-ml.io \:as weka]))" "(def ds (weka/make-dataset \\"trainigData\\" [\:text {\:sentiment_class [\:positive \:negative]}] [ [\\"bla \:)\\" \:positive] [\\"blaa \:(\\" \:negative] ]))" "(ns twitter-sentiment-analysis.data-processing\\r\\n  (\:use\\r\\n   [clj-ml.data \:as weka]))" "(ns twitter-sentiment-analysis.data-processing\\r\\n  (\:use\\r\\n   [clj-ml.data \:as weka-data]))" "(def ds (weka-data/make-dataset \\"trainigData\\" [\:text {\:sentiment_class [\:positive \:negative]}] [ [\\"bla \:)\\" \:positive] [\\"blaa \:(\\" \:negative] ]))" "(def ds (make-dataset \\"trainigData\\" [\:text {\:sentiment_class [\:positive \:negative]}] [ [\\"bla \:)\\" \:positive] [\\"blaa \:(\\" \:negative] ]))" "(def ds (make-dataset \\"trainigData\\" [\:text {\:sentiment-lass [\:positive \:negative]}]\\n                      [ [\\"bla \:)\\" \:positive] [\\"blaa \:(\\" \:negative] ]))" "(def ds (make-dataset \\"trainigData\\" [\:text {\:sentiment-lass [\:positive \:negative]}]))" "(def ds (make-dataset \\"trainigData\\" [\:text {\:sentiment-lass [\:positive \:negative]}] []))" "(def tweetovi [\\"prvi\\" \\"drugi\\" \\"treci\\" \\"cetvrti\\"])" "(map [tweet tweetovi] (weka-data ds [tweet \:positive]))" "(map \#(weka-data ds [% \:positive]) tweetovi)" "(map \#(weka-data/dataset-add ds [% \:positive]) tweetovi)" "(map \#(weka-data/dataset-add ds [% \:positive]) [1 2 3])" "(map \#(weka-data/dataset-add ds [% \:positive]) [\\"1\\" \\"2\\" \\"3\\"])" "(map \#(weka-data/dataset-add ds [% \:positive]) [\#\\"1\\" \#\\"2\\" \#\\"3\\"])" "(map \#(weka-data/dataset-add ds [(str %) \:positive]) [\\"1\\" \\"2\\" \\"3\\"])" "(def ds (make-dataset \\"trainigData\\" [\:text {\:sentiment-class [\:positive \:negative]}] []))" "(map \#(weka-data/dataset-add ds [% \:positive]) tweetovi)" "(map \#(weka-data/dataset-add ds [(str %) \:positive]) tweetovi)" "(def dataset (make-dataset \\"trainigData1\\" [\:text {\:sentiment-class [\:positive \:negative]}] [[\\"1\\" \:positive] [\\"2\\" \:nega]]))" "(def dataset (make-dataset \\"trainigData1\\" [\:text {\:sentiment-class [\:positive \:negative]}] [[\:text \\"1\\" \:positive] [\:text \:2\: \:nega]]))" "(def dataset (make-dataset \\"trainigData1\\" [\:text {\:sentiment-class [\:positive \:negative]}] [[\:text \\"1\\" \:positive] [\:text \:2\: \:negative]]))" "(def dataset (make-dataset \\"trainigData1\\" [\:text {\:sentiment-class [\:positive \:negative]}] [[\:text \\"1\\" \:positive] [\:text \\"2\\" \:negative]]))" "(def dataset (make-dataset \\"trainigData1\\" [{\:sentiment-class [\:positive \:negative]} \:text]))" "(def dataset (make-dataset \\"trainigData1\\" [{\:sentiment-class [\:positive \:negative]} \:text] []))" "dataset" "(string-attributes dataset)" "(weka-data/string-attributes dataset)" "(ns com.wekatest\\r\\n (\:import (weka.core Attribute FastVector Instance Instances)))" "(.FastVector 2)" "(weka.core.FastVector 2)" "(ns com.wekatest\\r\\n (\:import (weka.core)))" "(weka.core.Attribute)" "(new weka.core.FastVector 2)" "(def sentiment-classes (new weka.core.FastVector 2))" "(.addElement sentiment-classes \\"positive\\")" "(.addElement sentiment-classes \\"negative\\")" "(def sentimentAttr (new weka.core.Attribute \\"sentiment_class\\" sentiment-classes))" "(def textAttr (new weka.core.Attribute \\"text\\" null))" "(def textAttr (new weka.core.Attribute \\"text\\" nil))" "(def textAttr (new weka.core.Attribute \\"text\\" (new FastVector)))" "(def attributes (new FastVector))" "(.addElement attributes textAttr)" "(.addElement attributes sentimentAttr)" "(def data (new Instances \\"trainingData\\" attributes 1))" "(.setClassIndex data 1)" "data" "(def raw-data [\\"test\\" \\"testst\\" \\"testst\\"])" "(def instances (map (comp (.setValue sentimentAttr \\"positive\\") \#(.setValue textAttr %)(new Instance 2)) raw-data))" "(def instances (map (comp (.setValue instance sentimentAttr \\"positive\\") \#(.setValue  instance textAttr %)(def instance (new Instance 2))) raw-data))" "(def instance (new Instance))" "(\:use\\r\\n      [clj-ml.data \:as weka-data])" "(def instance (new weka.core.Instance 2))" "instance" "(.setValue instance textAttr \\"aaaa\\")" "(.setValue instance sentimentAttr \\"positive\\")" "(.setValue instance textAttrAttr \\"positive text\\")" "(.setValue instance textAttr \\"positive text\\")" "(.setValue instance textAttr \\"positive text\\" sentimentAttr \\"positive\\")" "instance" "(.setValue instance textAttr \\"asd\\")" "(.setValue instance \\"asd\\" \\"positive\\")" "(.setValue instance sentimentAttr \\"positive\\")" "(.setValue instance sentimentAttr \\"positive\\" textAttr \\"negative\\")" "(.add instances instance)" "(.add data instance)" "data" "(.setClassValue instance \\"positive\\")" "(def newInstance (new weka.core.Instance))" "(def newInstance (new weka.core.Instance 2))" "instance" "(.setClassValue \\"positive\\")" "(.setClassValue instance \\"positive\\")" "(.setDataset instance data)" "(.setClassValue instance \\"positive\\")" "(.setValue instance textAttr \\"text\\")" "instance" "(.setValue instance \\"text\\")" "(.setValue instance textAttr \\"text\\")" "(.setValue instance textAttr \\"8888\\")" "(.setValue instance textAttr 1)" "(def instance (.newInstance 2))" "(def instance (new Instance 2))" "instance" "(.setValue instance textAttr \\"positive\\")" "(.setValue instance textAttr \\"\\")" "(def instance (new weka.core.Instance 2))" "(def sentiment-classes (new FastVector 2))" "(.addElement sentiment-classes \\"positive\\")" "(.addElement sentiment-classes \\"negative\\")" "(def sentimentAttr (new weka.core.Attribute \\"sentiment_class\\" sentiment-classes))" "(def textAttr (new weka.core.Attribute \\"text\\" (cast (FastVector) nil)))" "(def textAttr (new weka.core.Attribute \\"text\\" (cast FastVector nil)))" "(def textAttr (new weka.core.Attribute \\"text\\" (cast FastVector null)))" "(def textAttr (new weka.core.Attribute \\"text\\" (cast FastVector nil)))" "(def textAttr (new weka.core.Attribute \\"text\\" (.FastVector nil )))" "(def textAttr (new weka.core.Attribute \\"text\\" (.FastVector nil)))" "textAttr" "(def textAttr (new weka.core.Attribute \\"text\\" (FastVector nil)))" "(def textAttr (new weka.core.Attribute \\"text\\" (cast weka.core.FastVector nil)))" "(def textAttr (new weka.core.Attribute \\"text\\" (cast weka.core.FastVector \\"\\")))" "(def textAttr (new weka.core.Attribute \\"text\\" (cast weka.core.FastVector [])))" "(def textAttr (new weka.core.Attribute \\"text\\" (.FastVector nil)))" "(def textAttr (new weka.core.Attribute \\"text\\" nil))" "(def textAttr (new weka.core.Attribute \\"text\\"))" "textAttr" "(def textAttr (new weka.core.Attribute \\"text\\" (new FastVector nil)))" "(def textAttr (new weka.core.Attribute \\"text\\" (cast FastVector (new Object))))" "(def textAttr (new weka.core.Attribute \\"text\\" (cast FastVector nil)))" "(def no-value nil)" "(def textAttr (new weka.core.Attribute \\"text\\" (cast FastVector no-value)))" "(def textAttr (new weka.core.Attribute \\"text\\" (FastVector no-value)))" "(cast FastVector nil)" "(def fv (new FastVector))" "fv" "(def textAttr (new weka.core.Attribute \\"text\\" (List nil)))" "(def list (new java.util.List))" "(\:use java.util)" "(\:use java.util.List)" "(def lista (new List))" "(def lista (new java.util.List ))" "(use [clj-ml.data])" "(\:use [clj-ml.data])" "(ns twitter-sentiment-analysis.data_formatting\\r\\n  (\:use\\r\\n   [clj-ml/data]))" "(ns twitter-sentiment-analysis.data_processing\\r\\n  (\:use\\r\\n   [clj-ml.data]))" "(def dataset (make-dataset \\"training-set\\" [{\:text nil} {\:kind [\:good \:bad]}] []))" "dataset" "(def dataset (make-dataset \\"training-set\\" [{\:text []} {\:kind [\:good \:bad]}] []))" "dataset" "(def dataset (make-dataset \\"training-set\\" [{\:text} {\:kind [\:good \:bad]}] []))" "(def dataset (make-dataset \\"training-set\\" [{\:text } {\:kind [\:good \:bad]}] []))" "(def dataset (make-dataset \\"training-set\\" [{\:text nil} {\:kind [\:good \:bad]}] [[\\"bla \:good\\"]]))" "(def dataset (make-dataset \\"training-set\\" [{\:text nil} {\:kind [\:good \:bad]}] []))" "dataset" "(def dataset (make-dataset \\"training-set\\" [{\:text []} {\:kind [\:good \:bad]}] [[\\"basdasd\\" \:good]]))" "(def dataset (make-dataset \\"training-set\\" [{\:text \\"\\"} {\:kind [\:good \:bad]}] []))" "dataset" "(def dataset (make-dataset \\"training-set\\" [{\:text \\"\\"} {\:kind [\:good \:bad]}] [[\\"asdad\\" \:good]]))" "(ns com.wekatest\\r\\n    (\:import (weka.core Attribute FastVector Instance Instances)))" "(def sentiment-classes (new FastVector 2))" "(.addElement sentiment-classes \\"positive\\")" "(.addElement sentiment-classes \\"negative\\")" "(def sentiment-attr (new Attribute \\"sentiment_class\\" sentiment-classes))" "(def text-attr (new Attribute \\"text\\" ((new FastVector) nil)))" "(def text-attr (new Attribute \\"text\\" (new FastVector nil)))" "(def text-attr (new Attribute \\"text\\" (new FastVector) nil))" "(def text-attr (new Attribute \\"text\\" (new FastVector 0) nil))" "(def text-attr (new Attribute \\"text\\" (new FastVector 2) nil))" "(def text-attr (new Attribute \\"text\\" (new FastVector nil) nil))" "(def text-attr (new Attribute \\"text\\" (new FastVector) nil))" "(cast (new FastVector) nil)" "(cast FastVector nil)" "(def dataset (make-dataset \\"training-set\\" [{\:text [nil]} {\:kind [\:good \:bad]}] []))" "(ns twitter-sentiment-analysis.data_processing\\r\\n  (\:use\\r\\n   [clj-ml.data]))" "(def dataset (make-dataset \\"training-set\\" [{\:text [nil]} {\:kind [\:good \:bad]}] []))" "dataset" "(def dataset (make-dataset \\"training-set\\" [{\:text [\:nil nil]} {\:kind [\:good \:bad]}] []))" "(def dataset (make-dataset \\"training-set\\" [{\:text [\:nil]} {\:kind [\:good \:bad]}] []))" "dataset" "(def dataset (make-dataset \\"training-set\\" [{\:text {nil}} {\:kind [\:good \:bad]}] []))" "(def dataset (make-dataset \\"training-set\\" [{\:text nil} {\:kind [\:good \:bad]}] []))" "dataset" "(def dataset (make-dataset \\"training-set\\" [{\:text []} {\:kind [\:good \:bad]}] []))" "dataset" "(dataset-add [\\"sadadadad\\" \\"\:good\\"])" "(dataset-add [\\"sasd\\" \:good])" "(dataset-add 2 [\\"sasd\\" \:good])" "(dataset-add [])" "(dataset-add  dataset [\\"sadadad\\" \\"\:good\\"])" "dataset" "(def dataset (make-dataset \\"training-set\\" [{\:text [nil]} {\:kind [\:good \:bad]}] []))" "dataset" "(def dataset (make-dataset \\"training-set\\" [{\:text nil} {\:kind [\:good \:bad]}] []))" "dataset" "(def dataset (make-dataset \\"training-set\\" [{\:text {nil}} {\:kind [\:good \:bad]}] []))" "(def dataset (make-dataset \\"training-set\\" [{\:text \#{nil}} {\:kind [\:good \:bad]}] []))" "(ns twitter-sentiment-analysis.data-processing\\n  (\:use [cjl-ml io filters classifiers]))" "(ns twitter-sentiment-analysis.data-processing\\n  (\:use [cjl-ml.io] [cjl-ml.filters] [cjl-ml.classifiers]))" "(ns twitter-sentiment-analysis.data-processing\\n  (\:use [clj-ml.io] [clj-ml.filters] [clj-ml.classifiers]))" "(def ds (load-instances \:arff \\"file\:///resources/trainingDataSet.arff\\"))" "(def ds (load-instances \:arff \\"file\:///twitter-sentiment-analysis/resources/trainingDataSet.arff\\"))" "(def ds (load-instances \:arff \\"file\:resources/trainingDataSet.arff\\"))" "ds" "(clj-ml.data/dataset-set-class ds 1)" "(ns twitter-sentiment-analysis.data-processing\\n  (\:use [clj-ml.io] [clj-ml.filters] [clj-ml.classifiers] [clj-ml.data] [weka.core]))" "(ns twitter-sentiment-analysis.data-processing\\n  (\:use [clj-ml.io] [clj-ml.filters] [clj-ml.classifiers] [clj-ml.data])\\n  (\:import [weka.core]))" "(defn build classifier [classifier, arffFileName, nGramMaxSize]\\r\\n  (let [trainingData (load-instances \:arff \\"file\:///resources/trainingDataSet.arff\\")\\r\\n        tokenizer (new NGramTokenizer)\\n        textToWordfilter (new StringToWordVector)\\n        ranker (new Ranker)\\n        asFilter (new AttributeSelection)\\n        multiFilter (new MultiFilter)\\n        filteredClassifier (new FilteredClassifier)\\n        eval (new Evaluation trainingData)]\\r\\n    (dataset-set-class ds 1)\\n    (.setNGramMinSize tokenizer 1)\\n    (.setNGramMaxSize tokenizer nGramMaxSize)\\n    (.setDelimiters tokenizer \\" \\\\\\\\W\\")\\n    (.setTokenizer textToWordfilter tokenizer)\\n    (.setInputFormat textToWordfilter trainingData)\\n    (.setWordsToKeep textToWordfilter 10000)\\n    (.setDoNotOperateOnPerClassBasis textToWordfilter true)\\n    (.setLowerCaseTokens textToWordfilter false)\\n    (.setThreshold ranker 0.0)\\n    (.setEvaluator asFilter (new InfoGainAttributeEval))\\n    (.setSearch asFilter ranker)\\n    (.setFilters multiFilter [textToWordfilter asFilter])\\n    (.setClassifier filteredClassifier classifier)\\n    (.setFilter filteredClassifier multiFilter)\\n    (.buildClassifier filteredClassifier trainingData)\\n    (.evaluateModel eval filteredClassifier trainingData)\\n    (println (.toSummaryString eval))\\n    (println (.toMatrixString eval)))\\r\\n    )" "(defn build-classifier [classifier, arffFileName, nGramMaxSize]\\r\\n  (let [trainingData (load-instances \:arff \\"file\:///resources/trainingDataSet.arff\\")\\r\\n        tokenizer (new NGramTokenizer)\\n        textToWordfilter (new StringToWordVector)\\n        ranker (new Ranker)\\n        asFilter (new AttributeSelection)\\n        multiFilter (new MultiFilter)\\n        filteredClassifier (new FilteredClassifier)\\n        eval (new Evaluation trainingData)]\\r\\n    (dataset-set-class ds 1)\\n    (.setNGramMinSize tokenizer 1)\\n    (.setNGramMaxSize tokenizer nGramMaxSize)\\n    (.setDelimiters tokenizer \\" \\\\\\\\W\\")\\n    (.setTokenizer textToWordfilter tokenizer)\\n    (.setInputFormat textToWordfilter trainingData)\\n    (.setWordsToKeep textToWordfilter 10000)\\n    (.setDoNotOperateOnPerClassBasis textToWordfilter true)\\n    (.setLowerCaseTokens textToWordfilter false)\\n    (.setThreshold ranker 0.0)\\n    (.setEvaluator asFilter (new InfoGainAttributeEval))\\n    (.setSearch asFilter ranker)\\n    (.setFilters multiFilter [textToWordfilter asFilter])\\n    (.setClassifier filteredClassifier classifier)\\n    (.setFilter filteredClassifier multiFilter)\\n    (.buildClassifier filteredClassifier trainingData)\\n    (.evaluateModel eval filteredClassifier trainingData)\\n    (println (.toSummaryString eval))\\n    (println (.toMatrixString eval)))\\r\\n    )" "(def ds (load-instances \:arff \\"file\:resources/trainingDataSet.arff\\"))\\r\\n\\r\\n(clj-ml.data/dataset-set-class ds 1)" "(def classifier (make-classifier \:naive-bayes))" "(def classifier (make-classifier \:bayes \:naive))" "(classifier-train classifier ds)" "(def classifier (make-classifier \:support-vector-machine \:smo))" "(def classifier (make-classifier \:neural-network \:multilayer-perceptron))" "(classifier-train classifier ds)" "(ns twitter-sentiment-analysis.data-analysis\\r\\n  (\:use [clj-ml.io] [clj-ml.filters] [clj-ml.classifiers] [clj-ml.data])\\r\\n  (\:import [weka.attributeSelection.InfoGainAttributeEval]\\r\\n           [weka.attributeSelection.Ranker]\\r\\n           [weka.classifiers.Classifier]\\r\\n           [weka.classifiers.Evaluation]\\r\\n           [weka.classifiers.meta.FilteredClassifier]\\r\\n           [weka.core.Attribute]\\r\\n           [weka.core.FastVector]\\r\\n           [weka.core.Instance]\\r\\n           [weka.core.Instances]\\r\\n           [weka.core.tokenizers.NGramTokenizer]\\r\\n           [weka.filters.Filter]\\r\\n           [weka.filters.MultiFilter]\\r\\n           [weka.filters.supervised.attribute.AttributeSelection]\\r\\n           [weka.filters.unsupervised.attribute.StringToWordVector]\\r\\n           ))" "(defn build-classifier [classifier, arffFileName, nGramMaxSize]\\r\\n     (let [trainingData (load-instances \:arff \\"file\:///resources/trainingDataSet.arff\\")\\r\\n           tokenizer (new NGramTokenizer)\\r\\n           textToWordfilter (new StringToWordVector)\\r\\n           ranker (new Ranker)\\r\\n           asFilter (new AttributeSelection)\\r\\n           multiFilter (new MultiFilter)\\r\\n           filteredClassifier (new FilteredClassifier)\\r\\n           eval (new Evaluation trainingData)]\\r\\n       (dataset-set-class ds 1)\\r\\n       (.setNGramMinSize tokenizer 1)\\r\\n       (.setNGramMaxSize tokenizer nGramMaxSize)\\r\\n       (.setDelimiters tokenizer \\" \\\\\\\\W\\")\\r\\n       (.setTokenizer textToWordfilter tokenizer)\\r\\n       (.setInputFormat textToWordfilter trainingData)\\r\\n       (.setWordsToKeep textToWordfilter 10000)\\r\\n       (.setDoNotOperateOnPerClassBasis textToWordfilter true)\\r\\n       (.setLowerCaseTokens textToWordfilter false)\\r\\n       (.setThreshold ranker 0.0)\\r\\n       (.setEvaluator asFilter (new InfoGainAttributeEval))\\r\\n       (.setSearch asFilter ranker)\\r\\n       (.setFilters multiFilter [textToWordfilter asFilter])\\r\\n       (.setClassifier filteredClassifier classifier)\\r\\n       (.setFilter filteredClassifier multiFilter)\\r\\n       (.buildClassifier filteredClassifier trainingData)\\r\\n       (.evaluateModel eval filteredClassifier trainingData)\\r\\n       (println (.toSummaryString eval))\\r\\n       (println (.toMatrixString eval)))\\r\\n       )" "(defn build-classifier [classifier, arffFileName, nGramMaxSize]\\r\\n     (let [trainingData (load-instances \:arff \\"file\:///resources/trainingDataSet.arff\\")\\r\\n           tokenizer []\\r\\n           textToWordfilter (new StringToWordVector)\\r\\n           ranker (new Ranker)\\r\\n           asFilter (new AttributeSelection)\\r\\n           multiFilter (new MultiFilter)\\r\\n           filteredClassifier (new FilteredClassifier)\\r\\n           eval (new Evaluation trainingData)]\\r\\n       (dataset-set-class ds 1)\\r\\n       (.setNGramMinSize tokenizer 1)\\r\\n       (.setNGramMaxSize tokenizer nGramMaxSize)\\r\\n       (.setDelimiters tokenizer \\" \\\\\\\\W\\")\\r\\n       (.setTokenizer textToWordfilter tokenizer)\\r\\n       (.setInputFormat textToWordfilter trainingData)\\r\\n       (.setWordsToKeep textToWordfilter 10000)\\r\\n       (.setDoNotOperateOnPerClassBasis textToWordfilter true)\\r\\n       (.setLowerCaseTokens textToWordfilter false)\\r\\n       (.setThreshold ranker 0.0)\\r\\n       (.setEvaluator asFilter (new InfoGainAttributeEval))\\r\\n       (.setSearch asFilter ranker)\\r\\n       (.setFilters multiFilter [textToWordfilter asFilter])\\r\\n       (.setClassifier filteredClassifier classifier)\\r\\n       (.setFilter filteredClassifier multiFilter)\\r\\n       (.buildClassifier filteredClassifier trainingData)\\r\\n       (.evaluateModel eval filteredClassifier trainingData)\\r\\n       (println (.toSummaryString eval))\\r\\n       (println (.toMatrixString eval)))\\r\\n       )" "(defn build-classifier [classifier, arffFileName, nGramMaxSize]\\r\\n     (let [trainingData (load-instances \:arff \\"file\:///resources/trainingDataSet.arff\\")\\r\\n           tokenizer (new weka.core.tokenizers.NGramTokenizer)\\r\\n           textToWordfilter (new StringToWordVector)\\r\\n           ranker (new Ranker)\\r\\n           asFilter (new AttributeSelection)\\r\\n           multiFilter (new MultiFilter)\\r\\n           filteredClassifier (new FilteredClassifier)\\r\\n           eval (new Evaluation trainingData)]\\r\\n       (dataset-set-class ds 1)\\r\\n       (.setNGramMinSize tokenizer 1)\\r\\n       (.setNGramMaxSize tokenizer nGramMaxSize)\\r\\n       (.setDelimiters tokenizer \\" \\\\\\\\W\\")\\r\\n       (.setTokenizer textToWordfilter tokenizer)\\r\\n       (.setInputFormat textToWordfilter trainingData)\\r\\n       (.setWordsToKeep textToWordfilter 10000)\\r\\n       (.setDoNotOperateOnPerClassBasis textToWordfilter true)\\r\\n       (.setLowerCaseTokens textToWordfilter false)\\r\\n       (.setThreshold ranker 0.0)\\r\\n       (.setEvaluator asFilter (new InfoGainAttributeEval))\\r\\n       (.setSearch asFilter ranker)\\r\\n       (.setFilters multiFilter [textToWordfilter asFilter])\\r\\n       (.setClassifier filteredClassifier classifier)\\r\\n       (.setFilter filteredClassifier multiFilter)\\r\\n       (.buildClassifier filteredClassifier trainingData)\\r\\n       (.evaluateModel eval filteredClassifier trainingData)\\r\\n       (println (.toSummaryString eval))\\r\\n       (println (.toMatrixString eval)))\\r\\n       )" "(defn build-classifier [classifier, arffFileName, nGramMaxSize]\\r\\n     (let [trainingData (load-instances \:arff \\"file\:///resources/trainingDataSet.arff\\")\\r\\n           tokenizer (new weka.core.tokenizers.NGramTokenizer)\\r\\n           textToWordfilter (new weka.filters.unsupervised.attribute.StringToWordVector)\\r\\n           ranker (new weka.attributeSelection.Ranker)\\r\\n           asFilter (new weka.filters.supervised.attribute.AttributeSelection)\\r\\n           multiFilter (new weka.filters.MultiFilter)\\r\\n           filteredClassifier (new weka.classifiers.meta.FilteredClassifier)\\r\\n           eval (new weka.classifiers.Evaluation trainingData)]\\r\\n       (dataset-set-class ds 1)\\r\\n       (.setNGramMinSize tokenizer 1)\\r\\n       (.setNGramMaxSize tokenizer nGramMaxSize)\\r\\n       (.setDelimiters tokenizer \\" \\\\\\\\W\\")\\r\\n       (.setTokenizer textToWordfilter tokenizer)\\r\\n       (.setInputFormat textToWordfilter trainingData)\\r\\n       (.setWordsToKeep textToWordfilter 10000)\\r\\n       (.setDoNotOperateOnPerClassBasis textToWordfilter true)\\r\\n       (.setLowerCaseTokens textToWordfilter false)\\r\\n       (.setThreshold ranker 0.0)\\r\\n       (.setEvaluator asFilter (new InfoGainAttributeEval))\\r\\n       (.setSearch asFilter ranker)\\r\\n       (.setFilters multiFilter [textToWordfilter asFilter])\\r\\n       (.setClassifier filteredClassifier classifier)\\r\\n       (.setFilter filteredClassifier multiFilter)\\r\\n       (.buildClassifier filteredClassifier trainingData)\\r\\n       (.evaluateModel eval filteredClassifier trainingData)\\r\\n       (println (.toSummaryString eval))\\r\\n       (println (.toMatrixString eval)))\\r\\n       )" "(defn build-classifier [classifier, arffFileName, nGramMaxSize]\\r\\n     (let [trainingData (load-instances \:arff \\"file\:///resources/trainingDataSet.arff\\")\\r\\n           tokenizer (new weka.core.tokenizers.NGramTokenizer)\\r\\n           textToWordfilter (new weka.filters.unsupervised.attribute.StringToWordVector)\\r\\n           ranker (new weka.attributeSelection.Ranker)\\r\\n           asFilter (new weka.filters.supervised.attribute.AttributeSelection)\\r\\n           multiFilter (new weka.filters.MultiFilter)\\r\\n           filteredClassifier (new weka.classifiers.meta.FilteredClassifier)\\r\\n           eval (new weka.classifiers.Evaluation trainingData)]\\r\\n       (dataset-set-class trainingData 1)\\r\\n       (.setNGramMinSize tokenizer 1)\\r\\n       (.setNGramMaxSize tokenizer nGramMaxSize)\\r\\n       (.setDelimiters tokenizer \\" \\\\\\\\W\\")\\r\\n       (.setTokenizer textToWordfilter tokenizer)\\r\\n       (.setInputFormat textToWordfilter trainingData)\\r\\n       (.setWordsToKeep textToWordfilter 10000)\\r\\n       (.setDoNotOperateOnPerClassBasis textToWordfilter true)\\r\\n       (.setLowerCaseTokens textToWordfilter false)\\r\\n       (.setThreshold ranker 0.0)\\r\\n       (.setEvaluator asFilter (new InfoGainAttributeEval))\\r\\n       (.setSearch asFilter ranker)\\r\\n       (.setFilters multiFilter [textToWordfilter asFilter])\\r\\n       (.setClassifier filteredClassifier classifier)\\r\\n       (.setFilter filteredClassifier multiFilter)\\r\\n       (.buildClassifier filteredClassifier trainingData)\\r\\n       (.evaluateModel eval filteredClassifier trainingData)\\r\\n       (println (.toSummaryString eval))\\r\\n       (println (.toMatrixString eval)))\\r\\n       )" "(defn build-classifier [classifier, arffFileName, nGramMaxSize]\\r\\n     (let [trainingData (load-instances \:arff \\"file\:///resources/trainingDataSet.arff\\")\\r\\n           tokenizer (new weka.core.tokenizers.NGramTokenizer)\\r\\n           textToWordfilter (new weka.filters.unsupervised.attribute.StringToWordVector)\\r\\n           ranker (new weka.attributeSelection.Ranker)\\r\\n           asFilter (new weka.filters.supervised.attribute.AttributeSelection)\\r\\n           multiFilter (new weka.filters.MultiFilter)\\r\\n           filteredClassifier (new weka.classifiers.meta.FilteredClassifier)\\r\\n           eval (new weka.classifiers.Evaluation trainingData)]\\r\\n       (dataset-set-class trainingData 1)\\r\\n       (.setNGramMinSize tokenizer 1)\\r\\n       (.setNGramMaxSize tokenizer nGramMaxSize)\\r\\n       (.setDelimiters tokenizer \\" \\\\\\\\W\\")\\r\\n       (.setTokenizer textToWordfilter tokenizer)\\r\\n       (.setInputFormat textToWordfilter trainingData)\\r\\n       (.setWordsToKeep textToWordfilter 10000)\\r\\n       (.setDoNotOperateOnPerClassBasis textToWordfilter true)\\r\\n       (.setLowerCaseTokens textToWordfilter false)\\r\\n       (.setThreshold ranker 0.0)\\r\\n       (.setEvaluator asFilter (new weka.attributeSelection.InfoGainAttributeEval))\\r\\n       (.setSearch asFilter ranker)\\r\\n       (.setFilters multiFilter [textToWordfilter asFilter])\\r\\n       (.setClassifier filteredClassifier classifier)\\r\\n       (.setFilter filteredClassifier multiFilter)\\r\\n       (.buildClassifier filteredClassifier trainingData)\\r\\n       (.evaluateModel eval filteredClassifier trainingData)\\r\\n       (println (.toSummaryString eval))\\r\\n       (println (.toMatrixString eval)))\\r\\n       )" "build-classifier" "(build-classifier (new weka.classifier.bayes.NaiveBayes) \\"\\" 2)" "(ns twitter-sentiment-analysis.data-analysis\\r\\n  (\:use [clj-ml.io] [clj-ml.filters] [clj-ml.classifiers] [clj-ml.data])\\r\\n  (\:import [weka.attributeSelection.InfoGainAttributeEval]\\r\\n           [weka.attributeSelection.Ranker]\\r\\n           [weka.classifiers.Classifier]\\r\\n           [weka.classifiers.Evaluation]\\r\\n           [weka.classifiers.meta.FilteredClassifier]\\r\\n           [weka.core.Attribute]\\r\\n           [weka.core.FastVector]\\r\\n           [weka.core.Instance]\\r\\n           [weka.core.Instances]\\r\\n           [weka.core.tokenizers.NGramTokenizer]\\r\\n           [weka.filters.Filter]\\r\\n           [weka.filters.MultiFilter]\\r\\n           [weka.filters.supervised.attribute.AttributeSelection]\\r\\n           [weka.filters.unsupervised.attribute.StringToWordVector]\\n           [weka.classifiers.bayes.NaiveBayes]\\r\\n           ))" "(defn build-classifier [classifier, arffFileName, nGramMaxSize]\\r\\n     (let [trainingData (load-instances \:arff \\"file\:///resources/trainingDataSet.arff\\")\\r\\n           tokenizer (new weka.core.tokenizers.NGramTokenizer)\\r\\n           textToWordfilter (new weka.filters.unsupervised.attribute.StringToWordVector)\\r\\n           ranker (new weka.attributeSelection.Ranker)\\r\\n           asFilter (new weka.filters.supervised.attribute.AttributeSelection)\\r\\n           multiFilter (new weka.filters.MultiFilter)\\r\\n           filteredClassifier (new weka.classifiers.meta.FilteredClassifier)\\r\\n           eval (new weka.classifiers.Evaluation trainingData)]\\r\\n       (dataset-set-class trainingData 1)\\r\\n       (.setNGramMinSize tokenizer 1)\\r\\n       (.setNGramMaxSize tokenizer nGramMaxSize)\\r\\n       (.setDelimiters tokenizer \\" \\\\\\\\W\\")\\r\\n       (.setTokenizer textToWordfilter tokenizer)\\r\\n       (.setInputFormat textToWordfilter trainingData)\\r\\n       (.setWordsToKeep textToWordfilter 10000)\\r\\n       (.setDoNotOperateOnPerClassBasis textToWordfilter true)\\r\\n       (.setLowerCaseTokens textToWordfilter false)\\r\\n       (.setThreshold ranker 0.0)\\r\\n       (.setEvaluator asFilter (new weka.attributeSelection.InfoGainAttributeEval))\\r\\n       (.setSearch asFilter ranker)\\r\\n       (.setFilters multiFilter [textToWordfilter asFilter])\\r\\n       (.setClassifier filteredClassifier classifier)\\r\\n       (.setFilter filteredClassifier multiFilter)\\r\\n       (.buildClassifier filteredClassifier trainingData)\\r\\n       (.evaluateModel eval filteredClassifier trainingData)\\r\\n       (println (.toSummaryString eval))\\r\\n       (println (.toMatrixString eval)))\\r\\n       )" "(build-classifier (new weka.classifier.bayes.NaiveBayes) \\"\\" 2)" "(build-classifier (new weka.classifiers.bayes.NaiveBayes) \\"\\" 2)" "(defn build-classifier [classifier, arffFileName, nGramMaxSize]\\r\\n        (let [trainingData (load-instances \:arff \\"file\:resources/trainingDataSet.arff\\")\\r\\n              tokenizer (new weka.core.tokenizers.NGramTokenizer)\\r\\n              textToWordfilter (new weka.filters.unsupervised.attribute.StringToWordVector)\\r\\n              ranker (new weka.attributeSelection.Ranker)\\r\\n              asFilter (new weka.filters.supervised.attribute.AttributeSelection)\\r\\n              multiFilter (new weka.filters.MultiFilter)\\r\\n              filteredClassifier (new weka.classifiers.meta.FilteredClassifier)\\r\\n              eval (new weka.classifiers.Evaluation trainingData)]\\r\\n          (dataset-set-class trainingData 1)\\r\\n          (.setNGramMinSize tokenizer 1)\\r\\n          (.setNGramMaxSize tokenizer nGramMaxSize)\\r\\n          (.setDelimiters tokenizer \\" \\\\\\\\W\\")\\r\\n          (.setTokenizer textToWordfilter tokenizer)\\r\\n          (.setInputFormat textToWordfilter trainingData)\\r\\n          (.setWordsToKeep textToWordfilter 10000)\\r\\n          (.setDoNotOperateOnPerClassBasis textToWordfilter true)\\r\\n          (.setLowerCaseTokens textToWordfilter false)\\r\\n          (.setThreshold ranker 0.0)\\r\\n          (.setEvaluator asFilter (new weka.attributeSelection.InfoGainAttributeEval))\\r\\n          (.setSearch asFilter ranker)\\r\\n          (.setFilters multiFilter [textToWordfilter asFilter])\\r\\n          (.setClassifier filteredClassifier classifier)\\r\\n          (.setFilter filteredClassifier multiFilter)\\r\\n          (.buildClassifier filteredClassifier trainingData)\\r\\n          (.evaluateModel eval filteredClassifier trainingData)\\r\\n          (println (.toSummaryString eval))\\r\\n          (println (.toMatrixString eval))\\r\\n          ))" "(build-classifier (new weka.classifiers.bayes.NaiveBayes) \\"\\" 2)" "(defn build-classifier [classifier, arffFileName, nGramMaxSize]\\r\\n        (let [trainingData (load-instances \:arff \\"file\:resources/trainingDataSet.arff\\")\\r\\n              tokenizer (new weka.core.tokenizers.NGramTokenizer)\\r\\n              textToWordfilter (new weka.filters.unsupervised.attribute.StringToWordVector)\\r\\n              ranker (new weka.attributeSelection.Ranker)\\r\\n              asFilter (new weka.filters.supervised.attribute.AttributeSelection)\\r\\n              multiFilter (new weka.filters.MultiFilter)\\r\\n              filteredClassifier (new weka.classifiers.meta.FilteredClassifier)\\r\\n              eval (new weka.classifiers.Evaluation trainingData)]\\r\\n          (dataset-set-class trainingData 2)\\r\\n          (.setNGramMinSize tokenizer 1)\\r\\n          (.setNGramMaxSize tokenizer nGramMaxSize)\\r\\n          (.setDelimiters tokenizer \\" \\\\\\\\W\\")\\r\\n          (.setTokenizer textToWordfilter tokenizer)\\r\\n          (.setInputFormat textToWordfilter trainingData)\\r\\n          (.setWordsToKeep textToWordfilter 10000)\\r\\n          (.setDoNotOperateOnPerClassBasis textToWordfilter true)\\r\\n          (.setLowerCaseTokens textToWordfilter false)\\r\\n          (.setThreshold ranker 0.0)\\r\\n          (.setEvaluator asFilter (new weka.attributeSelection.InfoGainAttributeEval))\\r\\n          (.setSearch asFilter ranker)\\r\\n          (.setFilters multiFilter [textToWordfilter asFilter])\\r\\n          (.setClassifier filteredClassifier classifier)\\r\\n          (.setFilter filteredClassifier multiFilter)\\r\\n          (.buildClassifier filteredClassifier trainingData)\\r\\n          (.evaluateModel eval filteredClassifier trainingData)\\r\\n          (println (.toSummaryString eval))\\r\\n          (println (.toMatrixString eval))\\r\\n          ))" "(build-classifier (new weka.classifiers.bayes.NaiveBayes) \\"\\" 2)" "(defn build-classifier [classifier, arffFileName, nGramMaxSize]\\r\\n        (let [trainingData (load-instances \:arff \\"file\:resources/trainingDataSet.arff\\")\\r\\n              tokenizer (new weka.core.tokenizers.NGramTokenizer)\\r\\n              textToWordfilter (new weka.filters.unsupervised.attribute.StringToWordVector)\\r\\n              ranker (new weka.attributeSelection.Ranker)\\r\\n              asFilter (new weka.filters.supervised.attribute.AttributeSelection)\\r\\n              multiFilter (new weka.filters.MultiFilter)\\r\\n              filteredClassifier (new weka.classifiers.meta.FilteredClassifier)\\r\\n              eval (new weka.classifiers.Evaluation trainingData)]\\r\\n          (.setClassIndex trainingData 1)\\r\\n          (.setNGramMinSize tokenizer 1)\\r\\n          (.setNGramMaxSize tokenizer nGramMaxSize)\\r\\n          (.setDelimiters tokenizer \\" \\\\\\\\W\\")\\r\\n          (.setTokenizer textToWordfilter tokenizer)\\r\\n          (.setInputFormat textToWordfilter trainingData)\\r\\n          (.setWordsToKeep textToWordfilter 10000)\\r\\n          (.setDoNotOperateOnPerClassBasis textToWordfilter true)\\r\\n          (.setLowerCaseTokens textToWordfilter false)\\r\\n          (.setThreshold ranker 0.0)\\r\\n          (.setEvaluator asFilter (new weka.attributeSelection.InfoGainAttributeEval))\\r\\n          (.setSearch asFilter ranker)\\r\\n          (.setFilters multiFilter [textToWordfilter asFilter])\\r\\n          (.setClassifier filteredClassifier classifier)\\r\\n          (.setFilter filteredClassifier multiFilter)\\r\\n          (.buildClassifier filteredClassifier trainingData)\\r\\n          (.evaluateModel eval filteredClassifier trainingData)\\r\\n          (println (.toSummaryString eval))\\r\\n          (println (.toMatrixString eval))\\r\\n          ))" "(build-classifier (new weka.classifiers.bayes.NaiveBayes) \\"\\" 2)" "(defn build-classifier [classifier, arffFileName, nGramMaxSize]\\r\\n        (let [trainingData (load-instances \:arff \\"file\:resources/trainingDataSet.arff\\")\\r\\n              tokenizer (new weka.core.tokenizers.NGramTokenizer)\\r\\n              textToWordfilter (new weka.filters.unsupervised.attribute.StringToWordVector)\\r\\n              ranker (new weka.attributeSelection.Ranker)\\r\\n              asFilter (new weka.filters.supervised.attribute.AttributeSelection)\\r\\n              multiFilter (new weka.filters.MultiFilter)\\r\\n              filteredClassifier (new weka.classifiers.meta.FilteredClassifier)\\r\\n              eval (new weka.classifiers.Evaluation trainingData)]\\n          (trainingData)\\r\\n          (.setClassIndex trainingData 1)\\r\\n          (.setNGramMinSize tokenizer 1)\\r\\n          (.setNGramMaxSize tokenizer nGramMaxSize)\\r\\n          (.setDelimiters tokenizer \\" \\\\\\\\W\\")\\r\\n          (.setTokenizer textToWordfilter tokenizer)\\r\\n          (.setInputFormat textToWordfilter trainingData)\\r\\n          (.setWordsToKeep textToWordfilter 10000)\\r\\n          (.setDoNotOperateOnPerClassBasis textToWordfilter true)\\r\\n          (.setLowerCaseTokens textToWordfilter false)\\r\\n          (.setThreshold ranker 0.0)\\r\\n          (.setEvaluator asFilter (new weka.attributeSelection.InfoGainAttributeEval))\\r\\n          (.setSearch asFilter ranker)\\r\\n          (.setFilters multiFilter [textToWordfilter asFilter])\\r\\n          (.setClassifier filteredClassifier classifier)\\r\\n          (.setFilter filteredClassifier multiFilter)\\r\\n          (.buildClassifier filteredClassifier trainingData)\\r\\n          (.evaluateModel eval filteredClassifier trainingData)\\r\\n          (println (.toSummaryString eval))\\r\\n          (println (.toMatrixString eval))\\r\\n          ))" "(build-classifier (new weka.classifiers.bayes.NaiveBayes) \\"\\" 2)" "(def ds (load-instances \:arff \\"file\:resources/trainingDataSet.arff\\"))\\r\\n\\r\\n(clj-ml.data/dataset-set-class ds 1)" "(def tokenizer (new weka.core.tokenizers.NGramTokenizer))" "tokenizer" "(.setNGramMinSize tokenizer 1)" "(.setNGramMaxSize tokenizer 2)" "(.setDelimiters tokenizer \\" \\\\\\\\W\\")" "(def textToWordFilter (new weka.filters.unsupervised.attribute.StringToWordVector))" "(.setTokenizer textToWordFilter tokenizer)" "(.setInputFormat textToWordFilter trainingData)" "(.setInputFormat textToWordFilter ds)" "(.setWordsToKeep textToWordFilter 10000)" "(.setDoNotOperateOnPerClassBasis textToWordFilter true)" "(.setLowerCaseTokens textToWordFilter false)" "(def ranker (new weka.attributeSelection.Ranker))" "(.setTreshold ranker 0.0)" "(.setThreshold ranker 0.0)" "(def asFilter (new weka.filters.supervised.attribute.AttributeSelection))" "(.setEvaluator asFilter (new weka.attributeSelection.InfoGainAttributeEval))" "(.setSearch asFilter ranker)" "(def multiFilter (new weka.filters.MultiFilter))" "(def filteredClassifier (new weka.classifiers.meta.FilteredClassifier))" "(def eval (new weka.classifiers.Evaluation trainingData))" "(def eval (new weka.classifiers.Evaluation ds))" "(.setFilters multiFilter [textToWordFilter asFilter])" "(.setClassifier filteredClassifier classifier)" "(def classifier (new weka.classifiers.bayes.NaiveBayes))" "(.setClassifier filteredClassifier classifier)" "(.buildClassifier filteredClassifier trainingData)" "(.buildClassifier filteredClassifier ds)" "(def Filters[])" "(def Filters (new Filter[]))" "(def Filters (new weka.filters.Filter[]))" "(def Filters (new weka.filters.Filter))" "(def filters (make-array weka.filters.Filter 3))" "(.setFilters multiFilter (into-array weka.filters.Filter [textToWordfilter asFilter]))" "(.setFilters multiFilter (into-array weka.filters.Filter [textToWordFilter asFilter]))" "(.setClassifier filteredClassifier classifier)" "(.setFilter filteredClassifier multiFilter)" "(.buildClassifier filteredClassifier ds)" "(.evaluateModel eval filteredClassifier ds)" "eval" "(.evaluateModel eval filteredClassifier ds nil)" "(.evaluateModel eval filteredClassifier ds (new Object))" "(.evaluateModel eval filteredClassifier ds multiFilter)" "(.evaluateModel eval filteredClassifier ds)" "(.evaluateModel eval filteredClassifier ds \\"bla\\")" "(.evaluateModel eval filteredClassifier ds ds)" "(.evaluateModel eval filteredClassifier ds (make-array Object 2))" "(.evaluateModel eval filteredClassifier ds (make-array Object ))" "(.evaluateModel eval filteredClassifier ds (make-array Object 0))" "(.toSummaryString eval)" "(.toMatrixString eval)" "(defn build-classifier [classifier, arffFileName, nGramMaxSize]\\r\\n        (let [trainingData (load-instances \:arff \\"file\:resources/trainingDataSet.arff\\")\\r\\n              tokenizer (new weka.core.tokenizers.NGramTokenizer)\\r\\n              textToWordfilter (new weka.filters.unsupervised.attribute.StringToWordVector)\\r\\n              ranker (new weka.attributeSelection.Ranker)\\r\\n              asFilter (new weka.filters.supervised.attribute.AttributeSelection)\\r\\n              multiFilter (new weka.filters.MultiFilter)\\r\\n              filteredClassifier (new weka.classifiers.meta.FilteredClassifier)\\r\\n              eval (new weka.classifiers.Evaluation trainingData)]\\r\\n          (dataset-set-class trainingData 1)\\r\\n          (.setNGramMinSize tokenizer 1)\\r\\n          (.setNGramMaxSize tokenizer nGramMaxSize)\\r\\n          (.setDelimiters tokenizer \\" \\\\\\\\W\\")\\r\\n          (.setTokenizer textToWordfilter tokenizer)\\r\\n          (.setInputFormat textToWordfilter trainingData)\\r\\n          (.setWordsToKeep textToWordfilter 10000)\\r\\n          (.setDoNotOperateOnPerClassBasis textToWordfilter true)\\r\\n          (.setLowerCaseTokens textToWordfilter false)\\r\\n          (.setThreshold ranker 0.0)\\r\\n          (.setEvaluator asFilter (new weka.attributeSelection.InfoGainAttributeEval))\\r\\n          (.setSearch asFilter ranker)\\r\\n          (.setFilters multiFilter (into-array weka.filters.Filter [textToWordFilter asFilter]))\\r\\n          (.setClassifier filteredClassifier classifier)\\r\\n          (.setFilter filteredClassifier multiFilter)\\r\\n          (.buildClassifier filteredClassifier trainingData)\\r\\n          (.evaluateModel eval filteredClassifier trainingData (make-array Object 0))\\r\\n          (println (.toSummaryString eval))\\r\\n          (println (.toMatrixString eval))\\r\\n          ))" "(build-classifier (new weka.classifiers.bayes.NaiveBayes) \\"\\" 2)" "(dataset-set-class ds 1)" "(defn build-classifier [classifier, arffFileName, nGramMaxSize]\\r\\n        (let [trainingData ds\\r\\n              tokenizer (new weka.core.tokenizers.NGramTokenizer)\\r\\n              textToWordfilter (new weka.filters.unsupervised.attribute.StringToWordVector)\\r\\n              ranker (new weka.attributeSelection.Ranker)\\r\\n              asFilter (new weka.filters.supervised.attribute.AttributeSelection)\\r\\n              multiFilter (new weka.filters.MultiFilter)\\r\\n              filteredClassifier (new weka.classifiers.meta.FilteredClassifier)\\r\\n              eval (new weka.classifiers.Evaluation trainingData)]\\r\\n          (dataset-set-class trainingData 1)\\r\\n          (.setNGramMinSize tokenizer 1)\\r\\n          (.setNGramMaxSize tokenizer nGramMaxSize)\\r\\n          (.setDelimiters tokenizer \\" \\\\\\\\W\\")\\r\\n          (.setTokenizer textToWordfilter tokenizer)\\r\\n          (.setInputFormat textToWordfilter trainingData)\\r\\n          (.setWordsToKeep textToWordfilter 10000)\\r\\n          (.setDoNotOperateOnPerClassBasis textToWordfilter true)\\r\\n          (.setLowerCaseTokens textToWordfilter false)\\r\\n          (.setThreshold ranker 0.0)\\r\\n          (.setEvaluator asFilter (new weka.attributeSelection.InfoGainAttributeEval))\\r\\n          (.setSearch asFilter ranker)\\r\\n          (.setFilters multiFilter (into-array weka.filters.Filter [textToWordFilter asFilter]))\\r\\n          (.setClassifier filteredClassifier classifier)\\r\\n          (.setFilter filteredClassifier multiFilter)\\r\\n          (.buildClassifier filteredClassifier trainingData)\\r\\n          (.evaluateModel eval filteredClassifier trainingData (make-array Object 0))\\r\\n          (println (.toSummaryString eval))\\r\\n          (println (.toMatrixString eval))\\r\\n          ))" "(build-classifier (new weka.classifiers.bayes.NaiveBayes) \\"\\" 2)" "(def to-classify (make-instance ds {\:text \\"zasto bas sad \:(\\", \:sentiment-class \:negative}))" "ds" "(def to-classify (make-instance ds {\:text \\"zasto bas sad \:(\\", \:sentiment-class \\"negative\\"}))" "(def to-classify (make-instance ds {\:text \\"zasto bas sad \:(\\", \:sentiment_class \:negative}))" "(classifier-classify filteredClassifier to-classify)" "(classifier-label to-classify)" "(classifier-label filteredClassifier to-classify)" "to-classify" "(ns twitter_sentiment_analysis.data_formatting\\r\\n  (\:use\\r\\n   [clojure.java.io \:as io])\\r\\n  (\:require\\r\\n   [clojure.data.json \:as json]\\r\\n   [http.async.client \:as ac]))" "(def formatted-tweets [])\\r\\n\\r\\n(defn strip-emoticons [text]\\r\\n  (clojure.string/replace text \#\\"[\:;\=B](')*(-)*[)(P3DSO*sp]+\\" \\"\\"))\\r\\n\\r\\n(defn strip-usernames [text]\\r\\n  (clojure.string/replace text \#\\"@[A-Za-z0-9_]*\\" \\"USERNAME\\"))\\r\\n\\r\\n(defn strip-urls [text]\\r\\n  (clojure.string/replace text \#\\"http(s)*\://t.co/[A-Za-z0-9_]*\\" \\"URL\\"))\\r\\n\\r\\n(defn strip-repeated-letters [text]\\r\\n  (clojure.string/replace text \#\\"([a-zA-Z])\\\\1+\\" \\"$1$1\\"))" "(defn reduce-features [path]\\r\\n  (with-open [rdr (reader path)]\\r\\n  (doseq [line (line-seq rdr)]\\r\\n    (conj formatted-tweets ((comp strip-repeated-letters strip-urls strip-usernames strip-emoticons) (\:text (json/read-str line \:key-fn keyword)))))))" "(reduce-features \\"resources/positive.json\\")" "formatted-tweets" "(defn reduce-features [path]\\r\\n  (with-open [rdr (reader path)]\\r\\n  (doseq [line (line-seq rdr)]\\r\\n    (println ((comp strip-repeated-letters strip-urls strip-usernames strip-emoticons) (\:text (json/read-str line \:key-fn keyword)))))))" "(reduce-features \\"resources/positive.json\\")" "(conj [] 5)" "(defn reduce-features [path]\\r\\n  (with-open [rdr (reader path)]\\r\\n  (doseq [line (line-seq rdr)]\\r\\n    (conj formatted-tweets ((comp strip-repeated-letters strip-urls strip-usernames strip-emoticons) (\:text (json/read-str line \:key-fn keyword)))))))" "(reduce-features \\"resources/positive.json\\")" "formatted-tweets" "(defn reduce-features [path]\\r\\n  (with-open [rdr (reader path)]\\n  (let [formatted-tweets []]  \\r\\n  (doseq [line (line-seq rdr)]\\r\\n    (conj formatted-tweets ((comp strip-repeated-letters strip-urls strip-usernames strip-emoticons) (\:text (json/read-str line \:key-fn keyword)))))\\n   formatted-tweets)))" "(reduce-features \\"resources/positive.json\\")" "(conj [] \\"bla\\")" "(defn reduce-features [path]\\r\\n  (with-open [rdr (reader path)]\\n  (let [formatted-tweets [\\"bla\\"]]  \\r\\n  (doseq [line (line-seq rdr)]\\r\\n    (conj formatted-tweets ((comp strip-repeated-letters strip-urls strip-usernames strip-emoticons) (\:text (json/read-str line \:key-fn keyword)))))\\n   formatted-tweets)))" "(reduce-features \\"resources/positive.json\\")" "(defn reduce-features [path]\\r\\n  (with-open [rdr (reader path)]\\n  (let [formatted-tweets []]  \\r\\n  (doseq [line (line-seq rdr)]\\r\\n    (conj formatted-tweets ((comp strip-repeated-letters strip-urls strip-usernames strip-emoticons) (\:text (json/read-str line \:key-fn keyword))))))))" "(reduce-features \\"resources/positive.json\\")" "(conj []\\n      ((comp strip-repeated-letters strip-urls strip-usernames strip-emoticons) (\\"@blsdf aaa \:)\\")))" "(conj []\\n      ((comp strip-repeated-letters strip-urls strip-usernames strip-emoticons) \\"@blsdf aaa \:)\\"))" "(ns twitter-sentiment-analysis.data-analysis\\r\\n  (\:use [clj-ml.io] [clj-ml.filters] [clj-ml.classifiers] [clj-ml.data])\\r\\n  (\:import [weka.attributeSelection.InfoGainAttributeEval]\\r\\n           [weka.attributeSelection.Ranker]\\r\\n           [weka.classifiers.Classifier]\\r\\n           [weka.classifiers.Evaluation]\\r\\n           [weka.classifiers.meta.FilteredClassifier]\\r\\n           [weka.core.Attribute]\\r\\n           [weka.core.FastVector]\\r\\n           [weka.core.Instance]\\r\\n           [weka.core.Instances]\\r\\n           [weka.core.tokenizers.NGramTokenizer]\\r\\n           [weka.filters.Filter]\\r\\n           [weka.filters.MultiFilter]\\r\\n           [weka.filters.supervised.attribute.AttributeSelection]\\r\\n           [weka.filters.unsupervised.attribute.StringToWordVector]\\r\\n           ))" "(def ds (load-instances \:arff \\"file\:resources/trainingDataSet.arff\\"))" "(defn build-classifier [classifier, classifierFileName, nGramMaxSize, trainingData]\\r\\n        (let [tokenizer (new weka.core.tokenizers.NGramTokenizer)\\r\\n              textToWordfilter (new weka.filters.unsupervised.attribute.StringToWordVector)\\r\\n              ranker (new weka.attributeSelection.Ranker)\\r\\n              asFilter (new weka.filters.supervised.attribute.AttributeSelection)\\r\\n              multiFilter (new weka.filters.MultiFilter)\\r\\n              filteredClassifier (new weka.classifiers.meta.FilteredClassifier)\\r\\n              eval (new weka.classifiers.Evaluation trainingData)]\\r\\n          (dataset-set-class trainingData 1)\\r\\n          (.setNGramMinSize tokenizer 1)\\r\\n          (.setNGramMaxSize tokenizer nGramMaxSize)\\r\\n          (.setDelimiters tokenizer \\" \\\\\\\\W\\")\\r\\n          (.setTokenizer textToWordfilter tokenizer)\\r\\n          (.setInputFormat textToWordfilter trainingData)\\r\\n          (.setWordsToKeep textToWordfilter 10000)\\r\\n          (.setDoNotOperateOnPerClassBasis textToWordfilter true)\\r\\n          (.setLowerCaseTokens textToWordfilter false)\\r\\n          (.setThreshold ranker 0.0)\\r\\n          (.setEvaluator asFilter (new weka.attributeSelection.InfoGainAttributeEval))\\r\\n          (.setSearch asFilter ranker)\\r\\n          (.setFilters multiFilter (into-array weka.filters.Filter [textToWordFilter asFilter]))\\r\\n          (.setClassifier filteredClassifier classifier)\\r\\n          (.setFilter filteredClassifier multiFilter)\\r\\n          (.buildClassifier filteredClassifier trainingData)\\r\\n          (serialize-to-file filteredClassifier classifierFileName)\\r\\n          (.evaluateModel eval filteredClassifier trainingData (make-array Object 0))\\r\\n          (println (.toSummaryString eval))\\r\\n          (println (.toMatrixString eval))\\r\\n          ))" "(defn build-classifier [classifier, classifierFileName, nGramMaxSize, trainingData]\\r\\n        (let [tokenizer (new weka.core.tokenizers.NGramTokenizer)\\r\\n              textToWordFilter (new weka.filters.unsupervised.attribute.StringToWordVector)\\r\\n              ranker (new weka.attributeSelection.Ranker)\\r\\n              asFilter (new weka.filters.supervised.attribute.AttributeSelection)\\r\\n              multiFilter (new weka.filters.MultiFilter)\\r\\n              filteredClassifier (new weka.classifiers.meta.FilteredClassifier)\\r\\n              eval (new weka.classifiers.Evaluation trainingData)]\\r\\n          (dataset-set-class trainingData 1)\\r\\n          (.setNGramMinSize tokenizer 1)\\r\\n          (.setNGramMaxSize tokenizer nGramMaxSize)\\r\\n          (.setDelimiters tokenizer \\" \\\\\\\\W\\")\\r\\n          (.setTokenizer textToWordFilter tokenizer)\\r\\n          (.setInputFormat textToWordFilter trainingData)\\r\\n          (.setWordsToKeep textToWordFilter 10000)\\r\\n          (.setDoNotOperateOnPerClassBasis textToWordFilter true)\\r\\n          (.setLowerCaseTokens textToWordFilter false)\\r\\n          (.setThreshold ranker 0.0)\\r\\n          (.setEvaluator asFilter (new weka.attributeSelection.InfoGainAttributeEval))\\r\\n          (.setSearch asFilter ranker)\\r\\n          (.setFilters multiFilter (into-array weka.filters.Filter [textToWordFilter asFilter]))\\r\\n          (.setClassifier filteredClassifier classifier)\\r\\n          (.setFilter filteredClassifier multiFilter)\\r\\n          (.buildClassifier filteredClassifier trainingData)\\r\\n          (serialize-to-file filteredClassifier classifierFileName)\\r\\n          (.evaluateModel eval filteredClassifier trainingData (make-array Object 0))\\r\\n          (println (.toSummaryString eval))\\r\\n          (println (.toMatrixString eval))\\r\\n          ))" "(ns twitter-sentiment-analysis.data-analysis\\r\\n  (\:use [clj-ml.io] [clj-ml.filters] [clj-ml.classifiers] [clj-ml.data] [clj-ml.utils])\\r\\n  (\:import [weka.attributeSelection.InfoGainAttributeEval]\\r\\n           [weka.attributeSelection.Ranker]\\r\\n           [weka.classifiers.Classifier]\\r\\n           [weka.classifiers.Evaluation]\\r\\n           [weka.classifiers.meta.FilteredClassifier]\\r\\n           [weka.core.Attribute]\\r\\n           [weka.core.FastVector]\\r\\n           [weka.core.Instance]\\r\\n           [weka.core.Instances]\\r\\n           [weka.core.tokenizers.NGramTokenizer]\\r\\n           [weka.filters.Filter]\\r\\n           [weka.filters.MultiFilter]\\r\\n           [weka.filters.supervised.attribute.AttributeSelection]\\r\\n           [weka.filters.unsupervised.attribute.StringToWordVector]\\r\\n           ))" "(def ds (load-instances \:arff \\"file\:resources/trainingDataSet.arff\\"))" "(defn build-classifier [classifier, classifierFileName, nGramMaxSize, trainingData]\\r\\n        (let [tokenizer (new weka.core.tokenizers.NGramTokenizer)\\r\\n              textToWordFilter (new weka.filters.unsupervised.attribute.StringToWordVector)\\r\\n              ranker (new weka.attributeSelection.Ranker)\\r\\n              asFilter (new weka.filters.supervised.attribute.AttributeSelection)\\r\\n              multiFilter (new weka.filters.MultiFilter)\\r\\n              filteredClassifier (new weka.classifiers.meta.FilteredClassifier)\\r\\n              eval (new weka.classifiers.Evaluation trainingData)]\\r\\n          (dataset-set-class trainingData 1)\\r\\n          (.setNGramMinSize tokenizer 1)\\r\\n          (.setNGramMaxSize tokenizer nGramMaxSize)\\r\\n          (.setDelimiters tokenizer \\" \\\\\\\\W\\")\\r\\n          (.setTokenizer textToWordFilter tokenizer)\\r\\n          (.setInputFormat textToWordFilter trainingData)\\r\\n          (.setWordsToKeep textToWordFilter 10000)\\r\\n          (.setDoNotOperateOnPerClassBasis textToWordFilter true)\\r\\n          (.setLowerCaseTokens textToWordFilter false)\\r\\n          (.setThreshold ranker 0.0)\\r\\n          (.setEvaluator asFilter (new weka.attributeSelection.InfoGainAttributeEval))\\r\\n          (.setSearch asFilter ranker)\\r\\n          (.setFilters multiFilter (into-array weka.filters.Filter [textToWordFilter asFilter]))\\r\\n          (.setClassifier filteredClassifier classifier)\\r\\n          (.setFilter filteredClassifier multiFilter)\\r\\n          (.buildClassifier filteredClassifier trainingData)\\r\\n          (serialize-to-file filteredClassifier classifierFileName)\\r\\n          (.evaluateModel eval filteredClassifier trainingData (make-array Object 0))\\r\\n          (println (.toSummaryString eval))\\r\\n          (println (.toMatrixString eval))\\r\\n          ))" "(defn classify-a-tweet [classifierFileName text]\\r\\n  (let to-classify (make-instance ds {\:text text, \:sentiment_class \:negative}))\\r\\n\\t(classifier-classify (deserialize-from-file classifierFileName) to-classify)\\r\\n\\t(classifier-label (deserialize-from-file classifierFileName) to-classify))" "(defn classify-a-tweet [classifierFileName text]\\r\\n  (let [to-classify (make-instance ds {\:text text, \:sentiment_class \:negative})])\\r\\n\\t(classifier-classify (deserialize-from-file classifierFileName) to-classify)\\r\\n\\t(classifier-label (deserialize-from-file classifierFileName) to-classify))" "(defn classify-a-tweet [classifierFileName text]\\r\\n  (let [to-classify \\"bla\\"])\\r\\n\\t(classifier-classify (deserialize-from-file classifierFileName) to-classify)\\r\\n\\t(classifier-label (deserialize-from-file classifierFileName) to-classify))" "(defn classify-a-tweet [classifierFileName text]\\r\\n  (let [to-classify (make-instance ds {\:text text, \:sentiment_class \:negative})]\\r\\n\\t(classifier-classify (deserialize-from-file classifierFileName) to-classify)\\r\\n\\t(classifier-label (deserialize-from-file classifierFileName) to-classify)))" "(build-classifier (new NaiveBayes) \\"resources/classifier.txt\\" 2 ds)" "(build-classifier (new weka.classifiers.bayes.NaiveBayes) \\"resources/classifier.txt\\" 2 ds)" "(dataset-set-class trainingData 1)" "(dataset-set-class ds 1)" "(defn build-classifier [classifier, classifierFileName, nGramMaxSize, trainingData]\\r\\n        (let [tokenizer (new weka.core.tokenizers.NGramTokenizer)\\r\\n              textToWordFilter (new weka.filters.unsupervised.attribute.StringToWordVector)\\r\\n              ranker (new weka.attributeSelection.Ranker)\\r\\n              asFilter (new weka.filters.supervised.attribute.AttributeSelection)\\r\\n              multiFilter (new weka.filters.MultiFilter)\\r\\n              filteredClassifier (new weka.classifiers.meta.FilteredClassifier)\\r\\n              eval (new weka.classifiers.Evaluation trainingData)\\r\\n              ds (dataset-set-class trainingData 1)]\\r\\n          (.setNGramMinSize tokenizer 1)\\r\\n          (.setNGramMaxSize tokenizer nGramMaxSize)\\r\\n          (.setDelimiters tokenizer \\" \\\\\\\\W\\")\\r\\n          (.setTokenizer textToWordFilter tokenizer)\\r\\n          (.setInputFormat textToWordFilter trainingData)\\r\\n          (.setWordsToKeep textToWordFilter 10000)\\r\\n          (.setDoNotOperateOnPerClassBasis textToWordFilter true)\\r\\n          (.setLowerCaseTokens textToWordFilter false)\\r\\n          (.setThreshold ranker 0.0)\\r\\n          (.setEvaluator asFilter (new weka.attributeSelection.InfoGainAttributeEval))\\r\\n          (.setSearch asFilter ranker)\\r\\n          (.setFilters multiFilter (into-array weka.filters.Filter [textToWordFilter asFilter]))\\r\\n          (.setClassifier filteredClassifier classifier)\\r\\n          (.setFilter filteredClassifier multiFilter)\\r\\n          (.buildClassifier filteredClassifier trainingData)\\r\\n          (serialize-to-file filteredClassifier classifierFileName)\\r\\n          (.evaluateModel eval filteredClassifier trainingData (make-array Object 0))\\r\\n          (println (.toSummaryString eval))\\r\\n          (println (.toMatrixString eval))\\r\\n          ))" "(build-classifier (new weka.classifiers.bayes.NaiveBayes) \\"resources/classifier.txt\\" 2 ds)" "(classify-a-tweet \\"resources/classifier.txt\\" \\"mislim da je gotovo\\")" "(classify-a-tweet \\"resources/classifier.txt\\" \\"jeeee, srecaaaa\\")" "(defn classify-a-tweet [classifierFileName text]\\r\\n  (let to-classify (make-instance ds {\:text text, \:sentiment_class \:positive}))\\r\\n\\t(classifier-classify (deserialize-from-file classifierFileName) to-classify)\\r\\n\\t(classifier-label (deserialize-from-file classifierFileName) to-classify))" "(defn classify-a-tweet [classifierFileName text]\\r\\n  (let [to-classify (make-instance ds {\:text text, \:sentiment_class \:positive})]\\r\\n\\t(classifier-classify (deserialize-from-file classifierFileName) to-classify)\\r\\n\\t(classifier-label (deserialize-from-file classifierFileName) to-classify)))" "(classify-a-tweet \\"resources/classifier.txt\\" \\"jeeee, srecaaaa\\")" "(defn classify-a-tweet [classifierFileName text]\\r\\n  (let [to-classify (make-instance ds {\:text text, \:sentiment_class \:positive})]\\r\\n\\t(classifier-classify (deserialize-from-file classifierFileName) to-classify)\\r\\n\\t))" "(classify-a-tweet \\"resources/classifier.txt\\" \\"jeeee, srecaaaa\\")" "(defn classify-a-tweet [classifierFileName text]\\r\\n  (let [to-classify (make-instance ds {\:text text, \:sentiment_class \:positive})]\\r\\n\\t(classifier-label (deserialize-from-file classifierFileName) to-classify)))" "(classify-a-tweet \\"resources/classifier.txt\\" \\"jeeee, srecaaaa\\")" "(defn classify-a-tweet [classifierFileName text]\\r\\n  (let [to-classify (make-instance ds {\:text text, \:sentiment_class \:positive})]\\r\\n\\t(\:sentiment_class (classifier-label (deserialize-from-file classifierFileName) to-classify))))" "(classify-a-tweet \\"resources/classifier.txt\\" \\"jeeee, srecaaaa\\")" "(defn classify-a-tweet [classifierFileName text]\\r\\n  (let [to-classify (make-instance ds {\:text text, \:sentiment_class \:positive})]\\r\\n\\t(\:sentiment-class (classifier-label (deserialize-from-file classifierFileName) to-classify))))" "(classify-a-tweet \\"resources/classifier.txt\\" \\"jeeee, srecaaaa\\")" "(defn classify-a-tweet [classifierFileName text]\\r\\n  (let [to-classify (make-instance ds {\:text text, \:sentiment_class \:positive})]\\r\\n\\t(\:text (classifier-label (deserialize-from-file classifierFileName) to-classify))))" "(classify-a-tweet \\"resources/classifier.txt\\" \\"jeeee, srecaaaa\\")" "(defn classify-a-tweet [classifierFileName text]\\r\\n  (let [to-classify (make-instance ds {\:text text, \:sentiment_class \:positive})]\\r\\n\\t(instance-get-class (classifier-label (deserialize-from-file classifierFileName) to-classify))))" "(classify-a-tweet \\"resources/classifier.txt\\" \\"jeeee, srecaaaa\\")" "(defn classify-a-tweet [classifierFileName text]\\r\\n  (let [to-classify (make-instance ds {\:text text, \:sentiment_class \:positive})]\\r\\n\\t(instance-value-at (classifier-label (deserialize-from-file classifierFileName) to-classify) 1)))" "(classify-a-tweet \\"resources/classifier.txt\\" \\"jeeee, srecaaaa\\")" "(defn classify-a-tweet [classifierFileName text]\\r\\n  (let [to-classify (make-instance ds {\:text text, \:sentiment_class \:positive})]\\r\\n\\t(\:sentiment_class (instance-to-vector (classifier-label (deserialize-from-file classifierFileName) to-classify)))))" "(ns twitter_sentiment_analysis.data_analysis\\r\\n  (\:use [clj-ml.io] [clj-ml.filters] [clj-ml.classifiers] [clj-ml.data] [clj-ml.utils])\\r\\n  (\:import [weka.attributeSelection.InfoGainAttributeEval]\\r\\n           [weka.attributeSelection.Ranker]\\r\\n           [weka.classifiers.Classifier]\\r\\n           [weka.classifiers.Evaluation]\\r\\n           [weka.classifiers.meta.FilteredClassifier]\\r\\n           [weka.core.Attribute]\\r\\n           [weka.core.FastVector]\\r\\n           [weka.core.Instance]\\r\\n           [weka.core.Instances]\\r\\n           [weka.core.tokenizers.NGramTokenizer]\\r\\n           [weka.filters.Filter]\\r\\n           [weka.filters.MultiFilter]\\r\\n           [weka.filters.supervised.attribute.AttributeSelection]\\r\\n           [weka.filters.unsupervised.attribute.StringToWordVector]\\r\\n           ))" "(defn classify-a-tweet [classifierFileName text]\\r\\n  (let [to-classify (make-instance ds {\:text text, \:sentiment_class \:positive})]\\r\\n\\t(\:sentiment_class (instance-to-vector (classifier-label (deserialize-from-file classifierFileName) to-classify)))))" "(defn classify-a-tweet [classifierFileName text]\\r\\n  (let [empty-data-set (dataset-set-class (load-instances \:arff \\"file\:resources/trainingDataSet.arff\\") 1)\\r\\n        to-classify (make-instance empty-data-set {\:text text, \:sentiment_class \:negative})]\\r\\n\\t(classifier-classify (deserialize-from-file classifierFileName) to-classify)\\r\\n\\t(classifier-label (deserialize-from-file classifierFileName) to-classify)))" "(classify-a-tweet \\"resources/classifier.txt\\" \\"jeeee, srecaaaa\\")" "(defn classify-a-tweet [classifierFileName text]\\r\\n  (let [empty-data-set (dataset-set-class (load-instances \:arff \\"file\:resources/trainingDataSet.arff\\") 1)\\r\\n        to-classify (make-instance empty-data-set {\:text text, \:sentiment_class \:negative})]\\r\\n\\t(\:sentiment_class (instance-to-vector (classifier-label (deserialize-from-file classifierFileName) to-classify)))))" "(classify-a-tweet \\"resources/classifier.txt\\" \\"jeeee, srecaaaa\\")" "(defn classify-a-tweet [classifierFileName text]\\r\\n  (let [empty-data-set (dataset-set-class (load-instances \:arff \\"file\:resources/trainingDataSet.arff\\") 1)\\r\\n        to-classify (make-instance empty-data-set {\:text text, \:sentiment_class \:negative})]\\r\\n\\t(instance-to-vector (classifier-label (deserialize-from-file classifierFileName) to-classify))))" "(classify-a-tweet \\"resources/classifier.txt\\" \\"jeeee, srecaaaa\\")" "(defn classify-a-tweet [classifierFileName text]\\r\\n     (let [empty-data-set (dataset-set-class (load-instances \:arff \\"file\:resources/trainingDataSet.arff\\") 1)\\r\\n           to-classify (make-instance empty-data-set {\:text text, \:sentiment_class \:negative})]\\r\\n   \\t(instance-to-vector (classifier-label (deserialize-from-file classifierFileName) to-classify))))" "(classify-a-tweet \\"resources/classifier.txt\\" \\"jeeee, srecaaaa\\")" "(defn classify-a-tweet [classifierFileName text]\\r\\n     (let [empty-data-set (dataset-set-class (load-instances \:arff \\"file\:resources/trainingDataSet.arff\\") 1)\\r\\n           to-classify (make-instance empty-data-set {\:text text, \:sentiment_class \:negative})]\\r\\n   \\t(classifier-label (deserialize-from-file classifierFileName) to-classify)))" "(classify-a-tweet \\"resources/classifier.txt\\" \\"jeeee, srecaaaa\\")" "(instance-to-vector (classify-a-tweet \\"resources/classifier.txt\\" \\"jeeee, srecaaaa\\"))" "(def vektor (instance-to-vector (classify-a-tweet \\"resources/classifier.txt\\" \\"jeeee, srecaaaa\\")))" "vektor" "(defn get-sentiment [classifierFileName text]\\r\\n  (let [sentiment classify-a-tweet classifier-name text]\\r\\n  (if(\= sentiment 0)\\r\\n    \\"positive\\"\\r\\n    \\"negative\\")))" "(get-sentiment \\"resources/classifier.txt\\" \\"jeeee, srecaaaa\\")" "(ns twitter_sentiment_analysis.data_formatting\\r\\n  (\:use\\r\\n   [clojure.java.io \:as io])\\r\\n  (\:require\\r\\n   [clojure.data.json \:as json]\\r\\n   [http.async.client \:as ac]))\\r\\n\\r\\n(def formatted-tweets [])\\r\\n\\r\\n(defn strip-emoticons [text]\\r\\n  (clojure.string/replace text \#\\"[\:;\=B](')*(-)*[)(P3DSO*sp]+\\" \\"\\"))\\r\\n\\r\\n(defn strip-usernames [text]\\r\\n  (clojure.string/replace text \#\\"@[A-Za-z0-9_]*\\" \\"USERNAME\\"))\\r\\n\\r\\n(defn strip-urls [text]\\r\\n  (clojure.string/replace text \#\\"http(s)*\://t.co/[A-Za-z0-9_]*\\" \\"URL\\"))\\r\\n\\r\\n(defn strip-repeated-letters [text]\\r\\n  (clojure.string/replace text \#\\"([a-zA-Z])\\\\1+\\" \\"$1$1\\"))" "(defn reduce-features [path]\\r\\n  (with-open [rdr (reader path)]\\r\\n  (doseq [line (line-seq rdr)]\\r\\n    (println ((comp strip-repeated-letters strip-urls strip-usernames strip-emoticons) (\:text (json/read-str line \:key-fn keyword)))))))" "(defn reduce-features [path]\\r\\n  (with-open [rdr (reader path)]\\r\\n  (doseq [line (line-seq rdr)]\\r\\n    (swap\! formatted-tweets conj ((comp strip-repeated-letters strip-urls strip-usernames strip-emoticons) (\:text (json/read-str line \:key-fn keyword)))))))" "(reduce-features \\"resources/positive.json\\")" "(def formatted-tweets (atom ()))" "(defn reduce-features [path]\\r\\n  (with-open [rdr (reader path)]\\r\\n  (doseq [line (line-seq rdr)]\\r\\n    (swap\! formatted-tweets conj ((comp strip-repeated-letters strip-urls strip-usernames strip-emoticons) (\:text (json/read-str line \:key-fn keyword)))))))" "(reduce-features \\"resources/positive.json\\")" "formatted-tweets" "(ns twitter_sentiment_analysis.data_collection\\r\\n  (\:use\\r\\n   [twitter.oauth]\\r\\n   [twitter.callbacks]\\r\\n   [twitter.callbacks.handlers]\\r\\n   [twitter.api.streaming]\\r\\n   [clojure.java.io \:as io])\\r\\n  (\:require\\r\\n   [clojure.data.json \:as json]\\r\\n   [http.async.client \:as ac])\\r\\n  (\:import\\r\\n   (twitter.callbacks.protocols AsyncStreamingCallback)))" "(def my-creds (make-oauth-creds \\"nemg8aXAA9wtSo8h6ZZbrsYwk\\"\\r\\n                                \\"XmRZzVqlj1i5Ag72RJpd99IQLPoK6dfvbna89SwQ1NHIJVqN1l\\"\\r\\n                                \\"3021555060-YgU9ZHHTKVQGpkbDqNw9By4VNcFz7Ph455VAY0G\\"\\r\\n                                \\"XR9MZ3SE6quDbTfGcPNTiG2QebsdYnepStnZnrrzUeuye\\"))\\r\\n(def no-of-positive 0)\\r\\n(def no-of-negative 0)" "(defn collect-negative-tweets\\r\\n\\t(let [w (io/writer \\"negative.json\\")\\r\\n\\t      callback (AsyncStreamingCallback.\\r\\n\\t                 (fn [_resp payload]\\r\\n\\t                   (if (< no-of-positive 1000)\\r\\n\\t\\t                   (do\\r\\n\\t\\t                      (.write w (str payload))\\r\\n\\t\\t                      (inc no-of-tweets))\\r\\n\\t\\t                   (.close w)))\\r\\n\\t                 (fn [_resp]\\r\\n\\t                   (.close w))\\r\\n\\t                 (fn [_resp ex]\\r\\n\\t                   (.close w)\\r\\n\\t                   (.printStackTrace ex)))]\\r\\n\\t  (statuses-filter\\r\\n\\t    \:params {\:lang \\"en\\" \:track \\"\:)\\"}\\r\\n\\t    \:oauth-creds my-creds\\r\\n\\t    \:callbacks callback)))" "(defn collect-negative-tweets\\r\\n\\t(let [w (io/writer \\"negative.json\\")]\\r\\n\\t      callback (AsyncStreamingCallback.\\r\\n\\t                 (fn [_resp payload]\\r\\n\\t                   (if (< no-of-positive 1000)\\r\\n\\t\\t                   (do\\r\\n\\t\\t                      (.write w (str payload))\\r\\n\\t\\t                      (inc no-of-tweets))\\r\\n\\t\\t                   (.close w)))\\r\\n\\t                 (fn [_resp]\\r\\n\\t                   (.close w))\\r\\n\\t                 (fn [_resp ex]\\r\\n\\t                   (.close w)\\r\\n\\t                   (.printStackTrace ex)))\\r\\n\\t  (statuses-filter\\r\\n\\t    \:params {\:lang \\"en\\" \:track \\"\:)\\"}\\r\\n\\t    \:oauth-creds my-creds\\r\\n\\t    \:callbacks callback)))" "(defn collect-positive-tweets []\\r\\n\\t(let [w (io/writer \\"positive.json\\")\\r\\n\\t      callback (AsyncStreamingCallback.\\r\\n\\t                 (fn [_resp payload]\\r\\n\\t                   (if (< no-of-positive 1000)\\r\\n\\t\\t                   (do\\r\\n\\t\\t                      (.write w (str payload))\\r\\n\\t\\t                      (inc no-of-tweets))\\r\\n\\t\\t                   (.close w)))\\r\\n\\t                 (fn [_resp]\\r\\n\\t                   (.close w))\\r\\n\\t                 (fn [_resp ex]\\r\\n\\t                   (.close w)\\r\\n\\t                   (.printStackTrace ex)))]\\r\\n\\t  (statuses-filter\\r\\n\\t    \:params {\:lang \\"en\\" \:track \\"\:)\\"}\\r\\n\\t    \:oauth-creds my-creds\\r\\n\\t    \:callbacks callback)))" "(defn collect-positive-tweets []\\r\\n\\t(let [w (io/writer \\"positive.json\\")\\r\\n\\t      callback (AsyncStreamingCallback.\\r\\n\\t                 (fn [_resp payload]\\r\\n\\t                   (if (< no-of-positive 1000)\\r\\n\\t\\t                   (do\\r\\n\\t\\t                      (.write w (str payload))\\r\\n\\t\\t                      (inc no-of-positive))\\r\\n\\t\\t                   (.close w)))\\r\\n\\t                 (fn [_resp]\\r\\n\\t                   (.close w))\\r\\n\\t                 (fn [_resp ex]\\r\\n\\t                   (.close w)\\r\\n\\t                   (.printStackTrace ex)))]\\r\\n\\t  (statuses-filter\\r\\n\\t    \:params {\:lang \\"en\\" \:track \\"\:)\\"}\\r\\n\\t    \:oauth-creds my-creds\\r\\n\\t    \:callbacks callback)))" "(defn collect-negative-tweets []\\r\\n\\t(let [w (io/writer \\"negative.json\\")\\r\\n\\t      callback (AsyncStreamingCallback.\\r\\n\\t                 (fn [_resp payload]\\r\\n\\t                   (if (< no-of-negative 1000)\\r\\n\\t\\t                   (do\\r\\n\\t\\t                      (.write w (str payload))\\r\\n\\t\\t                      (inc no-of-negative))\\r\\n\\t\\t                   (.close w)))\\r\\n\\t                 (fn [_resp]\\r\\n\\t                   (.close w))\\r\\n\\t                 (fn [_resp ex]\\r\\n\\t                   (.close w)\\r\\n\\t                   (.printStackTrace ex)))]\\r\\n\\t  (statuses-filter\\r\\n\\t    \:params {\:lang \\"en\\" \:track \\"\:(\\"}\\r\\n\\t    \:oauth-creds my-creds\\r\\n\\t    \:callbacks callback)))" "(ns twitter_sentiment_analysis.data_collection\\r\\n  (\:use\\r\\n   [twitter.oauth]\\r\\n   [twitter.callbacks]\\r\\n   [twitter.callbacks.handlers]\\r\\n   [twitter.api.streaming]\\r\\n   [clojure.java.io \:as io])\\r\\n  (\:require\\r\\n   [clojure.data.json \:as json]\\r\\n   [http.async.client \:as ac])\\r\\n  (\:import\\r\\n   (twitter.callbacks.protocols AsyncStreamingCallback)))" "(def my-creds (make-oauth-creds \\"nemg8aXAA9wtSo8h6ZZbrsYwk\\"\\r\\n                                \\"XmRZzVqlj1i5Ag72RJpd99IQLPoK6dfvbna89SwQ1NHIJVqN1l\\"\\r\\n                                \\"3021555060-YgU9ZHHTKVQGpkbDqNw9By4VNcFz7Ph455VAY0G\\"\\r\\n                                \\"XR9MZ3SE6quDbTfGcPNTiG2QebsdYnepStnZnrrzUeuye\\"))" "(def no-of-positive 0)\\r\\n(def no-of-negative 0)" "(defn collect-positive-tweets []\\r\\n\\t(let [w (io/writer \\"positive.json\\")\\r\\n\\t      callback (AsyncStreamingCallback.\\r\\n\\t                 (fn [_resp payload]\\r\\n\\t                   (if (< no-of-positive 1000)\\r\\n\\t\\t                   (do\\r\\n\\t\\t                      (.write w (str payload))\\r\\n\\t\\t                      (inc no-of-positive))\\r\\n\\t\\t                   (.close w)))\\r\\n\\t                 (fn [_resp]\\r\\n\\t                   (.close w))\\r\\n\\t                 (fn [_resp ex]\\r\\n\\t                   (.close w)\\r\\n\\t                   (.printStackTrace ex)))]\\r\\n\\t  (statuses-filter\\r\\n\\t    \:params {\:lang \\"en\\" \:track \\"\:)\\" \:count \\"1000\\"}\\r\\n\\t    \:oauth-creds my-creds\\r\\n\\t    \:callbacks callback)))" "(defn collect-negative-tweets []\\r\\n\\t(let [w (io/writer \\"negative.json\\")\\r\\n\\t      callback (AsyncStreamingCallback.\\r\\n\\t                 (fn [_resp payload]\\r\\n\\t                   (if (< no-of-negative 1000)\\r\\n\\t\\t                   (do\\r\\n\\t\\t                      (.write w (str payload))\\r\\n\\t\\t                      (inc no-of-negative))\\r\\n\\t\\t                   (.close w)))\\r\\n\\t                 (fn [_resp]\\r\\n\\t                   (.close w))\\r\\n\\t                 (fn [_resp ex]\\r\\n\\t                   (.close w)\\r\\n\\t                   (.printStackTrace ex)))]\\r\\n\\t  (statuses-filter\\r\\n\\t    \:params {\:lang \\"en\\" \:track \\"\:(\\" \:count \\"1000\\"}\\r\\n\\t    \:oauth-creds my-creds\\r\\n\\t    \:callbacks callback)))" "(collect-positive-tweets)" "(ns twitter_sentiment_analysis.data_collection\\r\\n  (\:use\\r\\n   [twitter.oauth]\\r\\n   [twitter.callbacks]\\r\\n   [twitter.callbacks.handlers]\\r\\n   [twitter.api.streaming]\\r\\n   [clojure.java.io \:as io])\\r\\n  (\:require\\r\\n   [clojure.data.json \:as json]\\r\\n   [http.async.client \:as ac])\\r\\n  (\:import\\r\\n   (twitter.callbacks.protocols AsyncStreamingCallback)))\\r\\n\\r\\n(def my-creds (make-oauth-creds \\"nemg8aXAA9wtSo8h6ZZbrsYwk\\"\\r\\n                                \\"XmRZzVqlj1i5Ag72RJpd99IQLPoK6dfvbna89SwQ1NHIJVqN1l\\"\\r\\n                                \\"3021555060-YgU9ZHHTKVQGpkbDqNw9By4VNcFz7Ph455VAY0G\\"\\r\\n                                \\"XR9MZ3SE6quDbTfGcPNTiG2QebsdYnepStnZnrrzUeuye\\"))\\r\\n(def no-of-positive 0)\\r\\n(def no-of-negative 0)" "(defn collect-positive-tweets []\\r\\n\\t(let [w (io/writer \\"positive.json\\")\\r\\n\\t      callback (AsyncStreamingCallback.\\r\\n\\t                 (fn [_resp payload]\\r\\n\\t                   (if (< no-of-positive 1000)\\r\\n\\t\\t                   (do\\r\\n\\t\\t                      (.write w (str payload))\\r\\n\\t\\t                      (inc no-of-positive))\\r\\n\\t\\t                   (.close w)))\\r\\n\\t                 (fn [_resp]\\r\\n\\t                   (.close w))\\r\\n\\t                 (fn [_resp ex]\\r\\n\\t                   (.close w)\\r\\n\\t                   (.printStackTrace ex)))]\\r\\n\\t  (statuses-filter\\r\\n\\t    \:params {\:lang \\"en\\" \:track \\"\:)\\" \:count \\"1000\\"}\\r\\n\\t    \:oauth-creds my-creds\\r\\n\\t    \:callbacks callback)))" "(collect-positive-tweets)" "(defn collect-positive-tweets []\\r\\n\\t(let [w (io/writer \\"positive.json\\")\\r\\n\\t      callback (AsyncStreamingCallback.\\r\\n\\t                 (fn [_resp payload]\\r\\n\\t                   (if (< no-of-positive 1000)\\r\\n\\t\\t                   (do\\r\\n\\t\\t                      (.write w (str payload))\\r\\n\\t\\t                      (inc no-of-positive))\\r\\n\\t\\t                   (.close w)))\\r\\n\\t                 (fn [_resp]\\r\\n\\t                   (.close w))\\r\\n\\t                 (fn [_resp ex]\\r\\n\\t                   (.close w)\\r\\n\\t                   (.printStackTrace ex)))]\\r\\n\\t  (statuses-filter\\r\\n\\t    \:params {\:lang \\"en\\" \:track \\"\:)\\" \:count \\"500\\"}\\r\\n\\t    \:oauth-creds my-creds\\r\\n\\t    \:callbacks callback)))" "(collect-positive-tweets)" "(defn collect-positive-tweets []\\r\\n\\t(let [w (io/writer \\"positive.json\\")\\r\\n\\t      callback (AsyncStreamingCallback.\\r\\n\\t                 (fn [_resp payload]\\r\\n\\t                   (if (< no-of-positive 1000)\\r\\n\\t\\t                   (do\\r\\n\\t\\t                      (.write w (str payload))\\r\\n\\t\\t                      (inc no-of-positive))\\r\\n\\t\\t                   (.close w)))\\r\\n\\t                 (fn [_resp]\\r\\n\\t                   (.close w))\\r\\n\\t                 (fn [_resp ex]\\r\\n\\t                   (.close w)\\r\\n\\t                   (.printStackTrace ex)))]\\r\\n\\t  (statuses-filter\\r\\n\\t    \:params {\:lang \\"en\\" \:track \\"\:)\\"}\\r\\n\\t    \:oauth-creds my-creds\\r\\n\\t    \:callbacks callback)))" "(collect-positive-tweets)" "(defn collect-negative-tweets []\\r\\n\\t(let [w (io/writer \\"negative.json\\")\\r\\n\\t      callback (AsyncStreamingCallback.\\r\\n\\t                 (fn [_resp payload]\\r\\n\\t                   (if (< no-of-negative 1000)\\r\\n\\t\\t                   (do\\r\\n\\t\\t                      (.write w (str payload))\\r\\n\\t\\t                      (inc no-of-negative))\\r\\n\\t\\t                   (.close w)))\\r\\n\\t                 (fn [_resp]\\r\\n\\t                   (.close w))\\r\\n\\t                 (fn [_resp ex]\\r\\n\\t                   (.close w)\\r\\n\\t                   (.printStackTrace ex)))]\\r\\n\\t  (statuses-filter\\r\\n\\t    \:params {\:lang \\"en\\" \:track \\"\:(\\"}\\r\\n\\t    \:oauth-creds my-creds\\r\\n\\t    \:callbacks callback)))" "collect-negative-tweets" "(collect-negative-tweets)" "(ns twitter_sentiment_analysis.data_collection\\r\\n  (\:use\\r\\n   [twitter.oauth]\\r\\n   [twitter.callbacks]\\r\\n   [twitter.callbacks.handlers]\\r\\n   [twitter.api.streaming]\\r\\n   [clojure.java.io \:as io])\\r\\n  (\:require\\r\\n   [clojure.data.json \:as json]\\r\\n   [http.async.client \:as ac])\\r\\n  (\:import\\r\\n   (twitter.callbacks.protocols AsyncStreamingCallback)))\\r\\n\\r\\n(def my-creds (make-oauth-creds \\"nemg8aXAA9wtSo8h6ZZbrsYwk\\"\\r\\n                                \\"XmRZzVqlj1i5Ag72RJpd99IQLPoK6dfvbna89SwQ1NHIJVqN1l\\"\\r\\n                                \\"3021555060-YgU9ZHHTKVQGpkbDqNw9By4VNcFz7Ph455VAY0G\\"\\r\\n                                \\"XR9MZ3SE6quDbTfGcPNTiG2QebsdYnepStnZnrrzUeuye\\"))\\r\\n(def no-of-positive 0)\\r\\n(def no-of-negative 0)\\r\\n\\r\\n(defn collect-positive-tweets []\\r\\n\\t(let [w (io/writer \\"positive.json\\")\\r\\n\\t      callback (AsyncStreamingCallback.\\r\\n\\t                 (fn [_resp payload]\\r\\n\\t                   (if (< no-of-positive 1000)\\r\\n\\t\\t                   (do\\r\\n\\t\\t                      (.write w (str payload))\\r\\n\\t\\t                      (inc no-of-positive))\\r\\n\\t\\t                   (.close w)))\\r\\n\\t                 (fn [_resp]\\r\\n\\t                   (.close w))\\r\\n\\t                 (fn [_resp ex]\\r\\n\\t                   (.close w)\\r\\n\\t                   (.printStackTrace ex)))]\\r\\n\\t  (statuses-filter\\r\\n\\t    \:params {\:lang \\"en\\" \:track \\"\:)\\"}\\r\\n\\t    \:oauth-creds my-creds\\r\\n\\t    \:callbacks callback)))\\r\\n\\r\\n\\r\\n(defn collect-negative-tweets []\\r\\n\\t(let [w (io/writer \\"negative.json\\")\\r\\n\\t      callback (AsyncStreamingCallback.\\r\\n\\t                 (fn [_resp payload]\\r\\n\\t                   (if (< no-of-negative 1000)\\r\\n\\t\\t                   (do\\r\\n\\t\\t                      (.write w (str payload))\\r\\n\\t\\t                      (inc no-of-negative))\\r\\n\\t\\t                   (.close w)))\\r\\n\\t                 (fn [_resp]\\r\\n\\t                   (.close w))\\r\\n\\t                 (fn [_resp ex]\\r\\n\\t                   (.close w)\\r\\n\\t                   (.printStackTrace ex)))]\\r\\n\\t  (statuses-filter\\r\\n\\t    \:params {\:lang \\"en\\" \:track \\"\:(\\"S}\\r\\n\\t    \:oauth-creds my-creds\\r\\n\\t    \:callbacks callback)))" "(ns twitter_sentiment_analysis.data_collection\\r\\n  (\:use\\r\\n   [twitter.oauth]\\r\\n   [twitter.callbacks]\\r\\n   [twitter.callbacks.handlers]\\r\\n   [twitter.api.streaming]\\r\\n   [clojure.java.io \:as io])\\r\\n  (\:require\\r\\n   [clojure.data.json \:as json]\\r\\n   [http.async.client \:as ac])\\r\\n  (\:import\\r\\n   (twitter.callbacks.protocols AsyncStreamingCallback)))" "(def my-creds (make-oauth-creds \\"nemg8aXAA9wtSo8h6ZZbrsYwk\\"\\r\\n                                \\"XmRZzVqlj1i5Ag72RJpd99IQLPoK6dfvbna89SwQ1NHIJVqN1l\\"\\r\\n                                \\"3021555060-YgU9ZHHTKVQGpkbDqNw9By4VNcFz7Ph455VAY0G\\"\\r\\n                                \\"XR9MZ3SE6quDbTfGcPNTiG2QebsdYnepStnZnrrzUeuye\\"))" "(def no-of-positive 0)\\r\\n(def no-of-negative 0)" "(defn collect-positive-tweets []\\r\\n\\t(let [w (io/writer \\"positive.json\\")\\r\\n\\t      callback (AsyncStreamingCallback.\\r\\n\\t                 (fn [_resp payload]\\r\\n\\t                   (if (< no-of-positive 1000)\\r\\n\\t\\t                   (do\\r\\n\\t\\t                      (.write w (str payload))\\r\\n\\t\\t                      (inc no-of-positive))\\r\\n\\t\\t                   (.close w)))\\r\\n\\t                 (fn [_resp]\\r\\n\\t                   (.close w))\\r\\n\\t                 (fn [_resp ex]\\r\\n\\t                   (.close w)\\r\\n\\t                   (.printStackTrace ex)))]\\r\\n\\t  (statuses-filter\\r\\n\\t    \:params {\:lang \\"en\\" \:track \\"\:)\\"}\\r\\n\\t    \:oauth-creds my-creds\\r\\n\\t    \:callbacks callback)))" "(collect-positive-tweets)" "(ns twitter_sentiment_analysis.data_collection\\r\\n  (\:use\\r\\n   [twitter.oauth]\\r\\n   [twitter.callbacks]\\r\\n   [twitter.callbacks.handlers]\\r\\n   [twitter.api.streaming]\\r\\n   [clojure.java.io \:as io])\\r\\n  (\:require\\r\\n   [clojure.data.json \:as json]\\r\\n   [http.async.client \:as ac])\\r\\n  (\:import\\r\\n   (twitter.callbacks.protocols AsyncStreamingCallback)))" "(def my-creds (make-oauth-creds \\"nemg8aXAA9wtSo8h6ZZbrsYwk\\"\\r\\n                                \\"XmRZzVqlj1i5Ag72RJpd99IQLPoK6dfvbna89SwQ1NHIJVqN1l\\"\\r\\n                                \\"3021555060-YgU9ZHHTKVQGpkbDqNw9By4VNcFz7Ph455VAY0G\\"\\r\\n                                \\"XR9MZ3SE6quDbTfGcPNTiG2QebsdYnepStnZnrrzUeuye\\"))" "(def no-of-positive 0)\\r\\n(def no-of-negative 0)" "(defn collect-positive-tweets []\\r\\n\\t(let [w (io/writer \\"positive.json\\")\\r\\n\\t      callback (AsyncStreamingCallback.\\r\\n\\t                 (fn [_resp payload]\\r\\n\\t                   (if (< no-of-positive 1000)\\r\\n\\t\\t                   (do\\r\\n\\t\\t                      (.write w (str payload))\\r\\n\\t\\t                      (inc no-of-positive))\\r\\n\\t\\t                   (.close w)))\\r\\n\\t                 (fn [_resp]\\r\\n\\t                   (.close w))\\r\\n\\t                 (fn [_resp ex]\\r\\n\\t                   (.close w)\\r\\n\\t                   (.printStackTrace ex)))]\\r\\n\\t  (statuses-filter\\r\\n\\t    \:params {\:lang \\"en\\" \:track \\"\:)\\" \:count \\"1000\\"}\\r\\n\\t    \:oauth-creds my-creds\\r\\n\\t    \:callbacks callback)))" "(collect-positive-tweets)" "(defn collect-positive-tweets []\\r\\n\\t(let [w (io/writer \\"positive.json\\")\\r\\n\\t      callback (AsyncStreamingCallback.\\r\\n\\t                 (fn [_resp payload]\\r\\n\\t                   (if (< no-of-positive 1000)\\r\\n\\t\\t                   (do\\r\\n\\t\\t                      (.write w (str payload))\\r\\n\\t\\t                      (inc no-of-positive))\\r\\n\\t\\t                   (.close w)))\\r\\n\\t                 (fn [_resp]\\r\\n\\t                   (.close w))\\r\\n\\t                 (fn [_resp ex]\\r\\n\\t                   (.close w)\\r\\n\\t                   (.printStackTrace ex)))]\\r\\n\\t  (statuses-filter\\r\\n\\t    \:params {\:lang \\"en\\" \:track \\"\:)\\"}\\r\\n\\t    \:oauth-creds my-creds\\r\\n\\t    \:callbacks callback)))" "(collect-positive-tweets)" "(defn collect-positive-tweets []\\r\\n\\t(let [w (io/writer \\"positive.json\\")\\r\\n\\t      callback (AsyncStreamingCallback.\\r\\n\\t                 (fn [_resp payload]\\r\\n\\t                   (if (< no-of-positive 1000)\\r\\n\\t\\t                   (do\\r\\n\\t\\t                      (.write w (str payload))\\r\\n\\t\\t                      (inc no-of-positive))\\r\\n\\t\\t                   (.close w)))\\r\\n\\t                 (fn [_resp]\\r\\n\\t                   (.close w))\\r\\n\\t                 (fn [_resp ex]\\r\\n\\t                   (.close w)\\r\\n\\t                   (.printStackTrace ex)))]\\r\\n\\t  (statuses-filter\\r\\n\\t    \:params {\:lang \\"en\\" \:track \\"\:)\\"\:count \\"100\\"}\\r\\n\\t    \:oauth-creds my-creds\\r\\n\\t    \:callbacks callback)))" "(collect-positive-tweets)" "(defn collect-positive-tweets []\\r\\n\\t(let [w (io/writer \\"positive.json\\")\\r\\n\\t      callback (AsyncStreamingCallback.\\r\\n\\t                 (fn [_resp payload]\\r\\n\\t                   (if (< no-of-positive 1000)\\r\\n\\t\\t                   (do\\r\\n\\t\\t                      (.write w (str payload))\\r\\n\\t\\t                      (inc no-of-positive))\\r\\n\\t\\t                   (.close w)))\\r\\n\\t                 (fn [_resp]\\r\\n\\t                   (.close w))\\r\\n\\t                 (fn [_resp ex]\\r\\n\\t                   (.close w)\\r\\n\\t                   (.printStackTrace ex)))]\\r\\n\\t  (statuses-filter\\r\\n\\t    \:params {\:lang \\"en\\" \:track \\"\:)\\"\:count \\"1\\"}\\r\\n\\t    \:oauth-creds my-creds\\r\\n\\t    \:callbacks callback)))" "(collect-positive-tweets)" "(defn collect-positive-tweets []\\r\\n\\t(let [w (io/writer \\"positive.json\\")\\r\\n\\t      callback (AsyncStreamingCallback.\\r\\n\\t                 (fn [_resp payload]\\r\\n\\t                   (if (< no-of-positive 1000)\\r\\n\\t\\t                   (do\\r\\n\\t\\t                      (.write w (str payload))\\r\\n\\t\\t                      (inc no-of-positive))\\r\\n\\t\\t                   (.close w)))\\r\\n\\t                 (fn [_resp]\\r\\n\\t                   (.close w))\\r\\n\\t                 (fn [_resp ex]\\r\\n\\t                   (.close w)\\r\\n\\t                   (.printStackTrace ex)))]\\r\\n\\t  (statuses-filter\\r\\n\\t    \:params {\:lang \\"en\\" \:track \\"\:)\\"\:count 100}\\r\\n\\t    \:oauth-creds my-creds\\r\\n\\t    \:callbacks callback)))" "(collect-positive-tweets)" "(defn collect-positive-tweets []\\r\\n\\t(let [w (io/writer \\"positive.json\\")\\r\\n\\t      callback (AsyncStreamingCallback.\\r\\n\\t                 (fn [_resp payload]\\r\\n\\t                   (if (< no-of-positive 1000)\\r\\n\\t\\t                   (do\\r\\n\\t\\t                      (.write w (str payload))\\r\\n\\t\\t                      (inc no-of-positive))\\r\\n\\t\\t                   (.close w)))\\r\\n\\t                 (fn [_resp]\\r\\n\\t                   (.close w))\\r\\n\\t                 (fn [_resp ex]\\r\\n\\t                   (.close w)\\r\\n\\t                   (.printStackTrace ex)))]\\r\\n\\t  (statuses-filter\\r\\n\\t    \:params {\:lang \\"en\\" \:track \\"\:)\\"}\\r\\n\\t    \:oauth-creds my-creds\\r\\n\\t    \:callbacks callback)))" "(collect-positive-tweets)" "(statuses-filter\\r\\n\\t    \:params {\:lang \\"en\\" \:track \\"\:)\\"}\\r\\n\\t    \:oauth-creds my-creds)" "(ns twitter_sentiment_analysis.data_formatting\\r\\n  (\:use\\r\\n   [clojure.java.io \:as io])\\r\\n  (\:require\\r\\n   [clojure.data.json \:as json]\\r\\n   [http.async.client \:as ac]))\\r\\n\\r\\n(def formatted-tweets (atom ()))\\r\\n\\r\\n(defn strip-emoticons [text]\\r\\n  (clojure.string/replace text \#\\"[\:;\=B](')*(-)*[)(P3DSO*sp]+\\" \\"\\"))\\r\\n\\r\\n(defn strip-usernames [text]\\r\\n  (clojure.string/replace text \#\\"@[A-Za-z0-9_]*\\" \\"USERNAME\\"))\\r\\n\\r\\n(defn strip-urls [text]\\r\\n  (clojure.string/replace text \#\\"http(s)*\://t.co/[A-Za-z0-9_]*\\" \\"URL\\"))\\r\\n\\r\\n(defn strip-repeated-letters [text]\\r\\n  (clojure.string/replace text \#\\"([a-zA-Z])\\\\1+\\" \\"$1$1\\"))\\r\\n\\r\\n(defn reduce-features [path]\\r\\n  (with-open [rdr (reader path)]\\r\\n  (doseq [line (line-seq rdr)]\\r\\n    (swap\! formatted-tweets conj ((comp strip-repeated-letters strip-urls strip-usernames strip-emoticons) (\:text (json/read-str line \:key-fn keyword)))))))" "(reduce-featureas \\"negative.json\\")" "(reduce-features \\"negative.json\\")" "(reduce-features \\"positive.json\\")" "(formatted-tweets)" "formatted-tweets" "(defn save-formatted-tweets [path]\\r\\n  (let [io/writer path]\\r\\n    (doseq [tweet formatted-tweets]\\r\\n      (.write tweet))))" "(defn save-formatted-tweets [path]\\r\\n  (let [w (io/writer path)]\\r\\n    (doseq [tweet formatted-tweets]\\r\\n      (.write w tweet))))" "(save-formatted-tweets \\"formatted.json\\")" "(defn reduce-features [path1 path2]\\r\\n  (let [w [io/writer path2]]\\n  (with-open [rdr (reader path1)]\\r\\n  (doseq [line (line-seq rdr)]\\r\\n    (.write w ((comp strip-repeated-letters strip-urls strip-usernames strip-emoticons) (\:text (json/read-str line \:key-fn keyword))))))))" "(reduce-features \\"positive.json\\" \\"formatted.json\\")" "(defn reduce-features [path1 path2]\\r\\n  (let [w (io/writer path2)]\\n  (with-open [rdr (reader path1)]\\r\\n  (doseq [line (line-seq rdr)]\\r\\n    (.write w ((comp strip-repeated-letters strip-urls strip-usernames strip-emoticons) (\:text (json/read-str line \:key-fn keyword))))))))" "(reduce-features \\"positive.json\\" \\"formatted.json\\")" "(defn reduce-features [path1 path2]\\r\\n  (let [w (io/writer path2)]\\n  (with-open [rdr (reader path1)]\\r\\n  (doseq [line (line-seq rdr)]\\r\\n    (.write w ((comp strip-repeated-letters strip-urls strip-usernames strip-emoticons) \\"\:) blasssfd\\"))))))" "(reduce-features \\"positive.json\\" \\"formatted.json\\")" "(reduce-features \\"positive.json\\" \\"resources/formatted.json\\")" "(with-open [rdr (reader \\"resources/formatted.json\\")]\\n  (println (line-seq rdr)))" "(with-open [rdr (reader \\"resources/formatted.json\\")]\\n  (doseq [line (line-seq rdr)]\\n  (\:text (json/read-str line \:key-fn keyword))))" "(defn reduce-features [path1 path2]\\r\\n  (let [w (io/writer path2)]\\n  (with-open [rdr (reader path1)]\\r\\n  (doseq [line (line-seq rdr)]\\r\\n    (.write w ((comp strip-repeated-letters strip-urls strip-usernames strip-emoticons) (\:text (json/read-str line \:key-fn keyword))))))))" "(reduce-features \\"positive.json\\" \\"resources/formatted.json\\")" "(defn reduce-features [path1 path2]\\r\\n  (let [w (io/writer path2)]\\n  (with-open [rdr (reader path1)]\\r\\n  (doseq [line (line-seq rdr)]\\r\\n    (.write w (json/write-str {\:text ((comp strip-repeated-letters strip-urls strip-usernames strip-emoticons) (\:text (json/read-str line \:key-fn keyword))) \:sentiment \\"positive\\"}))))))" "(reduce-features \\"positive.json\\" \\"formatted-with-sentiment.json\\")" "(defn reduce-features [path1 path2]\\r\\n  (let [w (io/writer path2)]\\n  (with-open [rdr (reader path1)]\\r\\n  (doseq [line (line-seq rdr)]\\r\\n    (.write w (json/write-str {\:text ((comp strip-repeated-letters strip-urls strip-usernames strip-emoticons) (\:text (json/read-str line \:key-fn keyword))) \:sentiment \\"positive\\"}))\\n    (.write w \\"\\\\n\\")))))" "(reduce-features \\"positive.json\\" \\"formatted-with-sentiment.json\\")" "(reduce-features \\"negative.json\\" \\"formatted-with-sentiment.json\\")" "(defn reduce-features [path1 path2]\\r\\n  (let [w (io/writer path2)]\\n  (with-open [rdr (reader path1)]\\r\\n  (doseq [line (line-seq rdr)]\\r\\n    (spit path2 (json/write-str {\:text ((comp strip-repeated-letters strip-urls strip-usernames strip-emoticons) (\:text (json/read-str line \:key-fn keyword))) \:sentiment \\"positive\\"} \:append true))\\n    (spit path2 \\"\\\\n\\" \:append true)))))" "(reduce-features \\"positive.json\\" \\"formatted-with-sentiment.json\\")" "(reduce-features \\"negative.json\\" \\"formatted-with-sentiment.json\\")" "(defn reduce-features [path1 path2]\\r\\n  (let [w (io/writer path2)]\\n  (with-open [rdr (reader path1)]\\r\\n  (doseq [line (line-seq rdr)]\\r\\n    (spit path2 (json/write-str {\:text ((comp strip-repeated-letters strip-urls strip-usernames strip-emoticons) (\:text (json/read-str line \:key-fn keyword))) \:sentiment \\"positive\\"} \:append true))\\n    (spit path2 \\"\\\\n\\" \:append true)))\\n  (with-open [rdr (reader \\"negative.json\\")]\\r\\n  (doseq [line (line-seq rdr)]\\r\\n    (spit path2 (json/write-str {\:text ((comp strip-repeated-letters strip-urls strip-usernames strip-emoticons) (\:text (json/read-str line \:key-fn keyword))) \:sentiment \\"negative\\"} \:append true))\\r\\n    (spit path2 \\"\\\\n\\" \:append true)))))" "(reduce-features \\"positive.json\\" \\"formatted-with-sentiment.json\\")" "(defn reduce-features [path1 path2]\\r\\n  (let [w (io/writer path2)]\\n  (with-open [rdr (reader path1)]\\r\\n  (doseq [line (line-seq rdr)]\\r\\n    (.write w (json/write-str {\:text ((comp strip-repeated-letters strip-urls strip-usernames strip-emoticons) (\:text (json/read-str line \:key-fn keyword))) \:sentiment \\"positive\\"}))\\n    (.write w \\"\\\\n\\")))\\n  (with-open [rdr (reader \\"negative.json\\")]\\r\\n  (doseq [line (line-seq rdr)]\\r\\n    (spit path2 (json/write-str {\:text ((comp strip-repeated-letters strip-urls strip-usernames strip-emoticons) (\:text (json/read-str line \:key-fn keyword))) \:sentiment \\"negative\\"} \:append true))\\r\\n    (spit path2 \\"\\\\n\\" \:append true)))))" "(reduce-features \\"positive.json\\" \\"formatted-with-sentiment.json\\")" "(use 'incanter.core\\r\\n 'clojure.data.json)" "(\:use [incanter.core]\\r\\n [clojure.data.json])" "(to-dataset (read-json (slurp \\"formatted-with-sentiment\\")))" "(incanter/to-dataset (read-json (slurp \\"formatted-with-sentiment\\")))" "(incanter-core/to-dataset (read-json (slurp \\"formatted-with-sentiment\\")))" "(ns twitter_sentiment_analysis.data_collection\\r\\n  (\:use\\r\\n   [twitter.oauth]\\r\\n   [twitter.callbacks]\\r\\n   [twitter.callbacks.handlers]\\r\\n   [twitter.api.streaming]\\r\\n   [clojure.java.io \:as io])\\r\\n  (\:require\\r\\n   [clojure.data.json \:as json]\\r\\n   [http.async.client \:as ac])\\r\\n  (\:import\\r\\n   (twitter.callbacks.protocols AsyncStreamingCallback)))" "(def my-creds (make-oauth-creds \\"nemg8aXAA9wtSo8h6ZZbrsYwk\\"\\r\\n                                \\"XmRZzVqlj1i5Ag72RJpd99IQLPoK6dfvbna89SwQ1NHIJVqN1l\\"\\r\\n                                \\"3021555060-YgU9ZHHTKVQGpkbDqNw9By4VNcFz7Ph455VAY0G\\"\\r\\n                                \\"XR9MZ3SE6quDbTfGcPNTiG2QebsdYnepStnZnrrzUeuye\\"))" "(def no-of-positive 0)\\r\\n(def no-of-negative 0)" "(defn collect-positive-tweets []\\r\\n\\t(let [w (io/writer \\"positive.json\\")\\r\\n\\t      callback (AsyncStreamingCallback.\\r\\n\\t                 (fn [_resp payload]\\r\\n\\t                   (if (< no-of-positive 1000)\\r\\n\\t\\t                   (do\\r\\n\\t\\t                      (.write w (str payload))\\r\\n\\t\\t                      (inc no-of-positive))\\r\\n\\t\\t                   (.close w)))\\r\\n\\t                 (fn [_resp]\\r\\n\\t                   (.close w))\\r\\n\\t                 (fn [_resp ex]\\r\\n\\t                   (.close w)\\r\\n\\t                   (.printStackTrace ex)))]\\r\\n\\t  (statuses-filter\\r\\n\\t    \:params {\:lang \\"en\\" \:track \\"\:)\\"}\\r\\n\\t    \:oauth-creds my-creds\\r\\n\\t    \:callbacks callback)))" "(collect-positive-tweets)" "(statuses-filter\\r\\n\\t    \:params {\:lang \\"en\\" \:track \\"\:)\\"}\\r\\n\\t    \:oauth-creds my-creds)" "(collect-positive-tweets)" "(ns twitter_sentiment_analysis.data_formatting\\r\\n  (\:use\\r\\n   [clojure.java.io \:as io])\\r\\n  (\:require\\r\\n   [clojure.data.json \:as json]\\r\\n   [http.async.client \:as ac]\\r\\n   [clojure.data.csv \:as csv]))" "(def formatted-tweets (atom ()))\\r\\n\\r\\n(defn strip-emoticons [text]\\r\\n  (clojure.string/replace text \#\\"[\:;\=B](')*(-)*[)(P3DSO*sp]+\\" \\"\\"))\\r\\n\\r\\n(defn strip-usernames [text]\\r\\n  (clojure.string/replace text \#\\"@[A-Za-z0-9_]*\\" \\"USERNAME\\"))\\r\\n\\n(defn strip-urls [text]\\r\\n  (clojure.string/replace text \#\\"http(s)*\://t.co/[A-Za-z0-9_]*\\" \\"URL\\"))\\r\\n\\r\\n(defn strip-repeated-letters [text]\\r\\n  (clojure.string/replace text \#\\"([a-zA-Z])\\\\1+\\" \\"$1$1\\"))" "(defn reduce-features [path1 path2]\\r\\n  (with-open [out-file (io/writer path2)]\\r\\n  (with-open [rdr (reader path1)]\\r\\n  (doseq [line (line-seq rdr)]\\r\\n    (csv/write/csv out-file [(((comp strip-repeated-letters strip-urls strip-usernames strip-emoticons) (\:text (json/read-str line \:key-fn keyword)))) \\"positive\\"])))))" "(defn reduce-features [path1 path2]\\r\\n  (with-open [out-file (io/writer path2)]\\r\\n  (with-open [rdr (reader path1)]\\r\\n  (doseq [line (line-seq rdr)]\\r\\n    (csv/write-csv out-file [(((comp strip-repeated-letters strip-urls strip-usernames strip-emoticons) (\:text (json/read-str line \:key-fn keyword)))) \\"positive\\"])))))" "(reduce-features \\"positive.json\\" \\"formatted.csv\\")" "(defn reduce-features [path1 path2]\\r\\n  (with-open [out-file (io/writer path2)]\\r\\n  (with-open [rdr (reader path1)]\\r\\n  (doseq [line (line-seq rdr)]\\r\\n    (csv/write-csv out-file [((comp strip-repeated-letters strip-urls strip-usernames strip-emoticons) (\:text (json/read-str line \:key-fn keyword))) \\"positive\\"])))))" "(reduce-features \\"positive.json\\" \\"formatted.csv\\")" "(defn reduce-features [path1 path2]\\r\\n  (with-open [out-file (io/writer path2)]\\r\\n  (with-open [rdr (reader path1)]\\r\\n  (doseq [line (line-seq rdr)]\\r\\n    (csv/write-csv out-file [\\"negative\\" \\"positive\\"])))))" "(reduce-features \\"positive.json\\" \\"formatted.csv\\")" "(defn reduce-features [path1 path2]\\r\\n  (with-open [out-file (io/writer path2)]\\r\\n  (with-open [rdr (reader path1)]\\r\\n  (doseq [line (line-seq rdr)]\\r\\n    (csv/write-csv out-file [[\\"negative\\"] [\\"positive\\"]])))))" "(reduce-features \\"positive.json\\" \\"formatted.csv\\")" "(defn reduce-features [path1 path2]\\r\\n  (with-open [out-file (io/writer path2)]\\r\\n  (with-open [rdr (reader path1)]\\r\\n  (doseq [line (line-seq rdr)]\\r\\n    (csv/write-csv out-file [[\\"text\\" \\"sentiment\\"]])))))" "(reduce-features \\"positive.json\\" \\"formatted.csv\\")" "(defn reduce-features [path1 path2]\\r\\n  (with-open [out-file (io/writer path2) rdr (reader path1)]\\n    (csv/write-csv out-file [[\\"text\\" \\"sentiment\\"]])\\r\\n    (doseq [line (line-seq rdr)]\\r\\n      (csv/write-csv out-file [[(((comp strip-repeated-letters strip-urls strip-usernames strip-emoticons) (\:text (json/read-str line \:key-fn keyword)))) \\"positive\\"]]))))" "(reduce-features \\"positive.json\\" \\"formatted.csv\\")" "(defn reduce-features [path1 path2]\\r\\n  (with-open [out-file (io/writer path2) rdr (reader path1)]\\n    (csv/write-csv out-file [[\\"text\\" \\"sentiment\\"]])\\r\\n    (doseq [line (line-seq rdr)]\\r\\n      (csv/write-csv out-file [[((comp strip-repeated-letters strip-urls strip-usernames strip-emoticons) (\:text (json/read-str line \:key-fn keyword))) \\"positive\\"]]))))" "(reduce-features \\"positive.json\\" \\"formatted.csv\\")" "(reduce-features \\"positive_en.json\\" \\"formatted.csv\\")" "(defn reduce-features [path1 path2 path3]\\r\\n  (with-open [out-file (io/writer path2) rdrp (reader path1) rdrn (reader path3)]\\n    (csv/write-csv out-file [[\\"text\\" \\"sentiment\\"]])\\r\\n    (doseq [line (line-seq rdrp)]\\r\\n      (csv/write-csv out-file [[((comp strip-repeated-letters strip-urls strip-usernames strip-emoticons) (\:text (json/read-str line \:key-fn keyword))) \\"positive\\"]]))\\n    (doseq [line (line-seq rdrn)]\\r\\n      (csv/write-csv out-file [[((comp strip-repeated-letters strip-urls strip-usernames strip-emoticons) (\:text (json/read-str line \:key-fn keyword))) \\"negative\\"]]))))" "(reduce-features \\"positive_en.json\\" \\"formatted.csv\\" \\"negative_en.json\\")" "(ns twitter_sentiment_analysis.data_analysis\\r\\n  (\:use [clj-ml.io] [clj-ml.filters] [clj-ml.classifiers] [clj-ml.data] [clj-ml.utils])\\r\\n  (\:import [weka.attributeSelection.InfoGainAttributeEval]\\r\\n           [weka.attributeSelection.Ranker]\\r\\n           [weka.classifiers.Classifier]\\r\\n           [weka.classifiers.Evaluation]\\r\\n           [weka.classifiers.meta.FilteredClassifier]\\r\\n           [weka.core.Attribute]\\r\\n           [weka.core.FastVector]\\r\\n           [weka.core.Instance]\\r\\n           [weka.core.Instances]\\r\\n           [weka.core.tokenizers.NGramTokenizer]\\r\\n           [weka.filters.Filter]\\r\\n           [weka.filters.MultiFilter]\\r\\n           [weka.filters.supervised.attribute.AttributeSelection]\\r\\n           [weka.filters.unsupervised.attribute.StringToWordVector]\\r\\n           ))" "(def ds (dataset-set-class (load-instances \:csv \\"file\:formatted.csv\\") 1))" "(ns twitter_sentiment_analysis.data_analysis\\r\\n  (\:use [clj-ml.io] [clj-ml.filters] [clj-ml.classifiers] [clj-ml.data] [clj-ml.utils])\\r\\n  (\:import [weka.attributeSelection.InfoGainAttributeEval]\\r\\n           [weka.attributeSelection.Ranker]\\r\\n           [weka.classifiers.Classifier]\\r\\n           [weka.classifiers.Evaluation]\\r\\n           [weka.classifiers.meta.FilteredClassifier]\\r\\n           [weka.core.Attribute]\\r\\n           [weka.core.FastVector]\\r\\n           [weka.core.Instance]\\r\\n           [weka.core.Instances]\\r\\n           [weka.core.tokenizers.NGramTokenizer]\\r\\n           [weka.filters.Filter]\\r\\n           [weka.filters.MultiFilter]\\r\\n           [weka.filters.supervised.attribute.AttributeSelection]\\r\\n           [weka.filters.unsupervised.attribute.StringToWordVector]\\n           [weka.core.converters.CSVLoader]\\r\\n           ))" "(def ds (dataset-set-class (load-instances \:csv \\"file\:formatted.csv\\") 1))" "(load-instances \:csv \\"formatted.csv\\")" "(load-instances \:csv \\"file\:formatted.csv\\")" "(load-instances \:csv \\"file\:resources/formatted.csv\\")" "(ns twitter_sentiment_analysis.data_analysis\\r\\n  (import [weka.core.converters ArffLoader CSVLoader]\\r\\n           [java.io File]))" "(defn ->options\\r\\n [& opts]\\r\\n (into-array String\\r\\n (map str (flatten (remove nil? opts)))))" "(defn load-csv\\r\\n ([filename & {\:keys [header]\\r\\n \:or {header true}}]\\r\\n (let [options (->options (when-not header \\"-H\\"))\\r\\n loader (doto (CSVLoader.)\\r\\n (.setOptions options)\\r\\n (.setSource (File. filename)))]\\r\\n (.getDataSet loader))))" "(def data (load-csv \\"resources/formatted.csv\\"))" "data" "(def data (load-csv \\"resources/formatted.csv\\" \:header true))" "(def data (load-csv \\"resources/formatted.csv\\" \:header false))" "(def data (load-csv \\"resources/formatted1.csv\\" \:header false))" "data" "(def data (load-csv \\"resources/formatted1.csv\\" \:header false))" "data" "(def data (load-csv \\"resources/formatted.csv\\" \:header false))" "(def data (load-csv \\"resources/formatted1.csv\\"))" "data" "(def data (load-csv \\"resources/formatted1.csv\\") \:header false)" "(def data (load-csv \\"resources/formatted1.csv\\"))" "data" "(def data (load-csv \\"resources/formatted1.csv\\"))" "data" "(ns twitter_sentiment_analysis.data_formatting\\r\\n  (\:use\\r\\n   [clojure.java.io \:as io])\\r\\n  (\:require\\r\\n   [clojure.data.json \:as json]\\r\\n   [http.async.client \:as ac]\\r\\n   [clojure.data.csv \:as csv]))\\r\\n\\r\\n(def formatted-tweets (atom ()))\\r\\n\\r\\n(defn strip-emoticons [text]\\r\\n  (clojure.string/replace text \#\\"[\:;\=B](')*(-)*[)(P3DSO*sp]+\\" \\"\\"))\\r\\n\\r\\n(defn strip-usernames [text]\\r\\n  (clojure.string/replace text \#\\"@[A-Za-z0-9_]*\\" \\"USERNAME\\"))\\r\\n\\r\\n(defn strip-urls [text]\\r\\n  (clojure.string/replace text \#\\"http(s)*\://t.co/[A-Za-z0-9_]*\\" \\"URL\\"))\\r\\n\\r\\n(defn strip-repeated-letters [text]\\r\\n  (clojure.string/replace text \#\\"([a-zA-Z])\\\\1+\\" \\"$1$1\\"))\\r\\n\\r\\n(defn strip-commas [text]\\r\\n  (clojure.string/replace text \#\\",\\" \\" \\"))" "(defn reduce-features [path-positive path-negative path-dataset]\\r\\n  (with-open [out-file (io/writer path-dataset) rdrp (reader path-positive) rdrn (reader path-negative)]\\r\\n    (csv/write-csv out-file [[\\"text\\" \\"sentiment\\"]])\\r\\n    (doseq [line (line-seq rdrp)]\\r\\n      (csv/write-csv out-file [[((comp strip-repeated-letters strip-urls strip-usernames strip-emoticons strip-commas) (\:text (json/read-str line \:key-fn keyword))) \\"positive\\"]]))\\r\\n    (doseq [line (line-seq rdrn)]\\r\\n      (csv/write-csv out-file [[((comp strip-repeated-letters strip-urls strip-usernames strip-emoticons strip-commas) (\:text (json/read-str line \:key-fn keyword))) \\"negative\\"]]))))" "(reduce-features \\"positive_en.json\\" \\"negative_en.json\\" \\"trainingData.csv\\")" "(ns twitter_sentiment_analysis.data_analysis\\r\\n  (import [weka.core.converters ArffLoader CSVLoader]\\r\\n           [java.io File]))\\r\\n\\r\\n(defn ->options\\r\\n [& opts]\\r\\n (into-array String\\r\\n (map str (flatten (remove nil? opts)))))\\r\\n\\r\\n(defn load-csv\\r\\n ([filename & {\:keys [header]\\r\\n \:or {header true}}]\\r\\n (let [options (->options (when-not header \\"-H\\"))\\r\\n loader (doto (CSVLoader.)\\r\\n (.setOptions options)\\r\\n (.setSource (File. filename)))]\\r\\n (.getDataSet loader))))" "(def data (load-csv \\"trainingData.csv\\"))" "(def data (load-csv \\"trainingData1.csv\\"))" "data" "(def data (load-csv \\"trainingData1.csv\\" \:header true))" "data" "(def data (load-csv \\"trainingData1.csv\\" \:header true))" "data" "(def data (load-csv \\"trainingData1.csv\\" \:header true))" "data" "(ns twitter_sentiment_analysis.data_formatting\\r\\n  (\:use\\r\\n   [clojure.java.io \:as io])\\r\\n  (\:require\\r\\n   [clojure.data.json \:as json]\\r\\n   [http.async.client \:as ac]\\r\\n   [clojure.data.csv \:as csv]))\\r\\n\\r\\n(def formatted-tweets (atom ()))\\r\\n\\r\\n(defn strip-emoticons [text]\\r\\n  (clojure.string/replace text \#\\"[\:;\=B](')*(-)*[)(P3DSO*sp]+\\" \\"\\"))\\r\\n\\r\\n(defn strip-usernames [text]\\r\\n  (clojure.string/replace text \#\\"@[A-Za-z0-9_]*\\" \\"USERNAME\\"))\\r\\n\\r\\n(defn strip-urls [text]\\r\\n  (clojure.string/replace text \#\\"http(s)*\://t.co/[A-Za-z0-9_]*\\" \\"URL\\"))\\r\\n\\r\\n(defn strip-repeated-letters [text]\\r\\n  (clojure.string/replace text \#\\"([a-zA-Z])\\\\1+\\" \\"$1$1\\"))\\r\\n\\r\\n(defn strip-commas [text]\\r\\n  (clojure.string/replace text \#\\",\\" \\" \\"))\\r\\n\\r\\n(defn strip-apostrophe [text]\\r\\n  (clojure.string/replace text \#\\"'\\" \\" \\"))\\r\\n\\r\\n(defn strip-double-apostrophe [text]\\r\\n  (clojure.string/replace text \#\\"\\\\\\"\\" \\" \\"))" "(defn reduce-features [path-positive path-negative path-dataset]\\r\\n  (with-open [out-file (io/writer path-dataset) rdrp (reader path-positive) rdrn (reader path-negative)]\\r\\n    (csv/write-csv out-file [[\\"text\\" \\"sentiment\\"]])\\r\\n    (doseq [line (line-seq rdrp)]\\r\\n      (csv/write-csv out-file [[((comp strip-repeated-letters strip-urls strip-usernames strip-emoticons strip-commas strip-apostrophe strip-double-apostrophe) (\:text (json/read-str line \:key-fn keyword))) \\"positive\\"]]))\\r\\n    (doseq [line (line-seq rdrn)]\\r\\n      (csv/write-csv out-file [[((comp strip-repeated-letters strip-urls strip-usernames strip-emoticons strip-commas strip-apostrophe strip-double-apostrophe) (\:text (json/read-str line \:key-fn keyword))) \\"negative\\"]]))))" "(reduce-features \\"positive_en.json\\" \\"negative_en.json\\" \\"trainingData.csv\\")" "(ns twitter_sentiment_analysis.data_analysis\\r\\n  (import [weka.core.converters ArffLoader CSVLoader]\\r\\n           [java.io File]))\\r\\n\\r\\n(defn ->options\\r\\n [& opts]\\r\\n (into-array String\\r\\n (map str (flatten (remove nil? opts)))))\\r\\n\\r\\n(defn load-csv\\r\\n ([filename & {\:keys [header]\\r\\n \:or {header true}}]\\r\\n (let [options (->options (when-not header \\"-H\\"))\\r\\n loader (doto (CSVLoader.)\\r\\n (.setOptions options)\\r\\n (.setSource (File. filename)))]\\r\\n (.getDataSet loader))))" "(def data (load-csv \\"data/trainingData.csv\\"))" "(def data (load-csv \\"trainingData.csv\\"))" "(ns twitter_sentiment_analysis.data_formatting\\r\\n  (\:use\\r\\n   [clojure.java.io \:as io])\\r\\n  (\:require\\r\\n   [clojure.data.json \:as json]\\r\\n   [http.async.client \:as ac]\\r\\n   [clojure.data.csv \:as csv]))\\r\\n\\r\\n(def formatted-tweets (atom ()))\\r\\n\\r\\n(defn strip-emoticons [text]\\r\\n  (clojure.string/replace text \#\\"[\:;\=B](')*(-)*[)(P3DSO*sp]+\\" \\"\\"))\\r\\n\\r\\n(defn strip-usernames [text]\\r\\n  (clojure.string/replace text \#\\"@[A-Za-z0-9_]*\\" \\"USERNAME\\"))\\r\\n\\r\\n(defn strip-urls [text]\\r\\n  (clojure.string/replace text \#\\"http(s)*\://t.co/[A-Za-z0-9_]*\\" \\"URL\\"))\\r\\n\\r\\n(defn strip-repeated-letters [text]\\r\\n  (clojure.string/replace text \#\\"([a-zA-Z])\\\\1+\\" \\"$1$1\\"))\\r\\n\\r\\n(defn strip-commas [text]\\r\\n  (clojure.string/replace text \#\\",\\" \\" \\"))\\r\\n\\r\\n(defn strip-apostrophe [text]\\r\\n  (clojure.string/replace text \#\\"'\\" \\" \\"))\\r\\n\\r\\n(defn strip-double-apostrophe [text]\\r\\n  (clojure.string/replace text \#\\"\\\\\\"\\" \\" \\"))" "(defn reduce-features [path-positive path-negative path-dataset]\\r\\n  (with-open [out-file (io/writer path-dataset) rdrp (reader path-positive) rdrn (reader path-negative)]\\r\\n    (csv/write-csv out-file [[\\"text\\" \\"sentiment\\"]])\\r\\n    (doseq [line (line-seq rdrp)]\\r\\n      (csv/write-csv out-file [[((comp strip-repeated-letters strip-urls strip-usernames strip-emoticons strip-commas strip-apostrophe strip-double-apostrophe) (\:text (json/read-str line \:key-fn keyword))) \\"positive\\"]]))\\r\\n    (doseq [line (line-seq rdrn)]\\r\\n      (csv/write-csv out-file [[((comp strip-repeated-letters strip-urls strip-usernames strip-emoticons strip-commas strip-apostrophe strip-double-apostrophe) (\:text (json/read-str line \:key-fn keyword))) \\"negative\\"]]))))" "(reduce-features \\"positive_en.json\\" \\"negative_en.json\\" \\"trainingData1.csv\\")" "(reduce-features \\"positive_en.json\\" \\"negative_en.json\\" \\"trainingDataNew.csv\\")" "(ns twitter_sentiment_analysis.data_analysis\\r\\n  (import [weka.core.converters ArffLoader CSVLoader]\\r\\n           [java.io File]))\\r\\n\\r\\n(defn ->options\\r\\n [& opts]\\r\\n (into-array String\\r\\n (map str (flatten (remove nil? opts)))))\\r\\n\\r\\n(defn load-csv\\r\\n ([filename & {\:keys [header]\\r\\n \:or {header true}}]\\r\\n (let [options (->options (when-not header \\"-H\\"))\\r\\n loader (doto (CSVLoader.)\\r\\n (.setOptions options)\\r\\n (.setSource (File. filename)))]\\r\\n (.getDataSet loader))))" "(def data (load-csv \\"trainingDataNew.csv\\"))" "data" "(def filter (new NominalToString))\\r\\n(.setInputFormat filter data)\\r\\n(.userFilter (new Filter) data filter1)" "data" "(def filter-to-string (new NominalToString))" "(\:import [weka.filters.unsupervised.attribute.NominalToString])" "(def filter-to-string (new NominalToString))" "(def filter-to-string (new weka.filters.unsupervised.attribute.NominalToString))" "(.setInputFormat filter-to-string data)" "(.userFilter (new Filter) data filter1)" "(.userFilter (new weka.filters.Filter) data filter1)" "(.userFilter (new weka.filters.Filter) data filter-to-string)" "(weka.filters.Filter/useFilter data filter-to-string)" "(def filter-to-string (new weka.filters.unsupervised.attribute.NominalToString))" "(.setAttributeIndexes filter-to-string setAttributeIndexes)" "(.setAttributeIndexes filter-to-string 1)" "(.setAttributeIndexes filter-to-string \\"1\\")" "(.setInputFormat filter-to-string data)" "(weka.filters.Filter/useFilter data filter-to-string)" "(def data (load-csv \\"trainingDataNew.csv\\"))" "data" "(def data (load-csv \\"trainingDataNew.csv\\"))" "data" "(def data (load-csv \\"trainingDataNew.csv\\"))" "data" "(def data (load-csv \\"trainingDataNew.csv\\"))" "data" "(ns twitter_sentiment_analysis.data_analysis\\r\\n  (import [weka.core.converters ArffLoader CSVLoader]\\r\\n           [java.io File]))" "(defn ->options\\r\\n [& opts]\\r\\n (into-array String\\r\\n (map str (flatten (remove nil? opts)))))" "(defn load-csv\\r\\n ([filename & {\:keys [header]\\r\\n \:or {header true}}]\\r\\n (let [options (->options (when-not header \\"-H\\"))\\r\\n loader (doto (CSVLoader.)\\r\\n (.setOptions options)\\r\\n (.setSource (File. filename)))]\\r\\n (.getDataSet loader))))" "(def data (load-csv \\"data/trainingDataNew.csv\\"))" "(def data (load-csv \\"trainingDataNew.csv\\"))" "data" "(def filter-to-string (new weka.filters.unsupervised.attribute.NominalToString))" "(def data-string\\n  (.setAttributeIndexes filter-to-string 1)\\n  (.setInputFormat filter-to-string data)\\n  (weka.filters.Filter/useFilter data filter-to-string))" "(def data-string\\n  ((.setAttributeIndexes filter-to-string 1)\\n  (.setInputFormat filter-to-string data)\\n  (weka.filters.Filter/useFilter data filter-to-string)))" "(def data-string\\n  ((.setAttributeIndexes filter-to-string \\"1\\")\\n  (.setInputFormat filter-to-string data)\\n  (weka.filters.Filter/useFilter data filter-to-string)))" "(def data-string\\n  ((do\\n     (.setAttributeIndexes filter-to-string \\"1\\")\\n     (.setInputFormat filter-to-string data)\\n     (weka.filters.Filter/useFilter data filter-to-string))))" "(defn transform-to-string\\n  (.setAttributeIndexes filter-to-string \\"1\\")\\r\\n  (.setInputFormat filter-to-string data)\\r\\n  (weka.filters.Filter/useFilter data filter-to-string))" "(defn transform-to-string []\\n  (.setAttributeIndexes filter-to-string \\"1\\")\\r\\n  (.setInputFormat filter-to-string data)\\r\\n  (weka.filters.Filter/useFilter data filter-to-string))" "(transform-to-string)" "(def trainingData (transform-to-string))" "trainingData" "(\:use [clj-ml.io] [clj-ml.filters] [clj-ml.classifiers] [clj-ml.data] [clj-ml.utils])" "(\:use [clj-ml.filters] [clj-ml.classifiers] [clj-ml.data] [clj-ml.utils])" "(\:import [clj-ml.filters] [clj-ml.classifiers] [clj-ml.data] [clj-ml.utils])" "(use 'clj-ml.classifiers)" "(def classifier (make-classifier \:bayes \:naive))" "(dataset-set-class trainingData 2)" "(dataset-set-class trainingData 1)" "(classifier-train classifier ds)" "(classifier-train classifier trainingData)" "(dataset-set-class data 1)" "(classifier-train classifier data)" "(def evaluation   (classifier-evaluate classifier  \:dataset data data))" "(use 'clj-ml.utils)" "(serialize-to-file classifier \\"classifier.txt\\")" "(make-dataset ds \:text [text] \:sentiment [\\"positive\\"] [\\"negative\\"])" "(make-dataset ds \:text [\\"bla\\"] \:sentiment [\\"positive\\"] [\\"negative\\"])" "(def ds (make-dataset \\"ds\\" [{\:text [\:bla]} {\:sentiment [\:positive \:negative]}]))" "(def ds (make-dataset \\"ds\\" [{\:text [\:bla]} {\:sentiment [\:positive \:negative]}] [\:text \\"bla\\" \:sentiment \:negative]))" "(def ds (make-dataset \\"ds\\" [{\:text [\:bla]} {\:sentiment [\:positive \:negative]}] [\\"bla\\" \:negative]))" "(def ds (make-dataset \\"ds\\" [{\:text [\:bla]} {\:sentiment [\:positive \:negative]}] [\:bla \:negative]))" "(def ds (make-dataset \\"ds\\" [{\:text [\:bla]} {\:sentiment [\:positive \:negative]}] [\\"\:bla\\" \:negative]))" "(def ds (make-dataset \\"name\\" [{\:text [\:bla]} {\:sentiment [\:positive \:negative]}] []))" "(defn classify-a-tweet [classifierFileName text]\\r\\n  (let [to-classify ds {\:text \\"bla\\", \:sentiment_class \:negative}]\\r\\n\\t(classifier-classify (deserialize-from-file classifierFileName) to-classify)))" "(defn classify-a-tweet [classifierFileName text]\\r\\n  (let [to-classify (make-instance ds {\:text \\"bla\\", \:sentiment_class \:negative})]\\r\\n\\t(classifier-classify (deserialize-from-file classifierFileName) to-classify)))" "ds" "(classify-a-tweet \\"classifier.txt\\" \\"bla\\")" "(defn classify-a-tweet [classifierFileName text]\\r\\n  (let [to-classify ds {\:text \\"bla\\", \:sentiment_class \:negative}]\\r\\n\\t(classifier-classify (deserialize-from-file classifierFileName) to-classify)))" "ds" "(make-instance ds {\:text \\"bla\\", \:sentiment \:negative})" "(defn classify-a-tweet [classifierFileName text]\\r\\n  (let [to-classify (make-instance ds {\:text text, \:sentiment \:negative})]\\r\\n\\t(classifier-classify (deserialize-from-file classifierFileName) to-classify)))" "(classify-a-tweet \\"classifier.txt\\" \\"bla\\")" "(defn classify-a-tweet [classifierFileName text]\\r\\n  (let [to-classify (make-instance (make-dataset \:text [text] \:sentiment [\\"positive\\"] [\\"negative\\"]) {\:text text, \:sentiment_class \:negative})]\\r\\n\\t(classifier-classify (deserialize-from-file classifierFileName) to-classify)))" "(classify-a-tweet \\"classifier.txt\\" \\"bla\\")" "(defn classify-a-tweet [classifierFileName text]\\r\\n  (let [to-classify (make-instance (make-dataset \\"name\\" [{\:text [text]} {\:sentiment [\:positive \:negative]}] []) {\:text text, \:sentiment_class \:negative})]\\r\\n\\t(classifier-classify (deserialize-from-file classifierFileName) to-classify)))" "(classify-a-tweet \\"classifier.txt\\" \\"bla\\")" "(defn classify-a-tweet [classifierFileName text]\\r\\n  (let [to-classify (make-instance (make-dataset \\"name\\" [{\:text [\:text]} {\:sentiment [\:positive \:negative]}] []) {\:text text, \:sentiment_class \:negative})]\\r\\n\\t(classifier-classify (deserialize-from-file classifierFileName) to-classify)))" "(classify-a-tweet \\"classifier.txt\\" \\"bla\\")" "(defn classify-a-tweet [classifierFileName text]\\r\\n  (let [to-classify (make-instance (make-dataset \\"name\\" [{\:text [\:@text]} {\:sentiment [\:positive \:negative]}] []) {\:text text, \:sentiment_class \:negative})]\\r\\n\\t(classifier-classify (deserialize-from-file classifierFileName) to-classify)))" "(defn classify-a-tweet [classifierFileName text]\\r\\n  (let [to-classify (make-instance (make-dataset \\"name\\" [{\:text [text]} {\:sentiment [\:positive \:negative]}] []) {\:text text, \:sentiment_class \:negative})]\\r\\n\\t(classifier-classify (deserialize-from-file classifierFileName) to-classify)))" "(classify-a-tweet \\"classifier.txt\\" \\"bla\\")" "(defn classify-a-tweet [classifierFileName text]\\r\\n  (let [to-classify (make-instance (make-dataset \\"name\\" [{\:text [\:bla]} {\:sentiment [\:positive \:negative]}] []) {\:text text, \:sentiment_class \:negative})]\\r\\n\\t(classifier-classify (deserialize-from-file classifierFileName) to-classify)))" "(classify-a-tweet \\"classifier.txt\\" \\"bla\\")" "(make-dataset \\"name\\" [{\:text [\\"bla\\"]} {\:sentiment [\:positive \:negative]}] [])" "(defn classify-a-tweet [classifierFileName text]\\r\\n  (let [to-classify (make-instance (make-dataset \\"name\\" [{\:text [text]} {\:sentiment [\:positive \:negative]}] []) {\:text text, \:sentiment \:negative})]\\r\\n\\t(classifier-classify (deserialize-from-file classifierFileName) to-classify)))" "(classify-a-tweet \\"classifier.txt\\" \\"bla\\")" "(ns twitter_sentiment_analysis.data_analysis\\r\\n  (import [weka.core.converters ArffLoader CSVLoader]\\r\\n          [java.io File]\\r\\n          [clj-ml.io]\\r\\n          [clj-ml.classifiers]\\r\\n          [clj-ml.data]\\r\\n          [clj-ml.utils]))" "(defn ->options\\r\\n [& opts]\\r\\n (into-array String\\r\\n (map str (flatten (remove nil? opts)))))" "(defn load-csv\\r\\n ([filename & {\:keys [header]\\r\\n \:or {header true}}]\\r\\n (let [options (->options (when-not header \\"-H\\"))\\r\\n loader (doto (CSVLoader.)\\r\\n (.setOptions options)\\r\\n (.setSource (File. filename)))]\\r\\n (.getDataSet loader))))" "(def data (load-csv \\"trainingDataNew.csv\\"))" "(ns twitter_sentiment_analysis.data_analysis\\r\\n  (import [weka.core.converters ArffLoader CSVLoader]\\r\\n           [java.io File]))" "(defn ->options\\r\\n [& opts]\\r\\n (into-array String\\r\\n (map str (flatten (remove nil? opts)))))" "(defn load-csv\\r\\n ([filename & {\:keys [header]\\r\\n \:or {header true}}]\\r\\n (let [options (->options (when-not header \\"-H\\"))\\r\\n loader (doto (CSVLoader.)\\r\\n (.setOptions options)\\r\\n (.setSource (File. filename)))]\\r\\n (.getDataSet loader))))" "(def data (load-csv \\"trainingDataNew.csv\\"))" "(ns twitter_sentiment_analysis.data_analysis\\r\\n  (import [weka.core.converters ArffLoader CSVLoader ConverterUtils]\\r\\n          [java.io File]\\r\\n          [clj-ml.io]\\r\\n          [clj-ml.classifiers]\\r\\n          [clj-ml.data]\\r\\n          [clj-ml.utils]))\\r\\n\\r\\n(defn ->options\\r\\n [& opts]\\r\\n (into-array String\\r\\n (map str (flatten (remove nil? opts)))))\\r\\n\\r\\n(defn load-csv\\r\\n ([filename & {\:keys [header]\\r\\n \:or {header true}}]\\r\\n (let [options (->options (when-not header \\"-H\\"))\\r\\n loader (doto (CSVLoader.)\\r\\n (.setOptions options)\\r\\n (.setSource (File. filename)))]\\r\\n (.getDataSet loader))))\\r\\n\\r\\n(def data (load-csv \\"trainingDataNew.csv\\"))" "(ns twitter_sentiment_analysis.data_analysis\\r\\n  (import [weka.core.converters ArffLoader CSVLoader ConverterUtils]\\r\\n          [java.io File]\\r\\n          [clj-ml.io]\\r\\n          [clj-ml.classifiers]\\r\\n          [clj-ml.data]\\r\\n          [clj-ml.utils]))" "(ns twitter_sentiment_analysis.data_analysis\\r\\n  (import [weka.core.converters ArffLoader CSVLoader]\\r\\n          [java.io File]\\r\\n          [clj-ml.io]\\r\\n          [clj-ml.classifiers]\\r\\n          [clj-ml.data]\\r\\n          [clj-ml.utils]))" "(defn ->options\\r\\n [& opts]\\r\\n (into-array String\\r\\n (map str (flatten (remove nil? opts)))))" "(defn load-csv\\r\\n ([filename & {\:keys [header]\\r\\n \:or {header true}}]\\r\\n (let [options (->options (when-not header \\"-H\\"))\\r\\n loader (doto (CSVLoader.)\\r\\n (.setOptions options)\\r\\n (.setSource (File. filename)))]\\r\\n (.getDataSet loader))))" "(def data (load-csv \\"trainingDataNew.csv\\"))" "(ns twitter_sentiment_analysis.data_analysis\\r\\n  (import [weka.core.converters ArffLoader CSVLoader]))" "(defn ->options\\r\\n [& opts]\\r\\n (into-array String\\r\\n (map str (flatten (remove nil? opts)))))\\r\\n\\r\\n(defn load-csv\\r\\n ([filename & {\:keys [header]\\r\\n \:or {header true}}]\\r\\n (let [options (->options (when-not header \\"-H\\"))\\r\\n loader (doto (CSVLoader.)\\r\\n (.setOptions options)\\r\\n (.setSource (File. filename)))]\\r\\n (.getDataSet loader))))" "(def data (load-csv \\"trainingDataNew.csv\\"))" "(def data (load-csv \\"formatted.csv\\"))" "(ns twitter_sentiment_analysis.data_analysis\\r\\n  (\:require [clj-ml.io] [clj-ml.classifiers][clj-ml.utils]))" "(ns twitter_sentiment_analysis.data_analysis\\r\\n  (\:require [clj-ml.data][clj-ml.io] [clj-ml.classifiers][clj-ml.utils]))" "(ns twitter_sentiment_analysis.data_analysis\\r\\n  (\:use [clj-ml.io] [clj-ml.filters] [clj-ml.classifiers] [clj-ml.data] [clj-ml.utils]))" "(ns twitter_sentiment_analysis.data_analysis\\r\\n  (\:use  [clj-ml.data] [clj-ml.io] [clj-ml.filters] [clj-ml.classifiers] [clj-ml.utils]))" "(ns twitter_sentiment_analysis.data_analysis\\r\\n  (\:use [clj-ml.data] [clj-ml.io] [clj-ml.filters] [clj-ml.classifiers] [clj-ml.utils]))" "(ns twitter_sentiment_analysis.data_analysis\\r\\n  (\:use [clj-ml.data]))" "(ns twitter_sentiment_analysis.data_analysis\\r\\n  (\:use [clj-ml.io]))" "(ns twitter_sentiment_analysis.data_analysis\\r\\n  (\:use [clj-ml.filters]))" "(ns twitter_sentiment_analysis.data_analysis\\r\\n  (\:use [clj-ml.classifiers]))" "(ns twitter_sentiment_analysis.data_analysis\\r\\n  (\:use [clj-ml/data]))" "(ns twitter_sentiment_analysis.data_analysis\\r\\n  (\:use [clj-ml/core]))" "(ns twitter_sentiment_analysis.data_analysis\\r\\n  (\:use [clj-ml.core]))" "(ns twitter_sentiment_analysis.data_analysis\\r\\n  (\:use [clj-ml.core] [clj-ml.data]))" "(ns twitter_sentiment_analysis.data_analysis\\r\\n  (\:use [clj-ml.data]))" "(ns twitter_sentiment_analysis.data_analysis\\r\\n  (\:use [clj-ml.core] [clj-ml/data]))" "(ns twitter_sentiment_analysis.data_analysis\\r\\n  (\:use [clj-ml.io] [clj-ml.filters] [clj-ml.classifiers] [clj-ml.data] [clj-ml.utils])\\r\\n  (\:import [weka.attributeSelection.InfoGainAttributeEval]\\n           [weka.attributeSelection.Ranker]\\n           [weka.classifiers.Classifier]\\n           [weka.classifiers.Evaluation]\\n           [weka.classifiers.meta.FilteredClassifier]\\n           [weka.core.Attribute]\\n           [weka.core.FastVector]\\n           [weka.core.Instance]\\n           [weka.core.Instances]\\n           [weka.core.tokenizers.NGramTokenizer]\\n           [weka.filters.Filter]\\n           [weka.filters.MultiFilter]\\n           [weka.filters.supervised.attribute.AttributeSelection]\\n           [weka.filters.unsupervised.attribute.StringToWordVector]\\n           ))" "(use 'clj-ml.data)" "(use 'clj-ml.io)" "(use 'clj-ml.classifiers)" "(ns twitter_sentiment_analysis.data_analysis\\r\\n  (\:use [clj-ml.io] [clj-ml.filters] [clj-ml.classifiers] [clj-ml data] [clj-ml.utils])\\r\\n  (\:import [weka.attributeSelection.InfoGainAttributeEval]\\n           [weka.attributeSelection.Ranker]\\n           [weka.classifiers.Classifier]\\n           [weka.classifiers.Evaluation]\\n           [weka.classifiers.meta.FilteredClassifier]\\n           [weka.core.Attribute]\\n           [weka.core.FastVector]\\n           [weka.core.Instance]\\n           [weka.core.Instances]\\n           [weka.core.tokenizers.NGramTokenizer]\\n           [weka.filters.Filter]\\n           [weka.filters.MultiFilter]\\n           [weka.filters.supervised.attribute.AttributeSelection]\\n           [weka.filters.unsupervised.attribute.StringToWordVector]\\n           ))" "(\:use clj-ml/data)" "(\:use clj-ml.data)" "(\:require clj-ml/data)" "(\:require clj-ml.data)" "(use 'clj-ml.core)" "(use 'clj-ml.data)" "(use 'clj-ml)" "(use 'clj-ml.utils)" "(use 'clj-ml.io)" "(use 'clj-ml.data)" "(use 'clj-ml/data)" "(use 'clj-ml/data reload)" "(use 'clj-ml/data \:reload)" "(use 'clj-ml.data \:reload)" "(ns twitter_sentiment_analysis.data_analysis\\r\\n  (\:require [clj-ml.data][clj-ml.io] [clj-ml.classifiers][clj-ml.utils]))" "(ns twitter_sentiment_analysis.data_analysis\\r\\n  (\:use [clj-ml.data][clj-ml.io] [clj-ml.classifiers][clj-ml.utils] \:reload))" "(defn classify-a-tweet [classifierFileName text]\\r\\n  (let [to-classify (make-instance (make-dataset \\"name\\" [{\:text [text]} {\:sentiment [\:positive \:negative]}] []) {\:text text, \:sentiment \:negative})]\\r\\n\\t(classifier-classify (deserialize-from-file classifierFileName) to-classify)))" "(defn classify-a-tweet [classifierFileName text]\\r\\n  (let [to-classify (clj-ml/data/make-instance (make-dataset \\"name\\" [{\:text [text]} {\:sentiment [\:positive \:negative]}] []) {\:text text, \:sentiment \:negative})]\\r\\n\\t(classifier-classify (deserialize-from-file classifierFileName) to-classify)))" "(defn classify-a-tweet [classifierFileName text]\\r\\n  (let [to-classify (clj-ml.data/make-instance (make-dataset \\"name\\" [{\:text [text]} {\:sentiment [\:positive \:negative]}] []) {\:text text, \:sentiment \:negative})]\\r\\n\\t(classifier-classify (deserialize-from-file classifierFileName) to-classify)))" "(use 'clj-ml.data \:reload)" "(ns twitter_sentiment_analysis.data_analysis\\r\\n  (\:use [clj-ml.io] [clj-ml.filters] [clj-ml.classifiers] [clj-ml.data] [clj-ml.utils])\\r\\n  (\:import [weka.attributeSelection.InfoGainAttributeEval]\\r\\n           [weka.attributeSelection.Ranker]\\r\\n           [weka.classifiers.Classifier]\\r\\n           [weka.classifiers.Evaluation]\\r\\n           [weka.classifiers.meta.FilteredClassifier]\\r\\n           [weka.core.Attribute]\\r\\n           [weka.core.FastVector]\\r\\n           [weka.core.Instance]\\r\\n           [weka.core.Instances]\\r\\n           [weka.core.tokenizers.NGramTokenizer]\\r\\n           [weka.filters.Filter]\\r\\n           [weka.filters.MultiFilter]\\r\\n           [weka.filters.supervised.attribute.AttributeSelection]\\r\\n           [weka.filters.unsupervised.attribute.StringToWordVector]\\r\\n           ))" "(ns twitter_sentiment_analysis.data_analysis\\r\\n  (\:use [clj-ml.io] [clj-ml.filters] [clj-ml.classifiers] [clj-ml.data] [clj-ml.utils] \:reload)\\r\\n  (\:import [weka.attributeSelection.InfoGainAttributeEval]\\r\\n           [weka.attributeSelection.Ranker]\\r\\n           [weka.classifiers.Classifier]\\r\\n           [weka.classifiers.Evaluation]\\r\\n           [weka.classifiers.meta.FilteredClassifier]\\r\\n           [weka.core.Attribute]\\r\\n           [weka.core.FastVector]\\r\\n           [weka.core.Instance]\\r\\n           [weka.core.Instances]\\r\\n           [weka.core.tokenizers.NGramTokenizer]\\r\\n           [weka.filters.Filter]\\r\\n           [weka.filters.MultiFilter]\\r\\n           [weka.filters.supervised.attribute.AttributeSelection]\\r\\n           [weka.filters.unsupervised.attribute.StringToWordVector]\\r\\n           ))" "(ns twitter_sentiment_analysis.data_analysis\\r\\n  (\:use  [clj-ml.data] [clj-ml.io] [clj-ml.filters] [clj-ml.classifiers] [clj-ml.utils] \:reload)\\r\\n  (\:import [weka.attributeSelection.InfoGainAttributeEval]\\r\\n           [weka.attributeSelection.Ranker]\\r\\n           [weka.classifiers.Classifier]\\r\\n           [weka.classifiers.Evaluation]\\r\\n           [weka.classifiers.meta.FilteredClassifier]\\r\\n           [weka.core.Attribute]\\r\\n           [weka.core.FastVector]\\r\\n           [weka.core.Instance]\\r\\n           [weka.core.Instances]\\r\\n           [weka.core.tokenizers.NGramTokenizer]\\r\\n           [weka.filters.Filter]\\r\\n           [weka.filters.MultiFilter]\\r\\n           [weka.filters.supervised.attribute.AttributeSelection]\\r\\n           [weka.filters.unsupervised.attribute.StringToWordVector]\\r\\n           ))" "(ns twitter_sentiment_analysis.data_analysis\\r\\n  (\:use [clj-ml.data] [clj-ml.io] [clj-ml.classifiers][clj-ml.utils] \:reload))" "(use 'clj-ml.data \:reload)" "(ns twitter_sentiment_analysis.data_collection\\r\\n  (\:use\\r\\n   [twitter.oauth]\\r\\n   [twitter.callbacks]\\r\\n   [twitter.callbacks.handlers]\\r\\n   [twitter.api.streaming]\\r\\n   [clojure.java.io \:as io])\\r\\n  (\:require\\r\\n   [clojure.data.json \:as json]\\r\\n   [http.async.client \:as ac])\\r\\n  (\:import\\r\\n   (twitter.callbacks.protocols AsyncStreamingCallback)))\\r\\n\\r\\n(def my-creds (make-oauth-creds \\"nemg8aXAA9wtSo8h6ZZbrsYwk\\"\\r\\n                                \\"XmRZzVqlj1i5Ag72RJpd99IQLPoK6dfvbna89SwQ1NHIJVqN1l\\"\\r\\n                                \\"3021555060-YgU9ZHHTKVQGpkbDqNw9By4VNcFz7Ph455VAY0G\\"\\r\\n                                \\"XR9MZ3SE6quDbTfGcPNTiG2QebsdYnepStnZnrrzUeuye\\"))\\r\\n(def no-of-positive 0)\\r\\n(def no-of-negative 0)" "(defn collect-positive-tweets []\\r\\n\\t(let [w (io/writer \\"positive.json\\")\\r\\n\\t      callback (AsyncStreamingCallback.\\r\\n\\t                 (fn [_resp payload]\\r\\n\\t                   (if (< no-of-positive 1000)\\r\\n\\t\\t                   (do\\r\\n\\t\\t                      (.write w (str payload))\\r\\n\\t\\t                      (inc no-of-positive))\\r\\n\\t\\t                   (.close w)))\\r\\n\\t                 (fn [_resp]\\r\\n\\t                   (.close w))\\r\\n\\t                 (fn [_resp ex]\\r\\n\\t                   (.close w)\\r\\n\\t                   (.printStackTrace ex)))]\\r\\n\\t  (statuses-filter\\r\\n\\t    \:params {\:lang \\"en\\" \:track \\"\:)\\"}\\r\\n\\t    \:oauth-creds my-creds\\r\\n\\t    \:callbacks callback)))" "(collect-positive-tweets)" "(statuses-filter\\r\\n\\t    \:params {\:lang \\"en\\" \:track \\"\:)\\"}\\r\\n\\t    \:oauth-creds my-creds)" "(ns twitter_sentiment_analysis.data_collection\\r\\n  (\:use\\r\\n   [twitter.oauth]\\r\\n   [twitter.callbacks]\\r\\n   [twitter.callbacks.handlers]\\r\\n   [twitter.api.streaming]\\r\\n   [clojure.java.io \:as io])\\r\\n  (\:require\\r\\n   [clojure.data.json \:as json]\\r\\n   [http.async.client \:as ac])\\r\\n  (\:import\\r\\n   (twitter.callbacks.protocols AsyncStreamingCallback)))\\r\\n\\r\\n(def my-creds (make-oauth-creds \\"nemg8aXAA9wtSo8h6ZZbrsYwk\\"\\r\\n                                \\"XmRZzVqlj1i5Ag72RJpd99IQLPoK6dfvbna89SwQ1NHIJVqN1l\\"\\r\\n                                \\"3021555060-YgU9ZHHTKVQGpkbDqNw9By4VNcFz7Ph455VAY0G\\"\\r\\n                                \\"XR9MZ3SE6quDbTfGcPNTiG2QebsdYnepStnZnrrzUeuye\\"))\\r\\n(def no-of-positive (atom 0))\\r\\n(def no-of-negative (atom 0))" "(< no-of-negative 1000)" "(\= no-of-negative 0)" "(\= no-of-negative (atom 0))" "(\= @no-of-negative 0)" "(< @no-of-negative 1000)" "(defn collect-positive-tweets []\\r\\n\\t(let [w (io/writer \\"positive.json\\")\\r\\n\\t      callback (AsyncStreamingCallback.\\r\\n\\t                 (fn [_resp payload]\\r\\n\\t                   (if (< @no-of-positive 1000)\\r\\n\\t\\t                   (do\\r\\n\\t\\t                      (.write w (str payload))\\r\\n\\t\\t                      (swap\! no-of-positive inc))\\r\\n\\t\\t                   (.close w)))\\r\\n\\t                 (fn [_resp]\\r\\n\\t                   (.close w))\\r\\n\\t                 (fn [_resp ex]\\r\\n\\t                   (.close w)\\r\\n\\t                   (.printStackTrace ex)))]\\r\\n\\t  (statuses-filter\\r\\n\\t    \:params {\:lang \\"en\\" \:track \\"\:)\\"}\\r\\n\\t    \:oauth-creds my-creds\\r\\n\\t    \:callbacks callback)))" "(collect-positive-tweets)" "(statuses-filter\\r\\n\\t    \:params {\:lang \\"en\\" \:track \\"\:)\\"}\\r\\n\\t    \:oauth-creds my-creds)" "(def no-of-negative (atom 0))" "(swap\! no-of-negative inc)" "@no-of-negative" "(ns twitter_sentiment_analysis.data_collection\\r\\n  (\:use\\r\\n   [twitter.oauth]\\r\\n   [twitter.callbacks]\\r\\n   [twitter.callbacks.handlers]\\r\\n   [twitter.api.streaming]\\r\\n   [clojure.java.io \:as io])\\r\\n  (\:require\\r\\n   [clojure.data.json \:as json]\\r\\n   [http.async.client \:as ac])\\r\\n  (\:import\\r\\n   (twitter.callbacks.protocols AsyncStreamingCallback)))\\r\\n\\r\\n(def my-creds (make-oauth-creds \\"nemg8aXAA9wtSo8h6ZZbrsYwk\\"\\r\\n                                \\"XmRZzVqlj1i5Ag72RJpd99IQLPoK6dfvbna89SwQ1NHIJVqN1l\\"\\r\\n                                \\"3021555060-YgU9ZHHTKVQGpkbDqNw9By4VNcFz7Ph455VAY0G\\"\\r\\n                                \\"XR9MZ3SE6quDbTfGcPNTiG2QebsdYnepStnZnrrzUeuye\\"))\\r\\n(def no-of-positive (atom 0))\\r\\n(def no-of-negative (atom 0))" "(defn collect-negative-tweets []\\r\\n\\t(let [w (io/writer \\"negative.json\\")\\r\\n\\t      callback (AsyncStreamingCallback.\\r\\n\\t                 (fn [_resp payload]\\r\\n\\t                   (if (< @no-of-negative 1000)\\r\\n\\t\\t                   (do\\r\\n\\t\\t                      (.write w (str payload))\\r\\n\\t\\t                      (swap\! no-of-negative inc))\\r\\n\\t\\t                   (.close w)))\\r\\n\\t                 (fn [_resp]\\r\\n\\t                   (.close w))\\r\\n\\t                 (fn [_resp ex]\\r\\n\\t                   (.close w)\\r\\n\\t                   (.printStackTrace ex)))]\\r\\n\\t  (statuses-filter\\r\\n\\t    \:params {\:lang \\"en\\" \:track \\"\:(\\"}\\r\\n\\t    \:oauth-creds my-creds\\r\\n\\t    \:callbacks callback)))" "(collect-negative-tweets)" "(ns twitter_sentiment_analysis.data_analysis\\r\\n  (\:use [clj-ml.io] [clj-ml.filters] [clj-ml.classifiers] [clj-ml.data] [clj-ml.utils])\\r\\n  (\:import [weka.core.converters ArffLoader CSVLoader]\\r\\n           [java.io File]))" "(defn ->options\\r\\n [& opts]\\r\\n (into-array String\\r\\n (map str (flatten (remove nil? opts)))))\\r\\n\\r\\n(defn load-csv\\r\\n ([filename & {\:keys [header]\\r\\n \:or {header true}}]\\r\\n (let [options (->options (when-not header \\"-H\\"))\\r\\n loader (doto (CSVLoader.)\\r\\n (.setOptions options)\\r\\n (.setSource (File. filename)))]\\r\\n (.getDataSet loader))))\\r\\n\\r\\n(def data (load-csv \\"trainingDataNew.csv\\"))" "(def classifier (make-classifier \:bayes \:naive))" "(defn train-classifier [classifier data class-index]\\r\\n  (classifier-train classifier (dataset-set-class data class-index)))" "(train-classifier classifier data 1)" "(def evaluation (classifier-evaluate classifier  \:dataset data data))" "(defn save-classifier [classifier path]\\r\\n  (serialize-to-file classifier path))" "(save-classifier classifier \\"classifier_brisi.txt\\")" "(defn classify-a-tweet [classifierFileName text]\\r\\n  (let [to-classify (make-instance (make-dataset \\"name\\" [{\:text [text]} {\:sentiment [\:positive \:negative]}] []) {\:text text, \:sentiment \:positive})]\\r\\n\\t(classifier-classify (deserialize-from-file classifierFileName) to-classify)))" "(classify-a-tweet \\"classifier.txt\\" \\"your heroes\\")" "(classify-a-tweet \\"classifier.txt\\" \\"your he\\")" "(\= (classify-a-tweet \\"classifier.txt\\" \\"your he\\") 0)" "(\= (classify-a-tweet \\"classifier.txt\\" \\"your he\\") 0.0)" "(\= (classify-a-tweet \\"classifier.txt\\" \\"requiem\\") 0.0)" "\\r\\n(defn classify-a-tweet [classifierFileName text]\\r\\n  (let [to-classify (make-instance (make-dataset \\"name\\" [{\:text [text]} {\:sentiment [\:positive \:negative]}] []) {\:text text, \:sentiment \:negative})]\\r\\n\\t(classifier-classify (deserialize-from-file classifierFileName) to-classify)))" "(\= (classify-a-tweet \\"classifier.txt\\" \\"requiem\\") 0.0)" "(classifier-classify classifier (make-instance data {\:text \\"bla\\" \:sentiment \:positive}))" "(classifier-classify classifier (make-instance data {\:text \\"i miss zouis so much\\" \:sentiment \:positive}))" "(classifier-classify classifier (make-instance data {\:text \\"i miss zouis so much \\" \:sentiment \:positive}))" "(\= (classifier-classify classifier (make-instance data {\:text \\"i miss zouis so much \\" \:sentiment \:positive})) 1.0)" "(\= (classifier-classify classifier (make-instance data {\:text \\"i miss zouis so much \\" \:sentiment \:positive})) 0.0)" "(defn get-sentiment [classifierFileName text]\\r\\n  (let [sentiment (classify-a-tweet classifierFileName text)]\\r\\n  (if(\= sentiment 0.0)\\r\\n    (\\"positive\\")\\r\\n    (\\"negative\\"))))" "(get-sentiment \\"classifier.txt\\" \\"i miss zouis so much \\")" "(defn get-sentiment [classifierFileName text]\\r\\n  (let [sentiment (classify-a-tweet classifierFileName text)]\\r\\n  (if(\= sentiment 0.0)\\r\\n    (print-str \\"positive\\")\\r\\n    (print-str \\"negative\\"))))" "(get-sentiment \\"classifier.txt\\" \\"i miss zouis so much \\")" "(get-sentiment \\"classifier.txt\\" \\"aaaa \:( \\")" "(classify-a-tweet \\"classifier.txt\\" \\"i miss zouis so much \\")" "(classify-a-tweet \\"classifier.txt\\" \\"negative \\")" "(classify-a-tweet \\"classifier.txt\\" \\"effing bored \\")" "(classifier-classify classifier (make-instance data {\:text \\"i miss zouis so much \\" \:sentiment \:positive}))" "(def to-classify (make-instance (make-dataset \\"name\\" [{\:text [text]} {\:sentiment [\:positive \:negative]}] []) {\:text text, \:sentiment \:negative}))" "(def to-classify (make-instance (make-dataset \\"name\\" [{\:text [\\"i miss zouis so much \\"]} {\:sentiment [\:positive \:negative]}] []) {\:text i miss zouis so much , \:sentiment \:negative}))" "(def to-classify (make-instance (make-dataset \\"name\\" [{\:text [\\"i miss zouis so much \\"]} {\:sentiment [\:positive \:negative]}] []) {\:text \\"i miss zouis so much\\" , \:sentiment \:negative}))" "(def to-classify (make-instance (make-dataset \\"name\\" [{\:text [\\"i miss zouis so much \\"]} {\:sentiment [\:positive \:negative]}] []) {\:text \\"i miss zouis so much \\" , \:sentiment \:negative}))" "(classifier-classify classifier to-classify)"]
eclipse.preferences.version=1
